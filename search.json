[{"title":"tensorflow datasets","url":"%2Fblog%2Ftensorflow-datasets.html","content":"\n\n\nä¸ºäº†æ–¹ä¾¿å¤§å®¶å­¦ä¹ ï¼Œ tensorflowè‡ªèº«æä¾›äº†å¤šç§æ•°æ®é›†ï¼Œå¹¶ä¸”å°†è¿™äº›æ•°æ®é›†è¿›è¡Œäº†å°è£…ï¼Œæ–¹ä¾¿æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ã€‚å®ƒä½¿ç”¨ `tensorflow-datasets` è¿™ä¸ªmoduleå°è£…äº†æˆ‘ä»¬å¸¸ç”¨çš„å„ç§å…¬å…±æ•°æ®é›†ï¼Œåœ¨è¿›è¡Œæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ä¸­å¯ä»¥ç›´æ¥ç”¨è¯¥æ¨¡å—åŠ è½½æˆ‘ä»¬éœ€è¦çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒæŒºæ–¹ä¾¿çš„ã€‚æœ¬æ–‡ä»‹ç»è¯¥æ¨¡å—çš„ç®€å•ä½¿ç”¨ã€‚\n\n# å®‰è£…\n\n```shell\npip install tensorflow-datasets\n```\n\n\n\n# ä½¿ç”¨\n\n```python\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n```\n\n# æŸ¥çœ‹å¯ç”¨æ•°æ®é›†\n\n`tensorflow-datasets` æä¾›äº†æˆ‘ä»¬å¸¸ç”¨çš„æ•°æ®é›†ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹æœ‰å“ªäº›å¯ç”¨çš„æ•°æ®é›†.\n\n```python\ntfds.list_builders()\n# ç›®å‰æˆ‘è¿™é‡Œå¯ä»¥çš„æ•°æ®é›†å¦‚ä¸‹\n# ['bair_robot_pushing_small', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cifar10', 'cifar100', 'coco2014', 'diabetic_retinopathy_detection', 'dummy_dataset_shared_generator', 'dummy_mnist', 'fashion_mnist', 'image_label_folder', 'imagenet2012', 'imdb_reviews', 'lm1b', 'lsun', 'mnist', 'moving_mnist', 'nsynth', 'omniglot', 'open_images_v4', 'quickdraw_bitmap', 'squad', 'starcraft_video', 'svhn_cropped', 'tf_flowers', 'wmt_translate_ende', 'wmt_translate_enfr']\n```\n\n# äº†è§£æ•°æ®é›†\n\nç¡®å®šäº†å¯ç”¨æ•°æ®é›†ä»¥åæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªæ¥ä½¿ç”¨ï¼Œ é¦–å…ˆéœ€è¦äº†è§£ä¸€ä¸‹è¿™ä¸ªæ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯ã€‚ä¸‹é¢ä»¥ mnistä¸ºä¾‹ã€‚\n\n```python\nmnist_builder = tfds.builder(\"mnist\")\nmnist_builder.download_and_prepare()  # ä¸‹è½½æ•°æ®é›†\ninfo = mnist_builder.info\nprint(info)\n\n# tfds.core.DatasetInfo(\n#     name='mnist',\n#     version=1.0.0,\n#     description='The MNIST database of handwritten digits.',\n#     urls=['http://yann.lecun.com/exdb/mnist/'],\n#     features=FeaturesDict({\n#         'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n#         'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10)\n#     },  \n#     total_num_examples=70000,\n#     splits={\n#         'test': <tfds.core.SplitInfo num_examples=10000>,\n#         'train': <tfds.core.SplitInfo num_examples=60000>\n#     },  \n#     supervised_keys=('image', 'label'),\n#     citation='\"\"\"\n#         @article{lecun2010mnist,\n#           title={MNIST handwritten digit database},\n#           author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n#           journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n#           volume={2},\n#           year={2010}\n#         } \n#\n#     \"\"\"',\n# )\n```\n\né€šè¿‡è¯¥ä¿¡æ¯æˆ‘ä»¬å¯ä»¥çŸ¥é“ æ•°æ®é›†å…±æœ‰ 70000æ¡æ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒé›†60000å’Œæµ‹è¯•é›†10000ï¼› æ²¡æ¡æ•°æ®åŒ…å« 'image', 'label' ä¸¤éƒ¨åˆ†ã€‚ label åŒ…å«10ä¸ªç±»åˆ« ç­‰é‡è¦ä¿¡æ¯ã€‚\n\nåŒæ—¶åœ¨ä»£ç é‡Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨è¿™äº›ä¿¡æ¯\n\n```python\nprint(info.features)\nprint(info.features[\"label\"].num_classes)\nprint(info.features[\"label\"].names)\n\n# FeaturesDict({'image': Image(shape=(28, 28, 1), dtype=tf.uint8), 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10)})\n# 10\n# ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n```\n\n# ä½¿ç”¨æ•°æ®é›†\n\nä½¿ç”¨mnist_builder çš„as_dataset å‡½æ•°å³å¯è·å–æˆ‘ä»¬çš„æ•°æ®ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š\n\n```python\ndef as_dataset(self,\n               split=None,\n               batch_size=1,\n               shuffle_files=None,\n               as_supervised=False):\n    \"\"\"Constructs a `tf.data.Dataset`.\n\n      Callers must pass arguments as keyword arguments.\n\n      Args:\n        split: `tfds.core.SplitBase`, which subset(s) of the data to read. If None\n          (default), returns all splits in a dict\n          `<key: tfds.Split, value: tf.data.Dataset>`.\n        batch_size: `int`, batch size. Note that variable-length features will\n          be 0-padded if `batch_size > 1`. Users that want more custom behavior\n          should use `batch_size=1` and use the `tf.data` API to construct a\n          custom pipeline. If `batch_size == -1`, will return feature\n          dictionaries of the whole dataset with `tf.Tensor`s instead of a\n          `tf.data.Dataset`.\n        shuffle_files: `bool`, whether to shuffle the input files.\n          Defaults to `True` if `split == tfds.Split.TRAIN` and `False` otherwise.\n        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n          will have a 2-tuple structure `(input, label)` according to\n          `builder.info.supervised_keys`. If `False`, the default,\n          the returned `tf.data.Dataset` will have a dictionary with all the\n          features.\n\n      Returns:\n        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n        tfds.data.Dataset>`.\n\n        If `batch_size` is -1, will return feature dictionaries containing\n        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n      \"\"\"\n\n```\n\nä¸¾ä¾‹ï¼š \n\n```python\n# get dataset\nmnist_train = mnist_builder.as_dataset(split=tfds.Split.TRAIN)\nmnist_test = mnist_builder.as_dataset(split=tfds.Split.TEST)\nprint(mnist_train)\nprint(mnist_test)\n\n# <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n# <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n```\n\nè·å¾— tf.data.Dataset å¯¹è±¡ (mnist_train å’Œ mnist_test éƒ½æ˜¯çš„) ä»¥åæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ [`tf.data` API](https://www.tensorflow.org/guide/datasets) æä¾›çš„apiæ¥æ“ä½œæ•°æ®é›†ç”¨äºæ¨¡å‹é©¯è‰¯äº†\n\ntensorflow-datasets å¯¹äºæ•°æ®çš„ä½¿ç”¨æä¾›äº†æ›´é«˜çº§çš„å°è£… `load` å‡½æ•°ï¼Œ å¦‚ä¸‹æ‰€ç¤º\n\n```python\nmnist_train = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\nassert isinstance(mnist_train, tf.data.Dataset)\nprint(mnist_train)\n\n# ä¸‹è½½æ•°æ®\n#  <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n```\n\n\n\n# å‚è€ƒ\n\n[https://www.tensorflow.org/datasets/overview](https://www.tensorflow.org/datasets/overview)\n\napi æ¥å£ [https://www.tensorflow.org/datasets/api_docs/python/](https://www.tensorflow.org/datasets/api_docs/python/)","tags":["Neural Network"],"categories":["Neural Network"]},{"title":"ç›¸å…³æ€§æ‰“åˆ†å…¬å¼","url":"%2Fblog%2Fnote-of-relevence-socres.html","content":"\n\n\n# BM25\n\nBM25ç®—æ³•æ˜¯ä¸€ç§å¸¸è§ç”¨æ¥åšç›¸å…³åº¦æ‰“åˆ†çš„å…¬å¼ï¼Œæ€è·¯æ¯”è¾ƒç®€å•ï¼Œä¸»è¦å°±æ˜¯è®¡ç®—ä¸€ä¸ªqueryé‡Œé¢æ‰€æœ‰termå’Œæ–‡æ¡£çš„ç›¸å…³åº¦ï¼Œç„¶ååœ¨æŠŠåˆ†æ•°åšç´¯åŠ æ“ä½œ,è€Œæ¯ä¸ªè¯çš„ç›¸å…³åº¦åˆ†æ•°ä¸»è¦è¿˜æ˜¯å—åˆ°tf/idfçš„å½±å“ã€‚å…¬å¼å¦‚ä¸‹ï¼š\n$$\nscore(Q, d) = \\sum_{t \\in Q} W_t R(t, d)\n$$\nå…¶ä¸­ï¼š$R(t,d)$æ˜¯æ¯ä¸ªè¯å’Œæ–‡æ¡£çš„ç›¸å…³åº¦å€¼ï¼›$t$ä»£è¡¨queryä¸­çš„termï¼›$d$ä»£è¡¨ç›¸å…³çš„æ–‡æ¡£ï¼›$W_t$æ˜¯è¯$t$çš„æƒé‡ã€‚ \n\n$W_t$å¯ç”±å¤–éƒ¨è®¾ç½®ï¼Œé»˜è®¤æ˜¯$idf$å€¼ï¼Œidfå…¬å¼çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šè¯çš„é‡è¦ç¨‹åº¦å’Œå…¶å‡ºç°åœ¨æ€»æ–‡æ¡£é›†åˆé‡Œçš„é¢‘ç‡æˆåæ¯”ã€‚å…¶å…¬å¼å¦‚ä¸‹: \n$$\nIDF(t) = \\log \\frac{N-n(t) + 0.5}{n(t) + 0.5}\n$$\nå…¶ä¸­ï¼š$N$æ˜¯æ–‡æ¡£æ€»æ•°ï¼›$n(t)$æ˜¯åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°ï¼›0.5æ˜¯è°ƒæ•™ç³»æ•°ï¼Œé¿å…$n(t)$ä¸º0çš„æƒ…å†µã€‚å–ä¸ªlogæ˜¯ä¸ºäº†è®©idfçš„å€¼å—Nå’Œ$n(t)$çš„å½±å“æ›´åŠ å¹³æ»‘ã€‚\n\nä»è¿™ä¸ªå…¬å¼å¯ä»¥çœ‹å‡ºå½“$N$è¶Šå¤§ï¼Œ$n(ti)$è¶Šå°æ—¶$idf$å€¼è¶Šå¤§ï¼Œ\n\nä¸‹é¢æ˜¯$R(t,d)$çš„å…¬å¼ï¼Œ\n$$\n\\begin {align}\n& R(t, d) = \\frac{tf(t)(k_1 + 1)}{tf(t) + K(d)}   \\frac{qf(t)(k_2 + 1)}{qf(t) + k_2} \\\\\n\\\\\n& K(d) = k_1(1-b + b \\frac{dl}{avgdl})\n\\end {align}\n$$\nå…¶ä¸­: $k_1$ï¼Œ$k_2$ï¼Œ$b$éƒ½æ˜¯è°ƒèŠ‚å› å­ï¼Œä¸€èˆ¬$k1=2$ï¼Œ$k2=1$ï¼Œ$b=0.75$ï¼› $tf(t)$æ˜¯è¯$t$åœ¨æ–‡æ¡£ä¸­çš„æ¬¡æ•°ï¼Œ$qf(t)$ä»£è¡¨è¯åœ¨æŸ¥è¯¢å¥é‡Œçš„æ¬¡æ•°ï¼›$dl$æ˜¯æ–‡æ¡£é•¿åº¦ï¼Œ$avgdl$æ˜¯æ–‡æ¡£å¹³å‡é•¿åº¦ï¼›\n\nå¯ä»¥çœ‹å‡ºå¦‚æœå…¶ä»–å› ç´ ä¸€æ ·$dl$è¶Šå¤§ï¼Œç›¸å…³åº¦è¶Šä½ï¼›è‡³äºé™¤ä»¥ä¸€ä¸ª$avgdl$ï¼Œæˆ‘æƒ³æ˜¯æ‹¿æœ¬ç¯‡æ–‡æ¡£é•¿åº¦å’Œæ•´ä½“æ–‡æ¡£é•¿åº¦æ°´å¹³åšæ¯”è¾ƒ ï¼Œä»¥å…å•ç‹¬å–$dl$å€¼æ—¶è¿‡å¤§ã€‚\n\nä¹˜ç§¯çš„å·¦è¾¹å› æ•°ä»£è¡¨è¯åœ¨æ–‡æ¡£ä¸­çš„æ¬¡æ•°å…³ç³»ï¼Œä¹˜ç§¯çš„å³è¾¹å› æ•°ä»£è¡¨è¯åœ¨æŸ¥è¯¢è¯­å¥ä¸­çš„æ¬¡æ•°å…³ç³»ã€‚ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒæŸ¥è¯¢è¯åœ¨æŸ¥è¯¢è¯­å¥é‡Œé¢å‡ºç°ä¸€æ¬¡ï¼Œæ‰€ä»¥$qf(t)$å¯ä»¥çœ‹æˆæ˜¯1ï¼Œåˆå› ä¸º$k_2$ä¸º1ï¼Œæ‰€ä»¥å³è¾¹å› æ•°å…¶å®å°±ç­‰äº1ï¼Œæ‰€ä»¥å…¬å¼å¯åŒ–ç®€ä¸ºä¸‹é¢è¿™æ ·:\n$$\nR(q_i, d) = \\frac{tf(t)(k_1 + 1)}{tf(t) + K}\n$$\nå…¬å¼åŒ–ç®€åå¯å¾—:\n$$\nscore(Q, d) = \\sum_{t \\in Q} IDF(t) \\frac{tf(t)(k_1 + 1)}{tf(t) + k_1(1-b + b \\frac{dl}{avgdl})}   \\\\\n$$\nå½±å“BM25å…¬å¼çš„å› æ•°æœ‰\n\n1 $idf$ï¼š  $idf$è¶Šé«˜åˆ†æ•°è¶Šé«˜\n\n2 $tf$ï¼š  $tf$è¶Šé«˜åˆ†æ•°è¶Šé«˜\n\n3 $dl/avgdl$ ï¼š å¦‚æœè¯¥æ–‡æ¡£é•¿åº¦åœ¨æ–‡æ¡£æ°´å¹³ä¸­è¶Šé«˜åˆ™åˆ†æ•°è¶Šä½ã€‚\n\n4 $k_1, k_2, b$ä¸ºåˆ†æ•°çš„è°ƒèŠ‚å› å­\n\n\n\n# BM25F(2004)\n\nä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä¸€ç¯‡æ–‡ç« æ˜¯åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†çš„ï¼Œå¦‚ titleï¼Œcontentï¼Œdescriptionï¼Œanchor ç­‰ï¼Œåœ¨BM25Fç®—åˆ†å…¬å¼ä¸­ï¼Œè¿™äº›éƒ¨åˆ†è¢«ç§°ä¸ºåŸŸ(field)ã€‚æœ‰ä¸¤ç¯‡æ–‡ç« ï¼Œä¸€ç¯‡æ–‡ç« çš„titleéƒ¨åˆ†ä¸query çš„BM25ç›¸å…³æ€§å¾—åˆ†ä¸º aï¼Œ å¦ä¸€ç¯‡æ–‡ç« çš„contentéƒ¨åˆ†ä¸query çš„BM25ç›¸å…³æ€§å¾—åˆ†ä¹Ÿä¸º aã€‚å‡è®¾ä¸è€ƒè™‘è¿™ä¸¤ç¯‡æ–‡ç« å…¶ä»–éƒ¨åˆ†ä¸queryçš„ç›¸å…³æ€§æƒ…å†µï¼Œæ ¹æ®ç»éªŒï¼Œä¸€èˆ¬ç¬¬ä¸€ç¯‡æ–‡ç« åº”è¯¥åº”è¯¥æ¯”ç¬¬ä¸€ç¯‡æ–‡ç« æ›´ç›¸å…³ã€‚BM25F å¼•å…¥äº†æ–‡ç« dçš„æ¯ä¸ªåŸŸçš„ä¿¡æ¯ï¼Œå®ƒå°†æ¯ä¸ªtermåœ¨æ–‡ç« dä¸­çš„æ¯ä¸ªåŸŸä¸­çš„ç›¸å…³æ€§è¿›è¡Œäº†å¤„ç†ã€‚å…¬å¼å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\n& score(Q, d) = \\sum_{t \\in Q} IDF(t) \\frac{w(t,d)}{k_1 + w(t,d)}   \\\\\n\\\\\n& w(t,d) = \\sum_{f \\in d} \\frac{tf(t,f,d) \\times boost_f}{1-b_f + b_f \\frac{len(f, d)}{avglen(f)}}\n\\end {align}\n$$\n\n#  \n\n# OkaTP(2003)\n\nBM25 è¢«ä¹Ÿç§°ä¸º **Okapi BM25**,  OkaTPæ˜¯BM25ä¸ term proximity èåˆçš„ç›¸å…³æ€§è®¡ç®—å…¬å¼ã€‚\n\nqueryä¸­ç¬¬ $i$ ä¸ªtermçš„æƒé‡å®šä¹‰ä¸ºï¼š\n\n$$\nqw_i = \\frac{qtf_i}{k_3+qtf_i} \\cdot \\log \\frac{N-DF_i}{DF_i}\n$$\n\n- $N$ is the sum of documents within all collections,\n- $DF_i$  is the number of documents containing the term $t_i$ within all collections.\n- $qtf_i $query term frequency.\n\nå¯ä»¥å‘ç° $qw_i$ çš„å®šä¹‰åœ¨å½¢å¼ä¸Šæ¯”è¾ƒåƒBM25ä¸­ queryéƒ¨åˆ†ä¸IDF çš„ä¹˜ç§¯ã€‚ åŒºåˆ«æ˜¯å¸¸é‡å‚æ•°çš„è®¾ç½®ã€‚åœ¨è®ºæ–‡ä¸­è®¾ç½® $k_3 = 1000$.\n\nterm proximity å®šä¹‰å¦‚ä¸‹\n\n$$\n\\mathrm {tpi(t_i, t_j)} = \\frac {1.0}{dist[t_i, t_j] ^2}\n$$\n$d(t_i, t_j)$ is the distance expressed in number of words between search term $t_i$ and $t_j$.\n\nä¸‹å¼ä½“ç°äº†BM25ä¸ term proximity çš„èåˆï¼Œ è¯¥ç®—æ³•å°†BM25ä¸­çš„tf æ›¿æ¢æˆäº† $\\sum_{occ(t_i, t_j)}tpi(t_i, t_j)$\n$$\nw_d(t_i, t_j) = (k_1 +1) \\frac{\\sum_{occ(t_i, t_j)}tpi(t_i, t_j)}{K+ \\sum_{occ(t_i, t_j)}tpi(t_i, t_j)}\n$$\n\n$K$ çš„å®šä¹‰å’ŒBM25ç›¸åŒã€‚\n\nOkaTP æœ€ç»ˆå®šä¹‰ä¸º\n$$\nOkaTP(q,d) = BM25(q,d) + \\sum_{(t_i, t_j) \\in S}\\min\\{qw_i, qw_j\\} \\cdot w_d(t_i, t_j)\n$$\nå…¶ä¸­ Sæ˜¯ queryä¸­æ‰€æœ‰term ä¸¤ä¸¤ç»„åˆçš„é›†åˆã€‚\n\n\n\n# BM25TP(2006)\n\nè¯¥ç®—æ³•ä¸ OkaTP éå¸¸ç›¸ä¼¼ã€‚\n\nå…¶ä¸­TPå³ term proximityï¼Œåœ¨è¯¥ç®—æ³•ä¸­å¼•å…¥äº†proximity ä¿¡æ¯æ¥ä¼˜åŒ–ç›¸å…³æ€§è®¡ç®—æ•ˆæœã€‚\n\nå‡è®¾ä¸€ä¸ªquery qä¸­åŒ…å«nä¸ªterm $\\{t_1, \\dots, t_n \\}$ï¼Œ $d$ è¡¨ç¤ºä¸€ç¯‡æ–‡ç« ï¼Œä»»æ„ä¸¤ä¸ªä¸åŒterm $t_j, t_k$  åœ¨æ–‡ç«  $d$ ä¸­æ‰€å¤„ä½ç½®çš„è·ç¦»è¡¨ç¤ºä¸º $dist(t_j, t_k)$ã€‚è¿™ä¸¤ä¸ªtermçš„\n$$\n\\begin {align}\n& BM25TP(q, d) = BM25(qï¼Œd) + \\sum_i^n \\min\\{1, W_{t_i}\\} \\cdot \\frac{acc_d (t_i) \\cdot (k_1 + 1)}{acc_d (t_i) + K} \\\\\n\\\\\n& acc_d (t_i) = \\sum_{i \\neq j} W_{t_i} \\cdot \\mathrm{tpi}_d(t_i, t_j) \\\\\n\\\\\n& \\mathrm {tpi}_d = \\sum_{ o(t_i) \\in \\mathrm {occurrences\\ of\\ t_i\\ in\\ document\\ d}} \\frac {1}{dist[o(t_i), t_j] ^2}\n\\end {align}\n$$\nnote: queryä¸­çš„ç¬¬ $i$ ä¸ªtermå¯èƒ½åœ¨ document å¯èƒ½å‡ºç°å¤šæ¬¡ï¼Œ æ¯ä¸€æ¬¡å‡ºç°ç”¨ $o(t_i)$ è¡¨ç¤ºã€‚\n\nåŸå§‹è®ºæ–‡è§ Term proximity scoring for ad-hoc retrieval on very large text collections\nå¯ä»¥ç»“åˆæ–‡ç«  Selective Term Proximity Scoring Via BP-ANN ç†è§£ä¸Šé¢ç¬¬2ï¼Œ3ä¸¤ä¸ªå…¬å¼ã€‚\n\n# newTP(2008)\n\næ–‡ç« è®¤ä¸ºOkaTPå­˜åœ¨ä¸¤ä¸ªæ–¹é¢çš„é—®é¢˜ 1. OkaTP ç®—åˆ†å…¬å¼çš„åé¢éƒ¨åˆ†(å¯ä»¥çœ‹åšå¯¹äºpraseçš„ç®—åˆ†)å’Œå‰é¢çš„BM25éƒ¨åˆ†æ˜¯æœ‰é‡å çš„ï¼Œå³ä¸€ä¸ªtermä¼šåŒæ—¶å‡ºç°åœ¨å‰åä¸¤ä¸ªéƒ¨åˆ†ï¼› 2.Linear combination of scores of  unigrams  and  those of loose phrases may break the non-linear property of term frequencyã€‚\n\nåŸºäºè¿™ä¸¤ç‚¹æå‡ºäº† newTPç®—æ³•ã€‚ newTPä¸­å¼•å…¥äº† spançš„æ¦‚å¿µã€‚ span æ˜¯æ ¹æ®query termåœ¨ä¸€ä¸ªdocmentä¸­çš„å‘½ä¸­ä½ç½®ï¼Œå°†æ•´ä¸ªå‘½ä¸­åˆ—è¡¨åˆ†å‰²ä¸ºå¤šä¸ªç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µç§°ä¸ºä¸€ä¸ª expanded spanã€‚ span çš„ç¡®å®šè§„åˆ™å¦‚ä¸‹ã€‚\n\n> (1)  The distance between the current and the next is bigger  than a threshold MAX_DIS, then the chain is separated between these two hits;\n> (2)  The current and the next hit are identical, then the chain is separated between these two hits;\n> (3)  The next hit is identical to a hit with former continuous sub-chain, then the distance between the current and the next  and  the  distance  between  the  identical  hit  and  its next  is  compared,  the  chain  is  separated  at  the  bigger gap. \n> (4)  Otherwise, go on scanning the next hit.\n\nå…¶ä¸­ MAX_DIS æ˜¯è®¤ä¸ºè®¾å®šçš„ã€‚\n\næ ¹æ®spançš„ä¸­ query termçš„å¯†åº¦å’Œæ•°é‡æ¥ç¡®å®šä¸€ä¸ªtermå¯¹äºç›¸å…³æ€§çš„è´¡çŒ®ã€‚ä»è€Œå–ä»£OkaTP ä¸­çš„ tpi å’Œ tféƒ¨åˆ†ã€‚\n\nä¸€ä¸ª$span_i$ ä¸­çš„term tçš„ å¯¹äºç›¸å…³æ€§çš„è´¡çŒ®è¡¨ç¤ºä¸º:\n$$\nf(t, espan_i) = [\\frac {n_i}{width(espan_i)}]^x \\cdot (n_i)^y\n$$\nå…¶ä¸­:\n\n> - t is a query term,\n> - espan_i is an expanded span that contains t,\n> - n_i is the number of query terms that occur in espan_i\n> - Width(espan_i) is the width of espan_i\n> - x is an exponent that is used  to  restrain  that  the  value  decayed too rapidly with the density of an expanded span increasing,\n> - y is  an  exponent  that  is  used  to  prompting  the  case  that  more unique query terms appear in one expanded span.\n\n\n\nä¸€ä¸ªterm t åœ¨æ•´ä¸ªdocument ä¸­å¯¹ç›¸å…³æ€§çš„è´¡çŒ®ä¸º:\n$$\nrc_t = \\sum_i f(t, espan_i)\n$$\nå¯ä»¥çœ‹å‡º rc ä¸­åŒ…å«äº† proximityçš„ä¿¡æ¯ å’Œ tfçš„ä¿¡æ¯ã€‚\n\nç›´æ¥ç”¨ rc æ›¿æ¢ BM25 ä¸­çš„ tfï¼Œ å¾—åˆ°æ–°çš„ç›¸å…³æ€§è®¡ç®—å…¬å¼:\n$$\nnewTP = \\sum_{t \\in Q} w_t \\cdot \\frac{rc_t(k_1 + 1)}{rc_t + K}\n$$\n\n# BM25TOP(2012)\n\nå…¶ä¸­TOPå³ term order proximity.  è¯¥ç®—æ³•æ˜¯åœ¨BM25TPçš„åŸºç¡€ä¸Šè¿›è¡Œçš„ä¼˜åŒ–ã€‚åœ¨è¯¥ç®—æ³•ä¸­å¼•å…¥äº† term oderä¿¡æ¯ï¼Œ å¦‚æœä¸¤ä¸ªtermåœ¨ queryä¸­å‡ºç°çš„é¡ºåº ä¸ å…¶åœ¨documentä¸­çš„é¡ºåºç›¸ååˆ™è¿›è¡Œæƒ©ç½šï¼Œ å¦‚æœé¡ºåºç›¸åŒåˆ™ rewardã€‚\n\nåœ¨BM25TPç®—æ³•ä¸­ ä½¿ç”¨çš„ $dist[o(t_i), t_j] ^2$ æ¥è®¡ç®—proximityï¼Œ ä½†æ˜¯è¿™ä¸ªå…¬å¼å¯¹äºterm oderæ˜¯ä¸æ•æ„Ÿçš„ã€‚å°±ä¼šè®¤ä¸º John is faster than Mary å’Œ Mary is faster than John ä¸¤ä¸ªå¥å­æ˜¯ç›¸åŒçš„ã€‚\n\nä¸ºäº†è¯´æ˜ BM25TOPç®—æ³•ï¼Œå…ˆå¼•å…¥å¦‚ä¸‹ä¸¤ä¸ªå®šä¹‰ã€‚\n\n$p_{t_i;Q}$: The position of $t_i$ in query Q\n$p_{t_i;d}$: The position of $t_i$ in document d\n\n\n\nåœ¨BM25TOP ä¸­ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„å…¬å¼å¯¹ $dist[o(t_i), t_j] ^2$ è¿›è¡Œäº†æ›¿æ¢ã€‚è¿™ä¸ªå…¬å¼åº”è¯¥ç¬¦åˆå¦‚ä¸‹ä¸‰ä¸ªæ¡ä»¶ã€‚\n\n- always positive, regardless of $p_{t_x ;d} -  p_{t_y ;d} > 0$ or $p_{t_x ;d} -  p_{t_y ;d} < 0$; \n- rewards term proximity; becoming higher as $p_{t_x ;d} -  p_{t_y ;d}$ increases and vice versa; (since the score is inversely proportional to this quantity).\n- rewards correct term ordering, becoming higher in case of $p_{t_x ;d} -  p_{t_y ;d} < 0$ and vice versa; \n\næ»¡è¶³å‰ä¸¤æ¡æ˜¯BM25TPç®—æ³•ä¸­dist å‡½æ•°ä¹Ÿèƒ½æ»¡è¶³çš„ï¼Œç¬¬ä¸‰æ¡æ˜¯å¢åŠ è€ƒè™‘ term order çš„ä¸€é¡¹ã€‚\n\næ»¡è¶³ä»¥ä¸Šä¸‰ä¸ªæ¡ä»¶çš„å…¬å¼æŒºå¤šï¼Œè®ºæ–‡ä¸­é€‰æ‹©äº†å¦‚ä¸‹å…¬å¼ã€‚\n$$\n\\begin {align}\n& \\phi_d(t_x, t_y) = [a_d(t_x, t_y)]^2 - a_d(t_x, t_y) +1 \\\\\n\\\\\n & a_d(t_x, t_y) = \\frac{(p_{t_x ;d} -  p_{t_y ;d}) }{\\xi(t_x, t_y)} \\\\\n\\end {align}\n$$\nå…¶ä¸­\n$$\n\\xi(t_x, t_y) = \\left\\{\n \\begin {align}\n 1,  & p_{t_x ;Q} -  p_{t_y ;Q} > 0 \\\\\n -1, & p_{t_x ;Q} -  p_{t_y ;Q} < 0 \\\\\n \\end {align}\n \\right .\n$$\n$\\phi_d(t_x, t_y)$ çš„å¤§å°è¡¨ç¤º $t_x$ å’Œ $t_y$ åœ¨æ–‡æ¡£ d ä¸­ çš„ç›¸å¯¹è·ç¦»ï¼Œç¬¦å·è¯´æ˜äº†è¿™ä¸¤ä¸ªtermåœ¨queryå’Œdocumentä¸­çš„é¡ºåºæ˜¯å¦ç›¸åŒï¼Œæ­£å·(+) è¡¨ç¤ºé¡ºåºç›¸åŒï¼Œç¬¦å·(-) è¡¨ç¤ºè¿™ä¸¤ä¸ªtermåœ¨queryå’Œdocumentä¸­çš„é¡ºåºç›¸åã€‚\n\n$ \\phi_d(t_x, t_y)$ éš $a_d(t_x, t_y)$ çš„å‡½æ•°å˜åŒ–å›¾åƒå¦‚ä¸‹:\n\n![](note-of-relevence-socres/bm25top-phi.png)\n\nnote:  proximity æ˜¯ $\\phi_d(t_x, t_y)$ çš„å€’æ•°ï¼Œ  $\\phi_d(t_x, t_y)$ å€¼è¶Šå¤§ proximity è¶Šå°ã€‚\n\nä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œä¸¤ä¸ªtermè·ç¦»è¶Šè¿œï¼Œ $\\phi_d(t_x, t_y)$ è¶Šå¤§ï¼Œ proximity è¶Šå°ï¼›\n\nå½“å¯¹äºä¸¤å¯¹termçš„ $a_d$ çš„å¤§å°ç›¸ç­‰ï¼Œä½†ç¬¦å·ç›¸åæ—¶ï¼Œç¬¦å·ä¸ºè´Ÿ(-) çš„ä¸€å¯¹ term çš„proximityä¼šæ›´å°ã€‚\n\nä¸‹é¢å°†BM25TOPçš„è®¡ç®—å…¬æ­£æ•´ç†å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\n& BM25TOP(q, d) = BM25(qï¼Œd) + \\sum_i^n \\min\\{1, W_{t_i}\\} \\cdot \\frac{acc_d^\" (t_i) \\cdot (k_1 + 1)}{acc_d^\" (t_i) + K} \\\\\n\\\\\n& acc_d^\" (t_i) = \\sum_{i \\neq j} W_{t_i} \\cdot \\mathrm{tpi}_d^\"(t_i, t_j) \\\\\n\\\\\n& \\mathrm {tpi}_d'' = \\sum_{ o(t_i) \\in \\mathrm {occurrences\\ of\\ t_i\\ in\\ document\\ d}} \\frac {1}{\\phi_d(o(t_i), t_j)}  \\\\\n\\\\\n& \\phi_d(o(t_i), t_j) = [a_d(o(t_i), t_j)]^2 - a_d(o(t_i), t_j) +1 \\\\\n\\\\\n & a_d(t_i, t_j) = \\frac{(p_{o(t_i) ;d} -  p_{t_j ;d}) }{\\xi(o(t_i), t_j)} \\\\\n\\end {align}\n$$\nå…¶ä¸­:\n$$\n\\xi(o(t_i), t_j) = \\left\\{\n \\begin {align}\n 1,  & p_{o(t_i) ;d} -  p_{t_j ;d} > 0 \\\\\n -1, & p_{o(t_i) ;d} -  p_{t_j ;d} < 0 \\\\\n \\end {align}\n \\right .\n$$\n\n# å…¶ä»–\n\næ–‡ç« ï¼š An exploration of proximity measures in information retrieval \n\nå¯¹ Span-based approaches å’Œ Distance aggregation approaches ä¸¤å¤§ç±»æ–¹æ³•ä¸­çš„ 5 ä¸ªåº¦é‡ proximity çš„æ–¹æ³•è¿›è¡Œäº†æµ‹è¯•ï¼Œ ç¡®å®š min cover çš„åº¦é‡æ–¹æ³•æ˜¯å…¶ä¸­æœ€å¥½çš„ä¸€ç§ proximity åº¦é‡æ–¹æ³•ã€‚å¹¶å°† min cover proximity æ•´åˆåˆ°äº† BM25ä¸­\n\næ–‡ç«  Learning in a Pairwise Term-Term Proximity Framework for Information Retrieval ç½—åˆ—äº† 11ä¸ªproximity çš„åº¦é‡æ–¹æ³•ï¼Œå¹¶å°†å®ƒä»¬æ•´åˆåˆ°äº† ä¸€ä¸ªç»Ÿä¸€çš„ç›¸å…³æ€§è®¡ç®—å…¬å¼ä¸­ã€‚\n\n# æœªå®Œå¾…ç»­\n\n\n\n# å‚è€ƒèµ„æ–™\n\n1. [BM25ç›¸å…³åº¦æ‰“åˆ†å…¬å¼](https://www.cnblogs.com/hdflzh/p/4034602.html)\n2. BM25Fï¼š Robertson, S.E., Zaragoza, H., & Taylor, M.J. (2004). Simple BM25 extension to multiple weighted fields. *CIKM*.\n3. OkaTPï¼šY. Rasolofo and J. Savoy. Term Proximity Scoring for Keyword-Based Retrieval Systems. In Proceedings of the 25th European Conference on IR Research (ECIR 2003)  pages 207--218, April 2003\n4. BM25TPï¼š S. B uttcher, C. L. A. Clarke, and B. Lushman. Term proximity scoring for ad-hoc retrieval on very large text collections. In Proc. SIGIR, pages 621- 622, 2006.\n5. newTPï¼šSong R , Taylor M J , Wen J R , et al. Viewing Term Proximity from a Different Perspective[C]// European Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2008.\n6. BM25TOPï¼šL. Akritidis, D. Katsaros, and P. Bozanis. Improved retrieval effectiveness by effcient combination of term proximity and zone scoring: A simulation-based evaluation. Simul. Model. Pract. Th., 22:74{91, 2012.\n7. 5 proximity mmeasuresï¼šT. Tao and C. Zhai. An exploration of proximity measures in information retrieval. In Proc. SIGIR,pages 295-302, 2007\n8. 11 proximity mmeasuresï¼šCummins R , O'Riordan C . Learning in a Pairwise Term-Term Proximity Framework for Information Retrieval[C]// International Acm Sigir Conference on Research & Development in Information Retrieval. ACM, 2009.\n\n\n\n","tags":["IR"],"categories":["IR"]},{"title":"å¡æ–¹æ£€éªŒ","url":"%2Fblog%2FChi-Square-Test.html","content":"\nå¡æ–¹æ£€éªŒæ˜¯ä¸€ç§ç”¨é€”å¹¿æ³›çš„å‡è®¾æ£€éªŒæ–¹æ³•ï¼Œå®ƒå±äºéå‚æ•°æ£€éªŒæ–¹æ³•ã€‚ç”¨äºæ¯”è¾ƒä¸¤ä¸ªåŠä¸¤ä¸ªä»¥ä¸Šæ ·æœ¬ç‡( æ„æˆæ¯”ï¼‰ä»¥åŠä¸¤ä¸ªåˆ†ç±»å˜é‡çš„å…³è”æ€§åˆ†æã€‚æ€æƒ³æ˜¯æ¯”è¾ƒç†è®ºé¢‘æ•°å’Œå®é™…é¢‘æ•°çš„å»åˆç¨‹åº¦ã€‚\n\n# å¡æ–¹åˆ†å¸ƒ\n\nè®¾ $X_1, X_2, \\dots ,X_n$ æ˜¯æ¥è‡ªæ€»ä½“$N(0,1)$çš„æ ·æœ¬,åˆ™ç§°ç»Ÿè®¡é‡\n$$\n\\mathcal X^2=X_1^2 + X_2^2+ \\dots +X_n^2\n$$\næœä»è‡ªç”±åº¦ä¸º$n$çš„$\\chi^2$åˆ†å¸ƒ,è®°ä¸º$\\mathcal X^2 \\sim \\chi^2(n)$ . æ­¤å¤„,è‡ªç”±åº¦æ˜¯æŒ‡ä¸Šå¼å³ç«¯åŒ…å«çš„**ç‹¬ç«‹å˜é‡**çš„ä¸ªæ•°ã€‚\n\nå¡æ–¹åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š\n\n$$\nf(x;n)=\n \\begin{cases}\n \\dfrac {x^{\\frac {n}{2}-1}e^{-\\frac {x}{2}}}{2^{\\frac {n}{2}}   \\Gamma (\\frac {n}{2})},&x>0;\\\\\n 0,&{\\text{otherwise}}.\n \\end{cases}\n$$\n\n![](Chi-Square-Test/Chi-square_pdf.png)\n\n\n\nå¡æ–¹åˆ†å¸ƒæ˜¯ç”±æ­£æ€åˆ†å¸ƒæ„é€ è€Œæˆçš„ä¸€ä¸ªæ–°çš„åˆ†å¸ƒï¼Œå½“è‡ªç”±åº¦$n$å¾ˆå¤§æ—¶ï¼Œ$\\chi^2$åˆ†å¸ƒè¿‘ä¼¼ä¸ºæ­£æ€åˆ†å¸ƒã€‚æ­¤æ—¶ï¼š\nå‡å€¼ï¼š$E(\\chi^2)=n$\næ–¹å·®ï¼š$D(\\chi^2)=2n$\n\nåˆ†å¸ƒçš„å¯åŠ æ€§ï¼š è‹¥$\\chi^2(n_1)$  $\\chi^2(n_2)$ äº’ç›¸ç‹¬ç«‹ï¼Œåˆ™ï¼šæœä»$\\chi^2(n_1) + \\chi^2(n_2)$ è‡ªç”±åº¦ä¸º$n1+n2$ çš„ $\\chi^2$ åˆ†å¸ƒã€‚\n\n# å‡è®¾æ£€éªŒ\n\nå¯ä»¥å¤ä¹ è¿™ç¯‡æ–‡ç« [å‡è®¾æ£€éªŒ](https://weirping.github.io/blog/hypothesis-testing.html)ã€‚\n\n# å¡æ–¹æ£€éªŒ\n\nå¡æ–¹æ£€éªŒæ˜¯ä¸“ç”¨äºè§£å†³è®¡æ•°æ•°æ®ç»Ÿè®¡åˆ†æçš„å‡è®¾æ£€éªŒæ³•ã€‚å¡æ–¹æ£€éªŒçš„ä¸¤ä¸ªåº”ç”¨ï¼šæ‹Ÿåˆæ€§æ£€éªŒå’Œç‹¬ç«‹æ€§æ£€éªŒã€‚\n\n## æ‹Ÿåˆæ€§æ£€éªŒ\n\næ‹Ÿåˆä¼˜åº¦æ£€éªŒæ˜¯ä¸ºæ£€éªŒè§‚å¯Ÿåˆ°çš„ä¸€æ‰¹æ•°æ®æ˜¯å¦ä¸æŸç§ç†è®ºåˆ†å¸ƒç¬¦åˆï¼Œè€Œé‡‡ç”¨çš„è®¡ç®—æ–¹æ³•å°±æ˜¯å¡æ–¹æ£€éªŒæ³•ã€‚\n\nå‡è®¾ä¸€æ€»ä½“$X$æœä»åˆ†å¸ƒï¼š\n\n$$\nH_0:P(X=a_i)=p_i; \\ \\ \\ (i=1,\\dots,k) \\\\\n$$\n\nå…¶ä¸­: $a_i,p_i$éƒ½æ˜¯å·²çŸ¥çš„ï¼Œä¸”$a_1,\\dots,a_k$ä¸¤ä¸¤å„ä¸ç›¸åŒï¼Œ$p_i>0$.\n\nå…ˆè®¾æƒ³æ€»ä½“$X$çš„æ ·æœ¬æ•°é‡$n$è¶³å¤Ÿå¤§ï¼ŒæŒ‰å¤§æ•°å®šç†ï¼Œè‹¥ä»¥$v_i$è®°$X_1,\\dots,X_n$ä¸­ç­‰äº$a_i$çš„ä¸ªæ•°ï¼Œåº”æœ‰$v_i/nâ‰ˆp_i$ã€‚æˆ‘ä»¬æŠŠ$np_i$ç§°ä¸º$a_i$è¿™ä¸ªâ€œç±»â€çš„ç†è®ºå€¼ï¼Œè€ŒæŠŠ$v_i$ç§°ä¸ºå…¶ç»éªŒå€¼æˆ–è€…è§‚å¯Ÿå€¼ã€‚å¦‚ä¸‹è¡¨ï¼š\n\n| ç±»åˆ«   | $a_1$  | $a_2$  | $\\dots$ | $a_i$  | $\\dots$ | $a_k$  |\n| ---- | ------ | ------ | ------- | ------ | ------- | ------ |\n| ç†è®ºå€¼  | $np_1$ | $np_2$ | $\\dots$ | $np_i$ | $\\dots$ | $np_k$ |\n| ç»éªŒå€¼  | $v_1$  | $v_2$  | $\\dots$ | $v_i$  | $\\dots$ | $v_k$  |\n\nè¡¨ä¸­æœ€åä¸¤è¡Œçš„å·®å¼‚è¶Šå°ï¼Œåˆ™$H_0$è¶Šå¯èƒ½æ˜¯å¯¹çš„. ä¸ºäº†åæ˜ è¿™ç§å·®å¼‚,çš®å°”é€Šé‡‡ç”¨ zç»Ÿè®¡é‡ï¼š\n$$\n\\begin {align}\nZ &=âˆ‘\\frac{(ç†è®ºå€¼âˆ’ç»éªŒå€¼)^2}{ç†è®ºå€¼} \\\\\n&=âˆ‘_{i=1}^k \\frac{(np_iâˆ’v_i)^2}{(np_i)} \\sim \\chi_{k-1}^2\n\\end {align}\n$$\n\nå¯¹å…¬å¼è§£é‡Šåˆ†ä¸‰éƒ¨åˆ†ï¼š\n\n1. ç†è®ºå€¼âˆ’ç»éªŒå€¼ï¼šåæ˜ å·®å¼‚;\n2. (ç†è®ºå€¼âˆ’ç»éªŒå€¼)^2ï¼šæ’é™¤1ä¸­å­˜åœ¨æ­£è´ŸæŠµæ¶ˆçš„æƒ…å†µ;\n3. (ç†è®ºå€¼âˆ’ç»éªŒå€¼)^2/ç†è®ºå€¼ï¼šé’ˆå¯¹ä¸åŒâ€œç±»â€ä¸‹å€¼åŸŸå·®å¼‚è¿‡å¤§é—®é¢˜ï¼Œå°†å·®å¼‚è½¬æ¢åˆ°åŒç§å°ºåº¦å†…ã€‚\n\n\n**å¦‚æœåŸå‡è®¾$H_0$æˆç«‹ï¼Œåˆ™åœ¨æ ·æœ¬å¤§å°$n \\to \\infty$æ—¶ï¼Œ$Z$çš„åˆ†å¸ƒè¶‹å‘äºè‡ªç”±åº¦ä¸º$k-1$çš„$\\chi^2$åˆ†å¸ƒï¼Œå³$\\chi_{kâˆ’1}^2$ã€‚** \n\n\n\n![](Chi-Square-Test/Chi-square-test.png)\n\n\n\n\n\nscipy ä¸­æä¾›äº† chisquare è¿›è¡Œå¡æ–¹æ‹Ÿåˆä¼˜åº¦æ£€æŸ¥ï¼Œå…¶å‡½æ•°ç­¾åå¦‚ä¸‹ï¼š\n\n>chisquare(f_obs, f_exp=None, ddof=0, axis=0)\n>\n>f_obs : array_like\n>   å„ç±»åˆ«çš„è§‚å¯Ÿé¢‘ç‡\n>\n>f_exp : array_like, optional\n>   å„ç±»åˆ«çš„æœŸæœ›é¢‘ç‡. é»˜è®¤ä¸ºé¢‘ç‡ç›¸ç­‰ \n>\n>ddof : int, optional\n>   \"Delta degrees of freedom\": adjustment to the degrees of freedom\n>   for the p-value.  The p-value is computed using a chi-squared\n>   distribution with **k - 1 - ddof** degrees of freedom, where k\n>   is the number of observed frequencies.  The default value of ddof\n>   is 0.\n>\n>axis : int or None, optional\n>   The axis of the broadcast result of f_obs and f_exp along which to\n>   apply the test.  If axis is None, all values in f_obs are treated\n>   as a single data set.  Default is 0.\n\n\n\n\n```python\nfrom scipy.stats import chisquare\n\nchisquare([16, 18, 16, 14, 12, 12]) # é»˜è®¤çš„æœŸæœ›é¢‘æ•°æ—¶å‡åŒ€åˆ†å¸ƒçš„\n# Power_divergenceResult(statistic=2.0, pvalue=0.8491450360846096)\n\nchisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n#Power_divergenceResult(statistic=3.5, pvalue=0.6233876277495822)\n\n# ddof \"Delta degrees of freedom\" ï¼Œè®¡ç®—pvalueæ—¶ä½¿ç”¨çš„è‡ªç”±åº¦ä¸º  k - 1 - ddof\nchisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8], ddof=1)\n# Power_divergenceResult(statistic=3.5, pvalue=0.477878344488724)\n```\n\n\n\n\n\n\n## ç‹¬ç«‹æ€§æ£€éªŒ\n\nå¡æ–¹æ£€éªŒè¿˜å¯ä»¥ç”¨äºæ£€éªŒä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šå› ç´ ï¼ˆå„æœ‰ä¸¤é¡¹æˆ–ä»¥ä¸Šçš„åˆ†ç±»ï¼‰ä¹‹é—´æ˜¯å¦ç›¸äº’å½±å“çš„é—®é¢˜ï¼Œè¿™ç§æ£€éªŒç§°ä¸ºç‹¬ç«‹æ€§æ£€éªŒã€‚ä¾‹å¦‚è¦è®¨è®ºè¡€å‹ä¸æ€§æ ¼çš„å…³ç³»ï¼Œè¡€å‹æœ‰Aã€Bã€ABã€Oå››ç±»ï¼Œæ€§æ ¼é‡‡ç”¨å¿ƒç†å­¦ä¸Šçš„Aå‹æ€§æ ¼æ¥åˆ’åˆ†ï¼Œå³æœ‰Aå‹å’ŒBå‹ä¸¤ç§ï¼Œæ¯ä¸ªäººå¯èƒ½æ˜¯å®ƒä»¬ä¹‹é—´äº¤å‰æ‰€å½¢æˆçš„8ç§ç±»å‹ä¸­çš„ä¸€ç§ï¼Œé‚£ä¹ˆå€’åº•å®ƒä»¬ä¹‹é—´æœ‰ä¸æœ‰å…³ç³»ï¼Œå°±å¯ä»¥ç”¨å¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒã€‚\n\nå¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒçš„é›¶å‡è®¾æ˜¯å„å› ç´ ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚å› æ­¤ç†è®ºæ¬¡æ•°çš„è®¡ç®—ä¹Ÿæ˜¯åŸºäºè¿™ä¸€å‡è®¾ï¼Œå…·ä½“è®¡ç®—æ—¶ï¼Œé‡‡ç”¨åˆ—è”è¡¨çš„æ–¹å¼ï¼Œåé¢å°†ä¸¾ä¾‹è¯´æ˜ã€‚\n\nä¸¤ç»„å¤§ç™½é¼ åœ¨ä¸åŒè‡´ç™Œå‰‚ä½œç”¨ä¸‹çš„å‘ç™Œç‡å¦‚ä¸‹è¡¨ï¼Œé—®ä¸¤ç»„å‘ç™Œç‡æœ‰æ— å·®åˆ«ï¼ˆè‡´ç™Œå‰‚å¯¹å¤§ç™½é¼ çš„å‘ç™Œæ•°æ˜¯å¦ä¼šæœ‰å½±å“ï¼‰ï¼Ÿ\n\n![](Chi-Square-Test/sample01.png)\n\nè¡¨ä¸­åªæœ‰ 52, 19, 39,  3  è¿™å››ä¸ªæ•°æ®æ˜¯æ•´ä¸ªè¡¨ä¸­çš„åŸºæœ¬èµ„æ–™ï¼Œå…¶ä½™æ•°æ®å‡ç”±æ­¤æ¨ç®—å‡ºæ¥ï¼›è¿™å››æ ¼èµ„æ–™è¡¨å°±ç§°**å››æ ¼è¡¨**ï¼ˆfourfold tableï¼‰ï¼Œæˆ–ç§°2è¡Œ2åˆ—è¡¨ï¼ˆ2Ã—2 contingency tableï¼‰ã€‚ä»è¯¥èµ„æ–™ç®—å‡ºçš„ä¸¤ç»„å‘ç™Œç‡åˆ†åˆ«ä¸º73.24%å’Œ92.86%ï¼Œä¸¤è€…çš„å·®åˆ«å¯èƒ½æ˜¯ **æŠ½æ ·è¯¯å·®** æ‰€è‡´ï¼Œäº¦å¯èƒ½æ˜¯ **ä¸¤ç»„å‘ç™Œç‡ç¡®æœ‰ä¸åŒ** ã€‚è¿™é‡Œå¯é€šè¿‡å¡æ–¹æ£€éªŒæ¥éªŒè¯å…¶å·®å¼‚æœ‰æ— ç»Ÿè®¡å­¦æ„ä¹‰ï¼ˆå³ï¼Œæ˜¯å¦ä¸ºæŠ½æ ·è¯¯å·®æ‰€è‡´ï¼‰ï¼Œæ£€éªŒçš„åŸºæœ¬å…¬å¼ä¸ºï¼š\n$$\n\\chi^2 = \\sum \\frac {(A-T)^2}{T}\n$$\nå…¶ä¸­ï¼š\n\n- $A$ ä¸ºå®é™…æ•°ï¼Œå››æ ¼è¡¨çš„å››ä¸ªåŸºæœ¬æ•°æ®å°±æ˜¯å®é™…æ•°ï¼›\n- $T$ ä¸ºç†è®ºæ•°ï¼Œæ˜¯æ ¹æ®æ£€éªŒå‡è®¾æ¨æ–­å‡ºæ¥çš„ï¼›ç†è®ºæ•°çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š\n  1. å‡è®¾è¿™ä¸¤ç»„çš„å‘ç™Œç‡ç›¸åŒï¼Œå·®åˆ«ä»…æ˜¯ç”±æŠ½æ ·è¯¯å·®æ‰€è‡´\n  2. é‚£ä¹ˆç†è®ºå‘ç™Œç‡ä¸ºä¸¤ç»„åˆè®¡å‘ç™Œç‡ï¼Œå³ $91 \\div 113=80.3 \\% $ \n  3. ä»¥æ­¤ä¸ºä¾æ®ä¾¿å¯æ¨ç®—å‡ºå››æ ¼è¡¨ä¸­ç›¸åº”çš„å››æ ¼çš„ç†è®ºæ•°\n\nå¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒçš„æ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. å»ºç«‹å‡è®¾ï¼Œç¡®å®šæ˜¾è‘—æ€§æ°´å¹³ $\\alpha$ï¼š\n\n   >H0: ä¸¤ç»„çš„å‘ç™Œç‡ç›¸åŒ\n   >\n   >H1:ä¸¤ç»„çš„å‘ç™Œç‡ä¸åŒ\n   >\n   >$\\alpha = 0.05$\n\n2. è®¡ç®—ç†è®ºæ•°ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š\n   æ ¹æ® H0 å‡è®¾ï¼Œç†è®ºå‘ç™Œç‡ä¸º $91 \\div 113=80.3 \\% $ .\n   åˆ™ å››æ ¼è¡¨ä¸­çš„çœŸå®å‘ç™Œæ•° åˆ†åˆ«ä¸ºï¼š\n   ç¬¬1è¡Œ1åˆ—ï¼š $71 \\times 80.3 \\%=57.18$ \n   ç¬¬1è¡Œ2åˆ—ï¼š $71 \\times (1-80.3 \\%)=13.82$\n   ç¬¬2è¡Œ1åˆ—ï¼š $42 \\times 80.3 \\%=33.82$\n   ç¬¬2è¡Œ2åˆ—ï¼š $42 \\times (1-80.3 \\%)=8.18$\n   å³å¾—åˆ°å¦‚ä¸‹å››æ ¼è¡¨ï¼Œå…¶ä¸­æ‹¬å·é‡Œé¢ä¸ºç†è®ºå‘ç™Œæ•°ã€‚\n\n![](Chi-Square-Test/sample02.png)\n\n3. è®¡ç®—å¡æ–¹å€¼æŒ‰å…¬å¼ä»£å…¥\n   $$\n   \\begin {align}\n   \\chi^2 &=  \\sum \\frac {(A-T)^2}{T} \\\\\n   &= \\frac {(52-57.18)^2}{57.18} + \\frac {(19-13.82)^2}{13.82} + \\frac {(39-33.82)^2}{33.82} + \\frac {(3-8.18)^2}{8.18} \\\\\n   &=0.47+1.94+0.79+3.28 \\\\\n   &=6.48\n   \\end {align}\n   $$\n\n4. æŸ¥å¡æ–¹å€¼è¡¨æ±‚$P$å€¼\n   æœ¬ä¾‹ä¸­å¡æ–¹æ£€éªŒçš„è‡ªç”±åº¦ä¸º n=ï¼ˆè¡Œæ•°-1ï¼‰ï¼ˆåˆ—æ•°-1ï¼‰\n   æŸ¥å¡æ–¹ç•Œå€¼è¡¨ï¼Œæ‰¾åˆ° $\\chi_{0.05}^2(1) = 3.84$\n   ç”±äº $6.48 \\gt \\chi_{0.05}^2(1) = 3.84  $ æ‰€ä»¥ $P < 0.05$ \n   æ‰€ä»¥æ‹’ç»åŸå‡è®¾ H0ï¼Œæ¥å—å¤‡æ‹©å‡è®¾ H1ã€‚å·®å¼‚æœ‰æ˜¾è‘—ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œ å³ï¼Œä¸¤ç»„çš„å‘ç™Œç‡ä¸åŒã€‚ï¼ˆä¸æ˜ç™½çš„è¯ï¼Œå¯ä»¥å»å¤ä¹ å‡è®¾æ£€éªŒçš„ç›¸å…³çŸ¥è¯†ï¼‰\n\nscipy ä¸­æä¾›äº† åŸºäº contingency table ï¼ˆå››æ ¼è¡¨ï¼‰è¿›è¡Œ$chi^2$ç‹¬ç«‹æ€§æ£€éªŒçš„å‡½æ•°ï¼Œ\n\n```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ncontingency_table = np.array([[442, 514], [38, 6]])   # å…±å»ºå››æ ¼è¡¨\nchi2, pval, dof, expctd = chi2_contingency(contingency_table) # chi2ç»Ÿè®¡é‡, p-value, è‡ªç”±åº¦, æœŸæœ›å€¼\n```\n\n# å¡æ–¹æ£€éªŒç”¨äºç‰¹å¾é€‰æ‹©\n\nç”¨å¡æ–¹æ£€éªŒåšç‰¹å¾é€‰æ‹©æ›´å¥½çš„æ˜¯åº”ç”¨äºåˆ†ç±»é—®é¢˜ã€‚ ç‰¹å¾é€‰æ‹©çš„å‡è®¾ä¸ºï¼šå¦‚æœä¸€ä¸ªç‰¹å¾ä¸ä¸ªç±»åˆ«ç›¸äº’ç‹¬ç«‹ï¼Œåˆ™è®¤ä¸ºè¿™ä¸ªç‰¹å¾ä¸å¥½ï¼Œåä¹‹ï¼Œå¦‚æœä¸€ä¸ªç‰¹å¾ä¸ç±»åˆ«ç›¸å…³ï¼Œåˆ™è®¤ä¸ºè¿™ä¸ªç‰¹å¾æ¯”è¾ƒå¥½ã€‚åœ¨è¿™ç§åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„æ£€éªŒæ£€éªŒé—®é¢˜å¯ä»¥å®šä¹‰ä¸ºï¼š\n$$\n\\begin {align}\n& H_0: \\text{å˜é‡ä¸ç±»åˆ«ç›¸äº’ç‹¬ç«‹} \\\\\n& H_1: \\text{å˜é‡ä¸ç±»åˆ«ç›¸å…³}\n\\end {align}\n$$\nå¯ä»¥å°†è¯¥æ–¹æ³•ç†è§£ä¸ºæ‹Ÿåˆæ€§æ£€éªŒé—®é¢˜ã€‚å¦‚å¯¹äºä¸€ä¸ªç‰¹å¾æ¥è¯´ï¼Œå…¶ä¸­è§‚å¯Ÿå€¼ä¸ºå„ç±»åˆ«ä¸­è¯¥ç‰¹å¾çš„å€¼ä¹‹å’Œï¼ˆåˆ†ç±»æ•°é‡=è§‚å¯Ÿå€¼æ•°é‡=ç†è®ºå€¼æ•°é‡ï¼‰ï¼Œç†è®ºå€¼ä¸ºå„ç±»åˆ«åœ¨æ€»æ ·æœ¬é‡ä¸­æ‰€å çš„æ¯”ä¾‹ ä¹˜ä»¥ è¯¥ç‰¹å¾å€¼åªå’Œã€‚å¯ä»¥å‚è€ƒsklearnä¸­**å‡½æ•°chi2**çš„ä»£ç chi2\n\né‚£ä¹ˆ $\\chi^2$ ç»Ÿè®¡é‡è¶Šå¤§ æˆ–è€… p-value è¶Šå° åˆ™å˜é‡ä¸ç±»åˆ«è¶Šç›¸å…³ï¼Œè¶Šæ—¶ä¸€ä¸ªåº”è¯¥è¢«é€‰ç”¨çš„ç‰¹å¾ã€‚\n\næ­¤å¤„æˆ‘ä»¬å¯ä»¥ä¸å®šä¹‰æ˜¾è‘—æ€§æ°´å¹³ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡ä¸€èˆ¬æ—¶ä»è‹¥å¹²ä¸ªå˜é‡ä¸­ k ä¸ªç›¸å…³æ€§å¥½çš„ç‰¹å¾ï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‰ç…§ $\\chi^2$ ç»Ÿè®¡é‡æˆ–è€… p-value çš„å¤§å°å…³ç³»é€‰æ‹©æœ€å¥½çš„ k ä¸ªå˜é‡å³å¯ã€‚\n\nåœ¨ sklearn ä¸­æä¾›äº†è¿™ä¸ªåŠŸèƒ½ï¼š\n\n```python\nfrom sklearn.feature_selection import chi2\nfrom sklearn.datasets import load_iris\nchi2(iris.data, iris.target)\n# (array([ 10.81782088,   3.59449902, 116.16984746,  67.24482759]),\n#  array([4.47651499e-03, 1.65754167e-01, 5.94344354e-26, 2.50017968e-15]))\n```\n\nä¸Šé¢çš„ç»“æœä¸­ ç¬¬ä¸€è¡Œ ä¸º4ä¸ªç‰¹å¾å¯¹åº”çš„  $\\chi^2$ ç»Ÿè®¡é‡, ç¬¬äºŒè¡Œä¸º p-valueã€‚\n\nè¿˜å¯ä»¥å’Œ SelectKBest ä¸€èµ·ä½¿ç”¨ï¼Œç›´æ¥é€‰æ‹© kä¸ªæœ€å¥½çš„ç‰¹å¾ã€‚ ä¸‹é¢çš„ä¾‹å­æ˜¯ä»64ä¸ªç‰¹å¾ä¸­é€‰æ‹©20ä¸ªç‰¹å¾\n\n```\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import SelectKBest, chi2\nX, y = load_digits(return_X_y=True)\nX.shape\n# (1797, 64)\nmodel = SelectKBest(chi2, k=20)\nX_new = model.fit_transform(X, y)\nX_new.shape\n# (1797, 20)\n\nmodel.scores_ # å¯ä»¥å¾—åˆ°æ¯ä¸ªç‰¹å¾chi2 ç»Ÿè®¡é‡\n\nmodel.pvalues_ # å¯ä»¥å¾—åˆ°æ¯ä¸ªç‰¹å¾pvalue\n```\n\n# å¡æ–¹æ£€éªŒç”¨äºç‰¹å¾ç¦»æ•£åŒ–\n\nåœ¨è¯„åˆ†å¡æ¨¡å‹ä¸­éœ€è¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œåˆ†ç®±/ç¦»æ•£åŒ–ï¼Œå°±å¯ä»¥é‡‡ç”¨å¡æ–¹æ£€éªŒæ–¹æ³•è¿›è¡Œç¦»æ•£åŒ–ã€‚\n\nå…·ä½“æ–¹æ³•è§æˆ‘çš„ä»£ç å’Œblog(ç›®å‰ä¸å…¬å¼€)ã€‚\n\n# å‚è€ƒèµ„æ–™\n\n [å¡æ–¹æ£€éªŒç»¼è¿°](http://www.cnblogs.com/liyongzhao/articles/3369117.html)\n\n[ç‰¹å¾é€‰æ‹©-å¡æ–¹æ£€éªŒç”¨äºç‰¹å¾é€‰æ‹©](https://blog.csdn.net/ldcadai/article/details/72854462)\n\n [å¡æ–¹åˆ†å¸ƒã€å¡æ–¹ç‹¬ç«‹æ€§æ£€éªŒå’Œæ‹Ÿåˆæ€§æ£€éªŒç†è®ºåŠå…¶pythonå®ç°](https://www.cnblogs.com/Yuanjing-Liu/p/9252844.html)\n\n[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)","tags":["EDA"],"categories":["Math"]},{"title":"æœ€å¤§ç†µæ¨¡å‹","url":"%2Fblog%2FMaximum-Entropy-Model.html","content":"\n# æœ€å¤§ç†µåŸç†\n\nåœ¨å­¦ä¹ æ¦‚ç‡æ¨¡å‹æ—¶, æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹ä¸­ç†µæœ€å¤§çš„æ¨¡å‹æ˜¯æœ€å¥½çš„æ¨¡å‹; è‹¥æ¦‚ç‡æ¨¡å‹éœ€è¦æ»¡è¶³ä¸€äº›çº¦æŸ, åˆ™æœ€å¤§ç†µåŸç†å°±æ˜¯åœ¨æ»¡è¶³å·²çŸ¥çº¦æŸçš„æ¡ä»¶é›†åˆä¸­é€‰æ‹©ç†µæœ€å¤§æ¨¡å‹. æœ€å¤§ç†µåŸç†æŒ‡å‡º, å¯¹ä¸€ä¸ªéšæœºäº‹ä»¶çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé¢„æµ‹æ—¶, é¢„æµ‹åº”å½“æ»¡è¶³å…¨éƒ¨å·²çŸ¥çš„çº¦æŸ, è€Œå¯¹æœªçŸ¥çš„æƒ…å†µä¸è¦åšä»»ä½•ä¸»è§‚å‡è®¾. åœ¨è¿™ç§æƒ…å†µä¸‹, æ¦‚ç‡åˆ†å¸ƒæœ€å‡åŒ€, é¢„æµ‹çš„é£é™©æœ€å°, å› æ­¤å¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„ç†µæ˜¯æœ€å¤§. è¦ç‚¹ä¸ºä¸¤ä¸ª,å³:\n\n1. å¯¹äºå·²çŸ¥çº¦æŸçš„æƒ…å†µä¸‹,å»ºæ¨¡è¦è¦æ»¡è¶³çº¦æŸ;\n2. é™¤äº†çº¦æŸå¤–ä¸åšä»»ä½•å‡è®¾.\n\næœ€å¤§ç†µåŸç†çš„å®è´¨å°±æ˜¯, åœ¨å·²çŸ¥éƒ¨åˆ†çŸ¥è¯†çš„å‰æä¸‹, å…³äºæœªçŸ¥çš„åˆ†å¸ƒæœ€åˆç†çš„æ¨æ–­å°±æ˜¯ç¬¦åˆå·²çŸ¥çŸ¥è¯†æœ€éšæœºçš„æ¨æ–­.\nçœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­: è®¾$a \\in \\{x, y\\}$ ä¸” $b \\in \\{0, 1\\}$, è¦æ¨æ–­æ¦‚ç‡åˆ†å¸ƒ $p(a,b)$. ç°åœ¨æˆ‘ä»¬å”¯ä¸€æ‰€çŸ¥é“çš„ä¿¡æ¯æ˜¯ $p(x,0) + p(y,0) = 0.6$, å³ï¼š\n\n![](Maximum-Entropy-Model/max-entropy-pre1.png)\n\nç”±äºçº¦æŸæ¡ä»¶å¾ˆå°‘ï¼Œæ»¡è¶³æ¡ä»¶çš„åˆ†å¸ƒæœ‰æ— æ•°å¤šä¸ªï¼Œä¾‹å¦‚ä¸‹é¢çš„åˆ†å¸ƒå°±æ˜¯æ»¡è¶³å·²çŸ¥æ¡ä»¶çš„ä¸€ä¸ªåˆ†å¸ƒï¼š\n\n![](Maximum-Entropy-Model/max-entropy-pre2.png)\n\nä½†æŒ‰ç…§æœ€å¤§ç†µåŸåˆ™ï¼Œä¸Šè¿°åˆ†å¸ƒå´ä¸æ˜¯ä¸€ä¸ªå¥½çš„åˆ†å¸ƒï¼Œå› ä¸ºè¿™ä¸ªåˆ†å¸ƒçš„ç†µä¸æ˜¯æ»¡è¶³æ¡ä»¶çš„æ‰€æœ‰åˆ†å¸ƒä¸­ç†µæœ€å¤§çš„åˆ†å¸ƒã€‚æŒ‰ç…§æœ€å¤§ç†µçš„åŸåˆ™ï¼Œåº”è¯¥é€‰æ‹©çš„ä¸‹é¢çš„åˆ†å¸ƒï¼š\n\n![](Maximum-Entropy-Model/max-entropy-pre3.png)\n\nå› ä¸ºï¼Œæœ€å¤§ç†µåŸåˆ™è¦æ±‚ï¼Œåˆç†çš„åˆ†å¸ƒåº”è¯¥åŒæ—¶æ»¡è¶³è¦æ±‚ï¼š\n\n$$\n\\begin {align}\n& \\hat p = \\arg\\max_{p \\in P} H(p) = \\arg\\max_{p \\in P} [- \\sum_{a \\in \\{x, y\\}, b \\in \\{0, 1\\}} p(a, b) \\log p(a, b)] \\\\\n& p(x,0) + p(y,0) = 0.6 \\\\\n& p(x,0) + p(y,0) + p(x,1) + p(y,1) = 1 \\\\\n\\end {align}\n$$\n\n\n# æœ€å¤§ç†µæ¨¡å‹\n\nå°†æœ€å¤§ç†µåŸç†åº”ç”¨äºåˆ†ç±»é—®é¢˜, å¾—åˆ°çš„å°±æ˜¯æœ€å¤§ç†µæ¨¡å‹.\n\nä¸ºäº†ä¸‹æ–‡è®ºè¿°æ–¹ä¾¿, æ­¤å¤„å…ˆå®šä¹‰ä¸‹æ–‡ä¸­éœ€è¦ä½¿ç”¨çš„ç¬¦å·:\n\n- $\\mathcal {T} = \\{(x_1, y_1),(x_2, y_2), \\dots (x_N, y_N) \\} $ è¡¨ç¤ºè®­ç»ƒæ•°æ®é›†\n- $x_i \\in \\mathcal X$ è¡¨ç¤ºç‰¹å¾é›†åˆ\n- $y_i \\in \\mathcal Y$ è¡¨ç¤º label é›†åˆ\n\næœ€å¤§ç†µæ¨¡å‹æ˜¯ç”¨äºå¤„ç†åˆ†ç±»é—®é¢˜çš„, æ‰€ä»¥æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯æ±‚ $p(y_i|x_i)$ .\n\næŒ‰ç…§æœ€å¤§ç†µçš„åŸç†, æˆ‘ä»¬éœ€è¦è®©æ¨¡å‹æ»¡è¶³å·²çŸ¥çš„æ‰€æœ‰çº¦æŸæ¡ä»¶, é‚£ä¹ˆé—®é¢˜æ¥äº†:\n\n- è¿™äº›çº¦æŸæ¡ä»¶å“ªé‡Œæ¥å‘¢? \n- æ€ä¹ˆè¡¨ç¤ºè¿™äº›çº¦æŸ?\n\n## ç‰¹å¾å‡½æ•°\n\næœ€å¤§ç†µæ¨¡å‹éœ€è¦çš„çº¦æŸå®é™…ä¸Šå°±æ˜¯ä»è®­ç»ƒæ•°æ®$\\mathcal T$ä¸­æŠ½å–çš„.\n\né¦–å…ˆæˆ‘ä»¬æˆ‘ä»¬éœ€è¦ä»è®­ç»ƒæ•°æ®ä¸­æŠ½å–å²€äº†è‹¥å¹²ç‰¹å¾. å…¶æ¬¡ä½¿ç”¨ **ç‰¹å¾å‡½æ•°** æ¥è¡¨ç¤ºè¿™äº›ç‰¹å¾. å¯¹äºä¸€ä¸ªç»™å®šçš„æ ·æœ¬ $(x,y)$ ,ç‰¹å¾å‡½æ•°å¯ä»¥å®šä¹‰ä¸ºä»»æ„å®å€¼å‡½æ•°, é€šå¸¸æˆ‘ä»¬éƒ½æ˜¯ä½¿ç”¨äºŒå€¼å‡½æ•°æ¥å®šä¹‰. å¦‚ä¸‹æ‰€ç¤º:\n\n$$\nf(x, y) = \n\\left \\{\n\\begin {align}\n1, \\ \\ & x, yæ»¡è¶³æ¡ä»¶\\\\\n0, \\ \\ & otherwise\n\\end {align}\n\\right .\n$$\n\nä¸‹é¢ä½¿ç”¨è¯æ€§æ ‡æ³¨çš„ä¾‹å­æ¥è¯´æ˜: \n\nå‡è®¾æˆ‘ä»¬éœ€è¦åˆ¤æ–­\"æ‰“\"å­—æ˜¯åŠ¨è¯è¿˜æ˜¯é‡è¯,å·²çŸ¥çš„è®­ç»ƒæ•°æ®æœ‰:\n\n> (x_1,y_1)=(ä¸€æ‰“ç«æŸ´,é‡è¯)\n\n> (x_2,y_2)=(ä¸‰æ‰“å•¤é…’,é‡è¯)\n\n> (x_3,y_3)=(äº”æ‰“å¡‘æ–™è¢‹,é‡è¯)\n\n> (x_4,y_4)=(æ‰“ç”µè¯,åŠ¨è¯)\n\n> (x_5,y_5)=(æ‰“ç¯®çƒ,åŠ¨è¯)\n\né€šè¿‡è§‚å¯Ÿ, æˆ‘ä»¬å‘ç°, \"æ‰“\"å‰é¢ä¸ºæ•°å­—æ—¶, \"æ‰“\"æ˜¯é‡è¯, \"æ‰“\"åé¢ä¸ºåè¯æ—¶, \"æ‰“\"æ˜¯åŠ¨è¯,è¿™å°±æ˜¯ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ä¸¤ä¸ªç‰¹å¾, å¯åˆ†åˆ«ç”¨ç‰¹å¾å‡½æ•°è¡¨ç¤ºä¸º:\n\n$$\nf(x, y) = \n\\left \\{\n\\begin {align}\n1, \\ \\ & \"æ‰“\"å‰é¢ä¸ºæ•°å­— \\\\\n0, \\ \\ & otherwise\n\\end {align}\n\\right .\n$$\n\n$$\nf(x, y) = \n\\left \\{\n\\begin {align}\n1, \\ \\ & \"æ‰“\"å‰é¢ä¸ºåè¯ \\\\\n0, \\ \\ & otherwise\n\\end {align}\n\\right .\n$$\n\nå¯¹äºä¸Šé¢çš„5æ¡è®­ç»ƒæ•°æ®,æˆ‘ä»¬ä¾¿æœ‰:\n\n$$\nf_1(x_1, y_1) = f_1(x_2, y_2)=f_1(x_3,y_3)=1; f1(x_4, y_4)=f1(x_5, y_5)=0 \\\\\nf_2(x_1, y_1) = f_2(x_2, y_2)=f_2(x_3,y_3)=0; f2(x_4, y_4)=f2(x_5, y_5)=1 \\\\\n$$\n\n## ç»éªŒåˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒ\n\næ‰€è°“ç»éªŒ(æ¦‚ç‡)åˆ†å¸ƒæ˜¯æŒ‡åœ¨è®­ç»ƒæ•°æ®$\\mathcal T$è¿›è¡Œç»Ÿè®¡å¾—åˆ°çš„åˆ†å¸ƒ,ç”¨ $\\tilde p$ è¡¨ç¤º. ç”±äºçœŸå®åˆ†å¸ƒæ˜¯æ— æ³•ç›´æ¥è·å–çš„(å¦‚æœå†…å¾—åˆ°,æˆ‘ä»¬å°±ä¸éœ€è¦è®­ç»ƒæ¨¡å‹äº†), é€šå¸¸æˆ‘ä»¬éƒ½æ˜¯ä½¿ç”¨ç»éªŒåˆ†å¸ƒæ¥è¡¨ç¤ºçœŸå®åˆ†å¸ƒ(è¦æ±‚: è®­ç»ƒæ•°æ®é›†æ˜¯ä»æ€»ä½“ä¸­ **ç‹¬ç«‹åŒåˆ†å¸ƒ** æŠ½æ ·è·å–çš„).\n\nåœ¨æœ€å¤§ç†µæ¨¡å‹ä¸­, éœ€è¦è€ƒå¯Ÿä¸¤ä¸ªç»éªŒåˆ†å¸ƒ $\\tilde p(x,y)$ å’Œ $\\tilde p(x)$,å…¶å®šä¹‰åˆ†åˆ«ä¸º:\n\n$$\n\\begin {align}\n& \\tilde p(x, y) &= \\frac {cout(x, y)}{N} \\\\\n& \\tilde p(x)    &= \\frac {cout(x)}{N} \\\\\n\\end {align}\n$$\n\nå…¶ä¸­, $count(x,y)$ å’Œ $cont(x)$ åˆ†åˆ«è¡¨ç¤º $(x,y)$ å’Œ $x$ åœ¨è®­ç»ƒæ•°æ® $mathcal T$ ä¸­å‡ºç°çš„æ¬¡æ•°.\n\nåŒæ—¶å­˜åœ¨ä¸€ä¸ªæ¨¡å‹(å³æˆ‘ä»¬æœ€ç»ˆè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹, ç›®å‰æœªçŸ¥), è¯¥æ¨¡å‹ä¹Ÿå¯ä»¥é¢„æµ‹ $p(y|x)$. é‚£ä¹ˆåˆ©ç”¨Beyeså®šç† $(x, y)$ çš„é¢„æµ‹åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸º:\n\n$$\np(x, y) = p(x) p(y| x)\n$$\n\nä½†æ˜¯ä¸Šå¼ä¸­çš„ $p(x)$ æ˜¯æœªçŸ¥çš„. æ­¤å¤„ä½¿ç”¨ $\\tilde p(x)$ æ¥å¯¹ $p(x)$ è¿›è¡Œè¿‘ä¼¼, é‚£ä¹ˆ $(x, y)$ çš„é¢„æµ‹åˆ†å¸ƒå¯ä»¥è¿‘ä¼¼ä¸º:\n\n$$\np(x, y) \\approx \\tilde p(x) p(y| x)\n$$\n\nåˆ©ç”¨ä¸Šè¿°å®šä¹‰çš„ç‰¹å¾å‡½æ•°, ç»éªŒåˆ†å¸ƒå’Œé¢„æµ‹åˆ†å¸ƒ,å°±å¯ä»¥è¿›ä¸€æ­¥å®šä¹‰æˆ‘ä»¬æ‰€éœ€çš„çº¦æŸæ¡ä»¶äº†.\n\n\n## çº¦æŸæ¡ä»¶\n\nåœ¨æœ€å¤§ä¸Šæ¨¡å‹ä¸­, å»ºç«‹çº¦æŸæ¡ä»¶çš„åŸºç¡€æ˜¯æˆ‘ä»¬è®¤ä¸º:\n\n**æ¯ä¸€ä¸ªç‰¹å¾å¯¹åº”çš„ç‰¹å¾å‡½æ•° $f$, åœ¨ $\\mathcal T$ ä¸Šå…³äºç»éªŒåˆ†å¸ƒ $\\tilde p(x,y)$ çš„æ•°å­¦æœŸæœ› $E_{\\tilde p}(f)$ ä¸å®ƒä»¬åœ¨æ¨¡å‹ä¸­å…³äº $p(x,y)$ çš„æ•°å­¦æœŸ $E_p(f)$ æœ›ç›¸ç­‰.**\n\n$$\n\\begin {align}\nE_{\\tilde p}(f) & = \\sum_{(x,y) \\in \\mathcal T} \\tilde p(x, y)f(x, y) \\\\\nE_p(f)          & = \\sum_{(x,y) \\in \\mathcal T} p(x, y)f(x, y) \\\\\n                & = \\sum_{(x,y) \\in \\mathcal T} \\tilde p(x) p(y \\mid x)f(x, y) \\\\\n\\end {align}\n$$\n\nå³:\n\n$$\n\\begin {align}\n& E_{\\tilde p}(f) =  E_p(f) \\\\\n\\\\\n& \\sum_{(x,y) \\in \\mathcal T} \\tilde p(x, y)f(x, y) = \\sum_{(x,y) \\in \\mathcal T} \\tilde p(x) p(y \\mid x)f(x, y) \\\\\n\\end {align}\n$$\n\nå‡è®¾ä»è®­ç»ƒæ•°æ®ä¸­æŠ½å–äº† $n$ ä¸ªç‰¹å¾, ä¾¿æœ‰ $n$ ä¸ªç‰¹å¾å‡½æ•° $f_i, (i=1, 2, \\dots, n)$, ç›¸åº”çš„å°±æœ‰ $n$ ä¸ªçº¦æŸæ¡ä»¶.\n\n$$\n E_{\\tilde p}(f_i) =  E_p(f_i), (i=1, 2, \\dots, n)\n$$\n\n\n\n## æ¨¡å‹è¡¨ç¤º\n\nç»™å®šè®­ç»ƒæ•°æ® $\\mathcal T$, æˆ‘ä»¬çš„ç›®æ ‡æ˜¯:åˆ©ç”¨æœ€å¤§ç†µåŸç†é€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„åˆ†ç±»æ¨¡å‹, å³å¯¹äºä»»æ„ç»™å®šçš„è¾“å…¥ $x \\in \\mathcal X$,å¯ä»¥ä»¥æ¦‚ç‡ $p(y \\mid x)$ è¾“å‡º $y \\in \\mathcal Y$.\n\næœ€å¤§ç†µæ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ï¼Œå³åœ¨ç»™å®šçš„çº¦æŸä¸‹æ±‚ç›®æ ‡å‡½æ•°çš„æœ€å€¼é—®é¢˜ã€‚ ä¸Šé¢å·²ç»ä»‹ç»äº†çº¦æŸæ¡ä»¶ï¼ŒåŸºäºæœ€å¤§ç†µåŸç†æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°å³ä¸ºç†µçš„æœ€å¤§åŒ–ï¼Œå³ï¼š\n\n$$\n\\begin {align}\nH(p) = H(y \\mid x) & = -\\sum_{(x,y) \\in \\mathcal T} p(x, y) \\log p(y \\mid x) \\\\\n                   & = -\\sum_{(x,y) \\in \\mathcal T} p(x) p(y \\mid x) \\log p(y \\mid x) \\\\\n                   & = -\\sum_{(x,y) \\in \\mathcal T} \\tilde p(x) p(y \\mid x) \\log p(y \\mid x) \\\\\n\\end {align}\n$$\n\nç”±äº $p(x)$ æ˜¯æœªçŸ¥çš„. æ­¤å¤„ä½¿ç”¨ $\\tilde p(x)$ æ¥å¯¹ $p(x)$ è¿›è¡Œè¿‘ä¼¼ã€‚\n\nå¯¹ç»¼ä¸Šç»™å‡ºå½¢å¼åŒ–çš„æœ€å¤§ç†µæ¨¡å‹ï¼š\nå¯¹äºç»™å®šçš„è®­ç»ƒæ•°æ®$\\mathcal T$, ç‰¹å¾å‡½æ•° $f_i(x,y), i=1,2,â€¦,n$, æœ€å¤§ç†µæ¨¡å‹å°±æ˜¯æ±‚è§£:\n\n$$\n\\begin {align}\n\\max_p & H(p) & = -\\sum_{(x,y) \\in \\mathcal T} \\tilde p(x) p(y \\mid x) \\log p(y \\mid x) \\\\\ns.t.   &      & E_{\\tilde p}(f_i) =  E_p(f_i), (i=1, 2, \\dots, n)                       \\\\\n       &      & \\sum_y p(y \\mid x) = 1                                                  \\\\\n\\end {align}\n$$\n\nå…¶ä¸­çš„æ¡ä»¶ $\\sum_y p(y \\mid x)=1$ æ˜¯ä¸ºäº†ä¿è¯ $p(y \\mid x)$ æ˜¯ä¸€ä¸ªåˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒ.\n\n# å‚è€ƒèµ„æ–™\n\n[æœ€å¤§ç†µæ¨¡å‹ Maximum Entropy Model](https://www.cnblogs.com/ooon/p/5677098.html)\n\n[æœ€å¤§ç†µå­¦ä¹ ç¬”è®°](https://blog.csdn.net/itplus/article/details/26550597)\n\n\n","tags":["Machine Learning"],"categories":["Machine Learning"]},{"title":"nå…ƒæ¨¡å‹","url":"%2Fblog%2Fn-gram-model.html","content":"\n# æ¦‚è¿°\n\n## ç¬¦å·å®šä¹‰\n\n$L$ : è¯­è¨€, å¦‚ æ±‰è¯­, è‹±è¯­, æˆ–è€…ä¸€é—¨ä¸“é—¨çš„è¯­è¨€.\n\n$T$ : ä»è¯­è¨€$L$ ä¸­éšæœºæŠ½æ ·çš„æ ·æœ¬. \n\n$s$ : è¯­è¨€ä¸­çš„ä¸€ä¸ªå¥å­. \n\n## è¯­è¨€æ¨¡å‹(language model)\n\næ ¹æ®è¯­è¨€æ ·æœ¬ä¼°è®¡å‡ºçš„å¥å­çš„æ¦‚ç‡åˆ†å¸ƒ$P(s)â€‹$ç§°ä¸ºè¯­è¨€$ğ¿â€‹$çš„è¯­è¨€æ¨¡å‹. è¯­è¨€æ¨¡å‹ç»™å¥å­èµ‹ä»¥æ¦‚ç‡ï¼Œè¯­è¨€$Lâ€‹$ä¸­æ‰€æœ‰å¥å­çš„æ¦‚ç‡ä¹‹å’Œä¸º1. \n$$\n\\sum_{s \\in L} P(s) = 1\n$$\nè¯­è¨€æ¨¡å‹åº”ç”¨ä¸¾ä¾‹\n\n- è¯­éŸ³è¯†åˆ«\n  - I have **too** many books. ( âˆš )\n  - I have **to** many books. (Ã—)\n  - I have **two** many books. (Ã—)\n- æ±‰è¯­åˆ†è¯\n  - åˆ« **æŠŠ** **æ‰‹** ä¼¸ è¿› åˆ«äºº çš„ å£è¢‹ é‡Œ ( âˆš )\n  - åˆ« **æŠŠæ‰‹** ä¼¸ è¿› åˆ«äºº çš„ å£è¢‹ é‡Œ (Ã—)\n- æœºå™¨ç¿»è¯‘\n  æˆ‘å–œæ¬¢åƒè‹¹æœ â‡’\n  I like eating apple ( âˆš )\n  I eating like apple (Ã—)\n\n# è¯­è¨€å»ºæ¨¡\n\nç»™å®šè‡ªç„¶è¯­è¨€$Lâ€‹$ï¼Œ$p(ğ‘ )â€‹$æœªçŸ¥,  åˆ©ç”¨ç»™å®šçš„è¯­è¨€æ ·æœ¬ä¼°è®¡$p(ğ‘ )â€‹$çš„è¿‡ç¨‹è¢«ç§°ä½œè¯­è¨€å»ºæ¨¡. \n\nç»™å®šå¥å­$ğ‘  = w_1 w_2 \\dots w_ğ‘™$ ï¼Œå¦‚ä½•è®¡ç®—è¯¥å¥å­çš„æ¦‚ç‡: $p(ğ‘ )$\n\nç›´æ¥ç»Ÿè®¡è¯­æ–™åº“ä¸­å¥å­$ğ‘ â€‹$å‡ºç°çš„æ¬¡æ•°. \n\nåº”ç”¨é“¾å¼è§„åˆ™ï¼Œåˆ†è§£è®¡ç®—$p(ğ‘ )$\n$$\n\\begin {aligned}\np(ğ‘ ) &= p (w_1)p (w_2|w_1)p (w_3 | w_1w_2) \\dots p (w_l | w_1w_2â€¦ w_{lâˆ’1})  \\\\\n\\\\\n&=\\prod_{i=1} p(w_i|w_1w_2â€¦ w_{iâˆ’1})\n\\end {aligned}\n$$\n\nä¸¾ä¾‹å¦‚ä¸‹ï¼š\n\n$$\n\\begin {aligned}\n&p(john\\ read\\ a\\ book) \\\\\n& = p(john)\\times p(read\\ john) \\times p(a\\ john\\ read)\\times p(book|john\\ read\\ a)\n\\end{aligned}\n$$\n\näº‹å®ä¸Šä¸èƒ½ç”¨è¿™ç§æ–¹å¼è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡ï¼ŒåŸå› æœ‰ä¸¤ä¸ªï¼š\n\n- ç›´æ¥è¿™æ ·è®¡ç®—ä¼šå¯¼è‡´å‚æ•°ç©ºé—´è¿‡å¤§. ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å‚æ•°å°±æ˜¯æ‰€æœ‰çš„è¿™äº›æ¡ä»¶æ¦‚ç‡. \n  æ¯”å¦‚ï¼ŒæŒ‰ä¸Šé¢æ–¹å¼è®¡ç®—$P(w_5 |w_1 ,w_2 ,w_3 ,w_4 )â€‹$ï¼Œè¿™é‡Œæ¯ä¸ª$w_iâ€‹$å¯èƒ½çš„å–å€¼æœ‰$|V|â€‹$ä¸ª, å³ä¸€ä¸ªè¯å…¸å¤§å°. åˆ™è¯¥æ¨¡å‹çš„å‚æ•°ä¸ªæ•°æ˜¯$|V|^5â€‹$ï¼Œè€Œä¸”è¿™è¿˜ä¸åŒ…å«$P(w4 | w1, w2, w3)â€‹$çš„ä¸ªæ•°ï¼Œå¯ä»¥çœ‹åˆ°è¿™æ ·å»è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡ä¼šä½¿è¯­è¨€æ¨¡å‹å‚æ•°ä¸ªæ•°è¿‡å¤šè€Œæ— æ³•å®ç”¨. \n- æ•°æ®ç¨€ç–ä¸¥é‡. å­˜åœ¨å¤§é‡å¯èƒ½çš„å­—ç¬¦ä¸²æ˜¯åœ¨è¯­æ–™åº“ä¸­æœªå‡ºç°è¿‡çš„. \n\n# nå…ƒæ¨¡å‹\n\n## å®šä¹‰\n\nä¸ºäº†è§£å†³åƒæ•°ç©ºé—´è¿‡å¤§çš„é—®é¢˜. å¼•å…¥äº†é©¬å°”ç§‘å¤«å‡è®¾ï¼š**éšæ„ä¸€ä¸ªè¯å‡ºç°çš„æ¦‚ç‡åªä¸å®ƒå‰é¢å‡ºç°çš„æœ‰é™çš„ä¸€ä¸ªæˆ–è€…å‡ ä¸ªè¯æœ‰å…³. ** \n\n $w_iâ€‹$ çš„å‡ºç°åªä¸å…¶ä¹‹å‰çš„$nâˆ’1â€‹$ä¸ªè¯æœ‰å…³ï¼Œå³nå…ƒç»„(n-gram). \n$$\np(w_i|w_1w_2â€¦ w_{iâˆ’1}) = p(w_i|w_{i-n+1}w_{i-n+2}â€¦ w_{iâˆ’1})\n$$\næ­¤æ—¶ï¼š\n$$\n\\begin {aligned}\np(ğ‘ ) &= p (w_1)p (w_2|w_1)p (w_3 | w_1w_2) \\dots p (w_l | w_{i-n+1}w_{i-n+2}\\dots w_{lâˆ’1})  \\\\\n\\\\\n&=\\prod_{i=1}  p(w_i|w_{i-n+1}w_{i-n+2}\\dots w_{iâˆ’1})\n\\end {aligned}\n$$\næ ¹æ®$n$ çš„ä¸åŒå–å€¼å¯åˆ†ä¸º:\n\n- ä¸€å…ƒæ¨¡å‹(n=1, unigram)\n- äºŒå…ƒæ¨¡å‹(n=2, bigram)\n- ä¸‰å…ƒæ¨¡å‹(n=3, trigram)\n\n\n### nå…ƒæ¨¡å‹çš„å‚æ•°\n\n|         | å‚æ•°å½¢å¼                                 | å‚æ•°æ•°é‡           |\n| :-----: | :----------------------------------- | :------------- |\n| unigram | $p(w_i)$                             | $\\mid V\\mid$   |\n| bigram  | $p(w_i\\mid w_{i-1})$                 | $\\mid V\\mid^2$ |\n| trigram | $p(w_i\\mid w_{i-2}w_{i-1})$          | $\\mid V\\mid^3$ |\n| n-gram  | $p(w_i\\mid w_{i-n+1} \\dots w_{i-1})$ | $\\mid V\\mid^n$ |\n\n$w \\in V$, $V$åªè¯è¡¨ï¼Œ|V|ä»£è¡¨è¯è¡¨ä¸­è¯çš„æ•°é‡\nå¯ä»¥å‘ç°ï¼š\n- nè¶Šå¤§ï¼Œæ¨¡å‹éœ€è¦çš„å‚æ•°è¶Šå¤š\n- å‚æ•°æ•°é‡æŒ‡æ•°å¢é•¿\n\n\nå°ç»“ï¼š\n\nnå…ƒæ¨¡å‹è®¤ä¸ºï¼šå¥å­ä¸­å‰é¢å‡ºç°çš„è¯å¯¹åé¢å¯èƒ½å‡ºç°çš„è¯æœ‰å¾ˆå¼ºçš„é¢„ç¤ºä½œç”¨. \n\n$nâ€‹$è¶Šå¤§ï¼Œå†å²ä¿¡æ¯è¶Šå¤šï¼Œæ¨¡å‹è¶Šå‡†ç¡®. \n\n### nçš„é€‰æ‹©\n\n|       | n è¾ƒå¤§æ—¶              | n è¾ƒå°æ—¶       |\n| ----- | ------------------ | ----------- |\n| è¯­å¢ƒåŒºåˆ«æ€§ | æä¾›äº†æ›´å¤šçš„è¯­å¢ƒä¿¡æ¯ï¼Œè¯­å¢ƒæ›´å…·åŒºåˆ«æ€§ | å¢ƒä¿¡æ¯å°‘ï¼Œä¸å…·åŒºåˆ«æ€§  |\n| å‚æ•°    | å‚æ•°ä¸ªæ•°å¤šã€è®¡ç®—ä»£ä»·å¤§        | å‚æ•°ä¸ªæ•°å°‘ã€è®¡ç®—ä»£ä»·å° |\n| è®­ç»ƒè¯­æ–™  | éœ€è¦æ›´å¤šçš„è®­ç»ƒè¯­æ–™          | è®­ç»ƒè¯­æ–™æ— éœ€å¤ªå¤š    |\n| ç»“æœ    | å‚æ•°ä¼°è®¡ä¸å¯é             | å‚æ•°ä¼°è®¡å¯é       |\n\n# nå…ƒæ¨¡å‹æ„å»ºè¿‡ç¨‹\n\n1. æ•°æ®å‡†å¤‡: \n   - ç¡®å®šè®­ç»ƒè¯­æ–™\n   - å¯¹è¯­æ–™è¿›è¡Œè¯ä¾‹åŒ–(tokenization) æˆ–åˆ‡åˆ†\n   - å¥å­è¾¹ç•Œæ ‡è®°ï¼Œå¢åŠ ä¸¤ä¸ªç‰¹æ®Šçš„è¯<bos>å’Œ<eos> \n     I eat . â†’ <bos> I eat . <eos>\n     I sleep . â†’ <bos> I sleep . <eos>\n2. å‚æ•°ä¼°è®¡\n   åˆ©ç”¨è®­ç»ƒè¯­æ–™ï¼Œä¼°è®¡æ¨¡å‹å‚æ•°\n3. æ¨¡å‹è¯„ä»·\n\n## å‚æ•°ä¼°è®¡\n\nå¦‚ä½•è®¡ç®—å…¶ä¸­çš„æ¯ä¸€é¡¹æ¡ä»¶æ¦‚ç‡(å³å‚æ•°)å‘¢? **æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMaximum Likelihood Estimationï¼ŒMLEï¼‰**\n\nå‡è®¾è¯­æ–™åº“ä¸­å¥å­å’Œå¥å­äº’ç›¸ç‹¬ç«‹, æ˜¯ä»æœä»åˆ†å¸ƒä¸º$p(\\Theta) $çš„æ€»ä½“éšæœºæŠ½å–çš„. $\\Theta$ ä¸ºæ¨¡å‹å‚æ•°, å³ä¸Šé¢çš„æ¡ä»¶æ¦‚ç‡.\n\nä¸€ä¸ªå¥å­çš„æ¦‚ç‡ä¸º: $p(s \\mid \\Theta) $ .\n\næ•´ä¸ªè¯­æ–™åº“çš„æ¦‚ç‡(ä¼¼ç„¶å‡½æ•°):\n$$\np(T \\mid \\theta) = \\prod_{s_i \\in T} p(s \\mid \\Theta)\n$$\nä½¿è®­ç»ƒæ ·æœ¬ä¼¼ç„¶å€¼(æ¦‚ç‡)æœ€å¤§çš„å‚æ•°$\\Theta$ ä¸º\n$$\n\\Theta_{ML} = \\arg\\max p(T \\mid \\Theta)\n$$\nè¯¥ä¼˜åŒ–é—®é¢˜å…·æœ‰è§£æè§£, å…¶è§£è¡¨ç¤ºå¦‚ä¸‹:\n\nä»¤ $c(w_1w_2â€¦ w_n)â€‹$è¡¨ç¤º$nâ€‹$å…ƒç»„ $w_1,w_2â€¦ w_nâ€‹$ åœ¨è®­ç»ƒè¯­æ–™ä¸­å‡ºç°çš„æ¬¡æ•°. åˆ™ï¼š\n$$\np(w_n \\mid w_1â€¦ w_{nâˆ’1})=\\frac{c(w_1w_2 \\dots w_n)}{c(w_1w_2 \\dots w_{nâˆ’1})}\n$$\nè¯¥æ–¹æ³•ç§°ä¸º**ç›¸å¯¹é¢‘ç‡æ³•(relative frequency estimation)**.  \n\nå¦‚å¯¹äºå¦‚ä¸‹è®­ç»ƒè¯­æ–™ï¼š\n```\n<bos> John read Moby Dick <eos>\n<bos> Mary read a different book <eos>\n<bos> She read a book by Cher <eos>\n```\n\nä½¿ç”¨ç›¸å¯¹é¢‘ç‡å‘è®¡ç®—æ¨¡å‹å‚æ•°å¦‚ä¸‹ï¼š\n\n\n$$\n\\begin {aligned}\n& p( john \\mid bos) =\\frac {c( bos, john)}{c( bos )}= \\frac 13 \\\\\n& p (a \\mid read) =\\frac {c(read, a)}{c(read)}=\\frac 23 \\\\\n& p( eos \\mid book) =\\frac {c (book, eos)}{c (book)}=\\frac 12 \\\\\n& p( book \\mid a) =\\frac {c(a, book)}{c(a)}=\\frac 12 \\\\\n& p( read \\mid john) =\\frac {c(john, read)}{c (john)}=\\frac 11\n\\end {aligned}\n$$\n\n**note** : å–å¯¹æ•°é¿å…ä¸‹æº¢\n\n# æ¨¡å‹è¯„ä»·\n\nè¯­è¨€æ¨¡å‹å¸¸ç”¨çš„è¯„ä»·æŒ‡æ ‡æœ‰ä¸¤ä¸ª,  äº¤å‰ç†µ(Cross-Entropy)å’Œå›°æƒ‘åº¦(Perplexity)).\n\n## Cross-Entropy\n\nè¯­è¨€$L = (X_i) \\sim p(x)$ ä¸å…¶æ¨¡å‹qçš„äº¤å‰ç†µå®šä¹‰ä¸º:\n$$\nH(L, q) = - \\lim \\frac 1n \\sum_{x_1^n}p(x_1^n) \\log q(x_1^n)\n$$\n å…¶ä¸­ï¼š$x_1^n = x_1, \\dots, x_n$ ä¸ºè¯­è¨€$L$ ä¸­çš„å¥å­ï¼Œ $p(x_1^n)$ ä¸ºå¥å­$x_1^n$ åœ¨è¯­è¨€$L$ä¸­å‡ºç°çš„æ¦‚ç‡(çœŸå®)æ¦‚ç‡ï¼Œ$q(x_1^n)$ ä¸ºæ¨¡å‹$q$ å¯¹å¥å­ $x_1^n$ å‡ºçš„æ¦‚ç‡ä¼°è®¡. \n\nç°åœ¨ä»ç„¶æ— æ³•è®¡ç®—è¿™ä¸ªè¯­è¨€çš„äº¤å‰ç†µï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸çŸ¥é“çœŸå®æ¦‚ç‡$p(x_1^n)â€‹$ï¼Œä¸è¿‡å¯ä»¥å‡è®¾è¿™ç§è¯­è¨€æ˜¯ç†æƒ³çš„ï¼Œå³$nâ€‹$è¶‹äºæ— ç©·å¤§æ—¶ï¼Œå…¶å…¨éƒ¨ word çš„æ¦‚ç‡ä¹‹å’Œä¸º1. ä¹Ÿå°±æ˜¯è¯´ï¼Œæ ¹æ®ä¿¡æ¯è®ºçš„å®šç†ï¼šå‡å®šè¯­è¨€$Lâ€‹$ æ˜¯ç¨³æ€(stationary)éå†çš„(ergodic)éšæœºè¿‡ç¨‹ï¼Œ$Lâ€‹$ ä¸å…¶æ¨¡å‹$qâ€‹$çš„äº¤å‰ç†µè®¡ç®—å…¬å¼å°±å˜ä¸º:\n$$\nH(L, q) = - \\lim \\frac 1n  \\log q(x_1^n)\n$$\n\nä¸€èˆ¬åœ°ï¼Œåœ¨$n$è¶³å¤Ÿå¤§æ—¶æˆ‘ä»¬è¿‘ä¼¼åœ°é‡‡ç”¨å¦‚ä¸‹è®¡ç®—æ–¹æ³•ï¼š\n$$\nH(L, q) \\approx -  \\frac 1n  \\log q(x_1^n)\n$$\n\nåœ¨æµ‹è¯•è¯­æ–™ä¸Šæ¨å¯¼å¦‚ä¸‹:\n\nä»¤$T=w_1 w_2 \\dots w_N$ä¸ºæµ‹è¯•è¯­æ–™, æ­¤å¤„å‡è®¾$N$ æ˜¯æµ‹è¯•è¯­æ–™çš„æ–‡æœ¬é•¿åº¦, æ˜¯ä¸€ä¸ªè¶³å¤Ÿå¤§çš„å€¼. æ­¤æ—¶æ¨¡å‹$q$ åœ¨æµ‹è¯•è¯­æ–™ä¸Šçš„äº¤å‰ç†µå®šä¹‰ä¸º:\n$$\nH(T, q) = - \\frac 1N \\log q(T)\n$$\nå¯¹äºn-gramæ¨¡å‹æ¥è¯´ \n\n$$\n\\begin {align}\n& q(T) = \\prod_{w_1^n \\in V^n} q(w_n \\mid w_{1}^{n-1}) \\\\\n& p(w_1^n)= \\frac {c(w_1^n)}{N}\n\\end {align}\n$$\n\nåˆ™å…¶äº¤å‰ç†µå®šä¹‰ä¸º:\n\n$$\n\\begin {align}\nH_{\\mathrm{n-gram}}(T, q) &= - \\frac 1N \\log q(T)  \\\\ \n&= - \\frac 1N \\log  \\prod_{w_1^n \\in V^n} q(w_n \\mid w_{1}^{n-1}) \\\\\n&= - \\frac 1N \\sum_{w_1^n \\in V^n} \\log   q(w_n \\mid w_{1}^{n-1}) \\\\\n&= - \\sum_{w_1^n \\in V^n} \\frac 1N  \\log   q(w_n \\mid w_{1}^{n-1}) \\\\\n&= - \\sum_{w_1^n \\in V^n} p(w_1^n)  \\log   q(w_n \\mid w_{1}^{n-1}) \\\\\n\\end {align}\n$$\n\näº¤å‰ç†µè¶Šå°, è¯­è¨€æ¨¡å‹è´¨é‡è¶Šå¥½. \n\nä¾‹å¦‚unigram:\n$$\n\\left .\n\\begin {align}\nq(T) =  \\prod_{i=1}^{N} q(w_i) \\\\\np(w_i)= \\frac {c(w_i)}{N}\n\\end {align}\n\\right \\} \n\\Rightarrow H_1(T, q) = -  \\sum_{i=1}^{N} p(w_i) \\log q(w_i)\n$$\næ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹:\n$$\nH_1(T, q) = - \\frac 1N \\log q(T)  = - \\frac 1N \\log  \\prod_{i=1}^{N} q(w_i) =  -  \\sum_{i=1}^{N} \\frac 1N \\log q(w_i) = -  \\sum_{i=1}^{N} p(w_i) \\log q(w_i) \n$$\n## Perplexity\n\nç»™å®šè¯­è¨€$L$ çš„æ ·æœ¬$T=w_1 w_2 \\dots w_N$, è¯­è¨€ $L$ çš„å›°æƒ‘åº¦å®šä¹‰ä¸º:\n$$\n\\begin {align}\nPP_q &= 2^{H(L, q)} \\approx 2^{-  \\frac 1N  \\log q(x_1^N)} \\\\\n&=q(T) ^{-\\frac 1N}\n\\end {align}\n$$\nå¯¹äºn-gramæ¨¡å‹æ¥è¯´: $q(T) = \\prod_{w_1^n \\in V^n} q(w_n \\mid w_{1}^{n-1})$\n$$\nPP_q = \\{\\prod_{w_1^n \\in V^n} q(w_n \\mid w_{1}^{n-1}) \\} ^{-\\frac 1N}\n$$\n\n- å›°æƒ‘åº¦è¶Šå°, è¯­è¨€æ¨¡å‹è´¨é‡è¶Šå¥½. \n- ä»ä»¥ä¸Šæ¨å¯¼å¯ä»¥çœ‹å‡º äº¤å‰ç†µ å’Œ å›°æƒ‘åº¦ æœ¬è´¨æ˜¯ä¸€è‡´çš„. \n- åœ¨è®¾è®¡è¯­è¨€æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨å›°æƒ‘åº¦æ¥ä»£æ›¿äº¤å‰ç†µè¡¡é‡è¯­è¨€æ¨¡å‹çš„å¥½å. \n  æ¯”å¦‚ä½ å¯¹æ¨¡å‹è¿›è¡Œäº†ä¸€ç‰ˆæ”¹è¿›: \n  ä½¿ç”¨äº¤å‰ç†µè¯„ä»·æ—¶, äº¤å‰ç†µå‡å°äº† 9.9 - 9.1 =0.8\n  ä½¿ç”¨å›°æƒ‘åº¦è¯„ä»·æ—¶, å›°æƒ‘åº¦å‡å°äº† 950 - 540 = 410\n  ç»™è€æ¿æ±‡æŠ¥æ—¶ä½ ä¼šé€‰ç”¨å“ªä¸ª? åæ­£ä¸»æµçš„è®ºæ–‡éƒ½æ˜¯ç”¨çš„å›°æƒ‘åº¦ä½œä¸ºè¯„ä»·æŒ‡æ ‡. \n\n# æ•°æ®ç¨€ç–é—®é¢˜\n\nç”±äºè®­ç»ƒæ ·æœ¬ä¸è¶³è€Œå¯¼è‡´æ‰€ä¼°è®¡çš„åˆ†å¸ƒä¸å¯é çš„é—®é¢˜ï¼Œç§°ä¸ºæ•°æ®ç¨€ç–é—®é¢˜\n\nä¸¾ä¾‹å¦‚ä¸‹: æœ‰å¦‚ä¸‹è®­ç»ƒè¯­æ–™ï¼š\n\n```\n<bos> John read Moby Dick <eos>\n<bos> Mary read a different book <eos>\n<bos> She read a book by Cher <eos>\n```\nå¯¹äºä¸€ä¸ªæ–°çš„å¥å­Cher read a book. ç”±äºc(Cher read) = 0. æ‰€ä»¥$p(Cher\\ read\\ a\\ book)=0$ è¿™ä¸ªç»“è®ºæ˜¯ä¸åˆç†çš„. é—®é¢˜å‡ºåœ¨ç°æœ‰è¯­æ–™æ²¡æœ‰è¦†ç›–æ‰€æœ‰æƒ…å†µ.\n\nNLPæ•°æ®ç¨€ç–é—®é¢˜æ±‡æ€»å¦‚ä¸‹:\n\n- zipfå®šå¾‹: åœ¨è‡ªç„¶è¯­è¨€çš„è¯­æ–™åº“é‡Œï¼Œä¸€ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ä¸å®ƒåœ¨é¢‘ç‡è¡¨é‡Œçš„æ’åæˆåæ¯”.\n- è¯­è¨€ä¸­åªæœ‰å¾ˆå°‘çš„å¸¸ç”¨è¯, å¤§éƒ¨åˆ†è¯éƒ½æ˜¯ä½é¢‘è¯, å¤§å¤šæ•°è¯( nå…ƒç»„)åœ¨è¯­æ–™ä¸­çš„å‡ºç°æ˜¯ç¨€ç–çš„.\n- è¯çš„åˆ†å¸ƒæ˜¯é•¿å°¾åˆ†å¸ƒï¼Œnå…ƒç»„åˆ†å¸ƒäº¦æ˜¯å¦‚æ­¤.\n- è¯­æ–™åº“å¯ä»¥æä¾›å°‘é‡å¸¸ç”¨è¯( nå…ƒç»„)çš„å¯é æ ·æœ¬.\n- è¯­æ–™åº“æ— è®ºæ€ä¹ˆæ‰©å¤§, æ€»æ˜¯ä¼šå‡ºç°æœªè¦†ç›–çš„è¯æˆ–( nå…ƒç»„).\n- **è¯­æ–™åº“è§„æ¨¡æ‰©å¤§ï¼Œä¸»è¦æ˜¯é«˜é¢‘è¯è¯ä¾‹çš„å¢åŠ , æ‰©å¤§è¯­æ–™è§„æ¨¡ä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³ç¨€ç–é—®é¢˜.**\n\n\n**ç”±äºæ•°æ®ç¨€ç–ï¼ŒMLEä¼°è®¡å€¼ä¸æ˜¯ç†æƒ³çš„å‚æ•°ä¼°è®¡å€¼**\n\n## å¹³æ»‘æŠ€æœ¯\n\nè§£å†³æ•°æ®ç¨€ç–é—®é¢˜çš„æ–¹æ³•ä¸ºå¹³æ»‘æŠ€æœ¯(smoothing), å®ƒçš„åŸºæœ¬æ€æƒ³ä¸º: æŠŠåœ¨è®­ç»ƒæ ·æœ¬ä¸­å‡ºç°è¿‡çš„äº‹ä»¶(å¥å­)çš„æ¦‚ç‡é€‚å½“å‡å°, æŠŠå‡å°å¾—åˆ°çš„æ¦‚ç‡å€¼åˆ†é…ç»™è®­ç»ƒè¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„äº‹ä»¶(å¥å­). \n\næ ¹æ®æ¦‚ç‡çš„é‡æ–°åˆ†é…æ–¹æ³•çš„ä¸åŒ, å¹³æ»‘æŠ€æœ¯åˆ†ä¸ºä¸åŒçš„æ–¹æ³•. \n\n## ç®€å•å¹³æ»‘\n\nç®€å•å¹³æ»‘ è®¤ä¸ºæœªå‡ºç°çš„nå…ƒç»„æ˜¯ç­‰æ¦‚ç‡åˆ†å¸ƒ\n\nè¿™äº›æ–¹æ³•åŒ…æ‹¬ åŠ æ³•å¹³æ»‘ã€ç•™å­˜å¹³æ»‘ã€Good-Turingå¹³æ»‘\n\n### åŠ æ³•å¹³æ»‘\n\n####  Add-one\n\nAdd-oneå¹³æ»‘è§„å®šnå…ƒç»„æ¯”çœŸå®å‡ºç°æ¬¡æ•°å¤šä¸€æ¬¡.\n\n$$\n\\begin {aligned}\n& new\\_count(n-gram)  = count(n-gram) +1 \\\\\n\\\\\n& p_{ML}(w_n \\mid w_1â€¦ w_{nâˆ’1})=\\frac{c(w_1w_2 \\dots w_n)}{c(w_1w_2 \\dots w_{nâˆ’1})} \\\\\n\\\\\n& p_{+1}(w_n \\mid w_1â€¦ w_{nâˆ’1})=\\frac{c(w_1w_2 \\dots w_n)+1}{c(w_1w_2 \\dots w_{nâˆ’1})+ \\mid V\\mid} \\\\\n\\end {aligned}\n$$\n\n$ \\mid V\\mid$ ä¸ºè¯­æ–™åº“ä¸­è¯çš„æ•°é‡. \n\næ­¤æ—¶:\n\n- æ²¡æœ‰å‡ºç°çš„nå…ƒç»„çš„é¢‘ç‡æ˜¯1, å…·æœ‰ä¸€ä¸ªè¾ƒå°çš„æ¦‚ç‡.\n- å‡ºç°è¿‡çš„nå…ƒç»„çš„é¢‘ç‡+1, ä½†æ˜¯å…¶**æ¦‚ç‡å‡å°äº†**.\n\n\nä¸‹é¢ä¸¾ä¾‹è¯´æ˜Add-oneå¹³æ»‘å­˜åœ¨çš„é—®é¢˜.\n\nbi-gramå¹³æ»‘å‰åäºŒå…ƒç»„çš„é¢‘ç‡è®¡æ•°\n\n![](n-gram-model/add-one-counts.png)\n\nbi-gramå¹³æ»‘å‰åäºŒå…ƒç»„çš„æ¦‚ç‡ç»Ÿè®¡:\n\n![](n-gram-model/add-one-prob.png)\n\n\n\n- ç”±äºè®­ç»ƒè¯­æ–™ä¸­æœªå‡ºç°nå…ƒç»„æ•°é‡å¤ªå¤š, å¹³æ»‘å, æ‰€æœ‰æœªå‡ºç°çš„nå…ƒç»„å æ®äº†æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªå¾ˆå¤§çš„æ¯”ä¾‹(æ‰€æœ‰è“è‰²æ¦‚ç‡çš„å’Œ). å› æ­¤, åœ¨NLPä¸­, Add-oneç»™è®­ç»ƒè¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„nå…ƒç»„åˆ†é…äº†å¤ªå¤šçš„æ¦‚ç‡ç©ºé—´.  åŒæ—¶å¤§å¹…å‡å°äº†å‡ºç°è¿‡çš„nå…ƒç»„çš„æ¦‚ç‡.  è¿™æ ·åšæ˜¾ç„¶æ˜¯ä¸åˆç†çš„. \n\n\n- add-one å°†å‡ºç°åœ¨è®­ç»ƒè¯­æ–™ä¸­çš„é‚£äº›nå…ƒç»„, éƒ½å¢åŠ åŒæ ·çš„é¢‘åº¦å€¼, è¿™æ˜¯å¦å…¬å¹³? ä¸åˆç†\n- add-one è®¤ä¸ºæ‰€æœ‰æœªå‡ºç°çš„nå…ƒç»„æ¦‚ç‡ç›¸ç­‰, è¿™æ˜¯å¦åˆç†? ä¸åˆç†\n\nä¼˜ç‚¹: Very simple to implement\nç¼ºç‚¹: Takes away too much probability mass from seen events. Assigns too much total probability mass to unseen events.  å®é™…å®éªŒä¸­å‘ç°æœªå‡ºç°çš„n-gramå çš„æ¦‚ç‡å’Œ==99.96% .\n\n#### Add-K\n\nåœ¨Add-oneçš„åŸºç¡€ä¸Šåšäº†ä¸€ç‚¹å°æ”¹åŠ¨, åŸæœ¬æ˜¯åŠ ä¸€, ç°åœ¨åŠ ä¸Šä¸€ä¸ªå°äº1çš„å¸¸æ•°$K$. \n$$\np_{+K}(w_n \\mid w_1â€¦ w_{nâˆ’1})=\\frac{c(w_1w_2 \\dots w_n)+K}{c(w_1w_2 \\dots w_{nâˆ’1})+ K\\mid V\\mid}\n$$\n- Add-K æ•ˆæœæ¯”Add-oneå¥½, ä»ä¸ç†æƒ³.\n- ç¼ºç‚¹æ˜¯è¿™ä¸ªå¸¸æ•°Kä»ç„¶éœ€è¦äººå·¥ç¡®å®š, å¯¹äºä¸åŒçš„è¯­æ–™åº“$K$å¯èƒ½ä¸åŒ.\n\nç®€å•å¹³æ»‘æ–¹æ³•è¿˜æœ‰ ç•™å­˜å¹³æ»‘, [Good-Turingå¹³æ»‘](https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation) ç­‰.\n\n## ç»„åˆå¹³æ»‘(æ’å€¼å’Œå›é€€)\n\nç®€å•å¹³æ»‘è®¤ä¸ºæœªå‡ºç°çš„nå…ƒç»„ç­‰æ¦‚ç‡åˆ†å¸ƒ. ç»„åˆå¹³æ»‘.\n\né‚£ä¹ˆæœªå‡ºç°çš„nå…ƒç»„æ¦‚ç‡å‡åŒ€åˆ†å¸ƒï¼Œæ˜¯å¦åˆç†ï¼Ÿå…¶å®æ˜¯ä¸åˆç†çš„.\nä¾‹å¦‚ï¼Œå‡è®¾ä¸‹é¢ä¸‰ä¸ªbigramå‡æœªåœ¨è®­ç»ƒé¢„æ–™ä¸­å‡ºç°\n> journal of \n> journal from \n> journal never \n\nä½†æ˜¯æ ¹æ®ç»éªŒ  journal of  åº”è¯¥æ›´å¸¸è§ï¼Œæ¦‚ç‡åº”è¯¥æ›´å¤§.  \n\nåŒæ—¶æˆ‘ä»¬å‘ç°è¶Šæ˜¯é«˜é˜¶nå…ƒç»„, ç¨€ç–é—®é¢˜è¶Šä¸¥é‡. \n\nç»„åˆå¹³æ»‘çš„æ€æƒ³å°±æ˜¯å‚è€ƒä½é˜¶nå…ƒç»„ä¼°ç®—é«˜é˜¶nå…ƒç»„çš„æ¦‚ç‡åˆ†å¸ƒ. æ¯”å¦‚: é€šè¿‡ç»Ÿè®¡å‘ç°unigram ä¸­ \n\n- â€œofâ€  é¢‘ç‡é«˜äºâ€œfromâ€ å’Œ â€œneverâ€ \n- æ¦‚ç‡p (of) >p (from) >p(never)\n\n\næ‰€ä»¥  journal of çš„æ¦‚ç‡å¤§ä¸å¦å¤–ä¸¤ä¸ª. \n\nç»„åˆå¹³æ»‘åˆ†ä¸º æ’å€¼å’Œå›é€€ ä¸¤ç±».\n\n### æ’å€¼å¹³æ»‘\n\n#### ç®€å•çº¿æ€§æ’å€¼å¹³æ»‘\n\nå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œæ—¢ç„¶é«˜é˜¶ç»„åˆå¯èƒ½å‡ºç°æ¬¡æ•°ä¸º0ï¼Œé‚£ç¨å¾®ä½é˜¶ä¸€ç‚¹çš„ç»„åˆæ€»æœ‰ä¸ä¸º0çš„. å¦‚ä¸‹æ˜¯ä¸€ä¸ªä¸‰é˜¶ç»„åˆï¼Œå‡è®¾$p(w_n|w_{nâˆ’1}w_{nâˆ’2})=0â€‹$ï¼Œè€Œ$p(w_n|w_{nâˆ’1})>0â€‹$ä¸”$p(w_n)>0â€‹$ï¼Œåˆ™åŠ æƒå¹³å‡åçš„æ¦‚ç‡ä¸ä¸º$0â€‹$ï¼Œä»è€Œè¾¾åˆ°å¹³æ»‘çš„æ•ˆæœ. \n$$\n\\hat p(w_n|w_{nâˆ’1}w_{nâˆ’2})=\\lambda_3 p(w_n|w_{nâˆ’1}w_{nâˆ’2})+\\lambda_2 p(w_n|w_{nâˆ’1})+\\lambda_1 p(w_n)\n$$\nå…¶ä¸­: $\\sum_i\\lambda_i = 1$ ,  $\\lambda_i$ æ ¹æ®ç»éªŒå–å€¼,  ä¹Ÿå¯ä»¥åŸºäºå¼€å‘é›†è‡ªåŠ¨å­¦ä¹ \n\nç®€å•çº¿æ€§æ’å€¼å¹³æ»‘å¯ä»¥è¡¨ç¤ºä¸ºå¦‚ä¸‹é€’å½’å¼:\n$$\np_{interp} (w_i \\mid w_{i-n+1}^{i-1}) = \\lambda_i p_{MLE} (w_i \\mid  w_{i-n+1}^{i-1}) + (1- \\lambda_i) p_{interp}(w_i \\mid  w_{i-n+2}^{i-1})\n$$\n#### Jelinek-Mercerå¹³æ»‘\n\nç®€å•çº¿å½¢æ’å€¼å¹³æ»‘ä¸­ï¼Œæƒå€¼ $\\lambda_i$ ä¸€æ—¦ç¡®å®šå°±å›ºå®šä¸å˜äº†.\n\n Jelinek-Mercerå¹³æ»‘è®¤ä¸ºè‹¥é«˜é˜¶nå…ƒç»„å¯é ï¼Œ$\\lambda$åº”è¯¥å¤§;  è‹¥é«˜é˜¶nå…ƒç»„ä¸å¯é ï¼Œ$\\lambda$åº”è¯¥å°. nå…ƒç»„çš„å¯é ç¨‹åº¦ä¸ n å…ƒç»„$w_{i-n+1}^{i}$å…·ä½“çš„å†å²$w_{i-n+1}^{i-1}$å‡ºç°çš„é¢‘æ¬¡æ­£ç›¸å…³. å³: $\\lambda$åº”è¯¥å’Œ n å…ƒç»„$w_{i-n+1}^{i}$å…·ä½“çš„å†å²$w_{i-n+1}^{i-1}$çš„é¢‘æ¬¡å…³è”èµ·æ¥.\n\nJelinek-Mercerå¹³æ»‘å¯ä»¥è¡¨ç¤ºä¸ºé€’å½’çš„æ’å€¼æ¨¡å‹\n$$\np_{JM} (w_i \\mid w_{i-n+1}^{i-1}) = \\lambda_{w_{i-n+1}^{i-1}} p_{MLE} (w_i \\mid  w_{i-n+1}^{i-1}) + (1- \\lambda_{w_{i-n+1}^{i-1}}) p_{JM}(w_i \\mid  w_{i-n+2}^{i-1})\n$$\né€’å½’çš„ç»ˆæ­¢æ¡ä»¶: \n\n1. ç»ˆæ­¢äºå¹³æ»‘åçš„ä¸€å…ƒæ¨¡å‹ $p_{smooth}(w_i)$. \n\n\n2. ç»ˆæ­¢äº0å…ƒæ¨¡å‹. 0å…ƒæ¨¡å‹å®šä¹‰ä¸ºå‡åŒ€åˆ†å¸ƒ: $\\frac {1}{\\mid V \\mid}$. \n\n$\\lambda_{w_{i-n+1}^{i-1}}$çš„ä¼°è®¡æ–¹æ³•\n\n1. ç»Ÿè®¡é¢‘æ¬¡$c(w_{i-n+1}^{i-1})$, æŒ‰ç…§é¢‘æ¬¡å°†$c(w_{i-n+1}^{i-1})$åˆ†ä¸ºè‹¥å¹²(k)ä¸ªåŒºé—´.\n2. æ¯ä¸ªåŒºé—´å¯¹åº”ä¸€ä¸ª $\\lambda_{w_{i-n+1}^{i-1}}$ å€¼, å…±$k$ä¸ª.\n3. é€šè¿‡æµ·é‡è®­ç»ƒæ•°æ®è®­ç»ƒç¡®å®š $k$ä¸ª $\\lambda$ å€¼.\n\n### å›é€€å¹³æ»‘\n\nå›é€€å¹³æ»‘è®¤ä¸ºåœ¨é«˜é˜¶æ¨¡å‹å¯é æ—¶ï¼Œå°½å¯èƒ½ä½¿ç”¨é«˜é˜¶æ¨¡å‹,  å¦åˆ™, æ‰ä½¿ç”¨ä½é˜¶æ¨¡å‹.\n\nå›é€€æ¨¡å‹çš„ä¸€èˆ¬å½¢å¼å¦‚ä¸‹:\n$$\np_{smooth} (w_i \\mid w_{i-n+1}^{i-1}) =\n\\left \\{\n\t\\begin {aligned}\n\t\t&p_{MLE} (w_i \\mid  w_{i-n+1}^{i-1})    & c( w_{i-n+1}^{i-1}) > 0 \\\\\n\t   \t\t&\\alpha_{w_{i-n+1}^{i-1}} p_{smooth}(w_i \\mid  w_{i-n+2}^{i-1})  &  otherwise\\\\\n\t\\end {aligned}\n\\right .\n$$\nå…¶ä¸­: $\\alpha_{w_{i-n+1}^{i-1}}$ æ˜¯ä¸ºäº†ä¿è¯ $\\sum_i p_{smooth} (w_i \\mid w_{i-n+1}^{i-1}) = 1$ çš„å½’ä¸€åŒ–å‚æ•°.\n\nå¸¸è§çš„å›é€€å¹³æ»‘æ¨¡å‹å¦‚ä¸‹:\n\n- [Katzå¹³æ»‘](https://en.wikipedia.org/wiki/Katz%27s_back-off_model)\n\n\n- ç»å¯¹å‡å€¼æ³•\n\n\n- [Kneserâ€“Neyå¹³æ»‘](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) :ç»å¯¹å‡å€¼æ³•çš„æ”¹è¿›\n\n\n\nå›é€€æ¨¡å‹å’Œæ’å€¼æ¨¡å‹æ˜¯ä¸¤ç§æ¯”è¾ƒç›¸ä¼¼çš„å¹³æ»‘æ–¹æ³•, å…ˆæ€»ç»“å…¶å¼‚åŒå¦‚ä¸‹:\n\n- åœ¨å›é€€æ¨¡å‹å’Œæ’å€¼æ¨¡å‹ä¸­ï¼Œå½“é«˜é˜¶n å…ƒç»„æœªå‡ºç°æ—¶ï¼Œä½¿ç”¨ä½é˜¶nå…ƒç»„ä¼°ç®—é«˜é˜¶nå…ƒç»„çš„æ¦‚ç‡åˆ†å¸ƒ\n- åœ¨å›é€€æ¨¡å‹ä¸­ï¼Œé«˜é˜¶nå…ƒç»„ä¸€æ—¦å‡ºç°ï¼Œå°±ä¸å†ä½¿ç”¨ä½é˜¶nå…ƒç»„è¿›è¡Œä¼°è®¡\n- åœ¨æ’å€¼æ¨¡å‹ä¸­ï¼Œæ— è®ºé«˜é˜¶nå…ƒç»„æ˜¯å¦å‡ºç°ï¼Œä½é˜¶nå…ƒç»„éƒ½ä¼šè¢«ç”¨æ¥ä¼°è®¡é«˜é˜¶nå…ƒç»„çš„æ¦‚ç‡ä¼°å€¼\n\n# æ€»ç»“\n\nè¯­è¨€æ¨¡å‹ : è¯­è¨€ä¸­æ¯ä¸ªå¥å­çš„æ¦‚ç‡\n\nè¯­è¨€å»ºæ¨¡\n\nn-gram <- é©¬å°”ç§‘å¤«å‡è®¾\n\nâ€‹\tnçš„é€‰æ‹©\n\nâ€‹\tå‚æ•°ä¼°è®¡(ç›¸å¯¹é¢‘ç‡æ³•) <- MLE\n\nâ€‹\tå¹³æ»‘\n\n# å‚è€ƒèµ„æ–™\n\nhttps://courses.engr.illinois.edu/cs447/fa2017/Slides/Lecture04.pdf\n\n[è‡ªç„¶è¯­è¨€å¤„ç†NLPä¸­çš„N-gramæ¨¡å‹](https://blog.csdn.net/songbinxu/article/details/80209197)\n\n\n\n\n\n","tags":["NLP"],"categories":["NLP"]},{"title":"Simhash","url":"%2Fblog%2FSimhash.html","content":"\nsimhashæ˜¯ä¸€ç§**å±€éƒ¨æ•æ„Ÿhash**ã€‚å³ï¼Œå‡å®šä¸¤ä¸ªå­—ç¬¦ä¸²å…·æœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ï¼Œåœ¨hashä¹‹åï¼Œä»ç„¶èƒ½ä¿æŒè¿™ç§ç›¸ä¼¼æ€§ï¼Œå°±ç§°ä¹‹ä¸ºå±€éƒ¨æ•æ„Ÿhashã€‚simhashè¢«Googleç”¨æ¥åœ¨æµ·é‡æ–‡æœ¬ä¸­å»é‡ã€‚\n\n# SimHashç®—æ³•æ€æƒ³\n\nå‡è®¾æˆ‘ä»¬æœ‰æµ·é‡çš„æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®æ–‡æœ¬å†…å®¹å°†å®ƒä»¬è¿›è¡Œå»é‡ã€‚å¯¹äºæ–‡æœ¬å»é‡è€Œè¨€ï¼Œç›®å‰æœ‰å¾ˆå¤šNLPç›¸å…³çš„ç®—æ³•å¯ä»¥åœ¨å¾ˆé«˜ç²¾åº¦ä¸Šæ¥è§£å†³ï¼Œä½†æ˜¯æˆ‘ä»¬ç°åœ¨å¤„ç†çš„æ˜¯å¤§æ•°æ®ç»´åº¦ä¸Šçš„æ–‡æœ¬å»é‡ï¼Œè¿™å°±å¯¹ç®—æ³•çš„æ•ˆç‡æœ‰ç€å¾ˆé«˜çš„è¦æ±‚ã€‚è€Œå±€éƒ¨æ•æ„Ÿhashç®—æ³•å¯ä»¥å°†åŸå§‹çš„æ–‡æœ¬å†…å®¹æ˜ å°„ä¸ºæ•°å­—ï¼ˆhashç­¾åï¼‰ï¼Œè€Œä¸”è¾ƒä¸ºç›¸è¿‘çš„æ–‡æœ¬å†…å®¹å¯¹åº”çš„hashç­¾åä¹Ÿæ¯”è¾ƒç›¸è¿‘ã€‚SimHashç®—æ³•æ˜¯Googleå…¬å¸è¿›è¡Œæµ·é‡ç½‘é¡µå»é‡çš„é«˜æ•ˆç®—æ³•ï¼Œå®ƒé€šè¿‡å°†åŸå§‹çš„æ–‡æœ¬æ˜ å°„ä¸º64ä½çš„äºŒè¿›åˆ¶æ•°å­—ä¸²ï¼Œç„¶åé€šè¿‡æ¯”è¾ƒäºŒè¿›åˆ¶æ•°å­—ä¸²çš„å·®å¼‚è¿›è€Œæ¥è¡¨ç¤ºåŸå§‹æ–‡æœ¬å†…å®¹çš„å·®å¼‚ã€‚\n\n# SimHashæµç¨‹å®ç°\n\nsimhashæ˜¯ç”± Charikar åœ¨2002å¹´æå‡ºæ¥çš„ï¼Œå‚è€ƒ [ã€ŠSimilarity estimation techniques from rounding algorithmsã€‹](http://dl.acm.org/citation.cfm?id=509965) ã€‚ è¿™ä¸ªç®—æ³•ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š\n\n1. åˆ†è¯ï¼šå°†Docè¿›è¡Œå…³é”®è¯æŠ½å–(å…¶ä¸­åŒ…æ‹¬åˆ†è¯å’Œè®¡ç®—æƒé‡)ï¼ŒæŠ½å–å‡ºnä¸ª(å…³é”®è¯ï¼Œæƒé‡)å¯¹ï¼Œ å³å›¾ä¸­çš„å¤šä¸ª`(feature, weight)`ã€‚ è®°ä¸º `feature_weight_pairs = [fw1, fw2 â€¦ fwn]`ï¼Œå…¶ä¸­ `fwn = (feature_n,weight_n)`ã€‚\n2. hashï¼šå¯¹æ¯ä¸ª**feature_weight_pairs**ä¸­çš„`feature`è¿›è¡Œhashã€‚ å›¾ä¸­å‡è®¾hashç”Ÿæˆçš„ä½æ•°bits_count = 6ã€‚\n3. åŠ æƒåˆå¹¶ï¼šç„¶åå¯¹**hash_weight_pairs**è¿›è¡Œä½çš„çºµå‘ç´¯åŠ ï¼Œå¦‚æœè¯¥ä½æ˜¯1ï¼Œåˆ™`+weight`,å¦‚æœæ˜¯0ï¼Œåˆ™`-weight`ï¼Œæœ€åç”Ÿæˆbits_countä¸ªæ•°å­—ï¼Œå¦‚å›¾æ‰€ç¤ºæ˜¯`[13, 108, -22, -5, -32, 55]`, è¿™é‡Œäº§ç”Ÿçš„å€¼å’Œhashå‡½æ•°æ‰€ç”¨çš„ç®—æ³•ç›¸å…³ã€‚\n4. signï¼š`[13,108,-22,-5,-32,55] -> 110001`è¿™ä¸ªå°±å¾ˆç®€å•å•¦ï¼Œæ­£1è´Ÿ0ã€‚\n\nç°åœ¨é€šè¿‡è¿™æ ·çš„è½¬æ¢ï¼Œæˆ‘ä»¬æŠŠåº“é‡Œçš„æ–‡æœ¬éƒ½è½¬æ¢ä¸ºsimhash ä»£ç ï¼Œå¹¶è½¬æ¢ä¸ºlongç±»å‹å­˜å‚¨ï¼Œç©ºé—´å¤§å¤§å‡å°‘ã€‚\n\n# SimHashç­¾åè·ç¦»è®¡ç®—\n\nä½¿ç”¨æµ·æ˜è·ç¦»ï¼ˆHamming distanceï¼‰å°±å¯ä»¥è®¡ç®—å‡ºä¸¤ä¸ªsimhashçš„ç›¸ä¼¼åº¦ã€‚ä¸¤ä¸ªsimhashå¯¹åº”äºŒè¿›åˆ¶ï¼ˆ01ä¸²ï¼‰å–å€¼ä¸åŒçš„æ•°é‡ç§°ä¸ºè¿™ä¸¤ä¸ªsimhashçš„æµ·æ˜è·ç¦»ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š **1**01**01** å’Œ **0**01**10** ä»ç¬¬ä¸€ä½å¼€å§‹ä¾æ¬¡æœ‰ç¬¬ä¸€ä½ã€ç¬¬å››ã€ç¬¬äº”ä½ä¸åŒï¼Œåˆ™æµ·æ˜è·ç¦»ä¸º3ã€‚å¯¹äºäºŒè¿›åˆ¶å­—ç¬¦ä¸²çš„aå’Œbï¼Œæµ·æ˜è·ç¦»ä¸ºç­‰äºåœ¨a XOR bè¿ç®—ç»“æœä¸­1çš„ä¸ªæ•°ï¼ˆæ™®éç®—æ³•ï¼‰ã€‚\n\n# SimHashå­˜å‚¨å’Œç´¢å¼•\n\nç»è¿‡simhashæ˜ å°„ä»¥åï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªæ–‡æœ¬å†…å®¹å¯¹åº”çš„simhashç­¾åï¼Œè€Œä¸”ä¹Ÿç¡®å®šäº†åˆ©ç”¨æ±‰æ˜è·ç¦»æ¥è¿›è¡Œç›¸ä¼¼åº¦çš„è¡¡é‡ã€‚é‚£å‰©ä¸‹çš„å·¥ä½œå°±æ˜¯ä¸¤ä¸¤è®¡ç®—æˆ‘ä»¬å¾—åˆ°çš„simhashç­¾åçš„æ±‰æ˜è·ç¦»äº†ï¼Œè¿™åœ¨ç†è®ºä¸Šæ˜¯å®Œå…¨æ²¡é—®é¢˜çš„ï¼Œä½†æ˜¯è€ƒè™‘åˆ°æˆ‘ä»¬çš„æ•°æ®æ˜¯æµ·é‡çš„è¿™ä¸€ç‰¹ç‚¹ï¼Œæˆ‘ä»¬æ˜¯å¦åº”è¯¥è€ƒè™‘ä½¿ç”¨ä¸€äº›æ›´å…·æ•ˆç‡çš„å­˜å‚¨å‘¢ï¼Ÿå…¶å®SimHashç®—æ³•è¾“å‡ºçš„simhashç­¾åå¯ä»¥ä¸ºæˆ‘ä»¬å¾ˆå¥½å»ºç«‹ç´¢å¼•ï¼Œä»è€Œå¤§å¤§å‡å°‘ç´¢å¼•çš„æ—¶é—´ï¼Œé‚£åˆ°åº•æ€ä¹ˆå®ç°å‘¢ï¼Ÿ\n\næˆ‘ä»¬ä½¿ç”¨çš„æ–¹æ³•å°±æ˜¯ç±»ä¼¼äºhashmapçš„æ–¹æ¡ˆã€‚åœ¨hashmapä¸­æˆ‘ä»¬è¦æŸ¥æ‰¾ä¸€ä¸ªkeyå€¼æ—¶ï¼Œé€šè¿‡ä¼ å…¥ä¸€ä¸ªkeyå°±å¯ä»¥å¾ˆå¿«çš„è¿”å›ä¸€ä¸ªvalueï¼Œhashmapçš„å†…éƒ¨ç»“æ„å¦‚ä¸‹ï¼š\n\n![](Simhash/hashmap.png)\n\nhashmapçš„è¿ä½œåŸç†ï¼šå¦‚æœæˆ‘ä»¬éœ€è¦å¾—åˆ°keyå¯¹åº”çš„valueï¼Œéœ€è¦ç»è¿‡è¿™äº›è®¡ç®—ï¼Œä¼ å…¥keyï¼Œè®¡ç®—keyçš„hashcodeï¼Œå¾—åˆ°7çš„ä½ç½®ï¼›å‘ç°7ä½ç½®å¯¹åº”çš„valueè¿˜æœ‰å¥½å‡ ä¸ªï¼Œå°±é€šè¿‡é“¾è¡¨æŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°v72ã€‚å…¶å®é€šè¿‡è¿™ä¹ˆåˆ†æï¼Œå¦‚æœæˆ‘ä»¬çš„hashcodeè®¾ç½®çš„ä¸å¤Ÿå¥½ï¼Œhashmapçš„æ•ˆç‡ä¹Ÿä¸è§å¾—é«˜ã€‚\n\nå€Ÿé‰´è¿™ä¸ªç®—æ³•ï¼Œæ¥è®¾è®¡æˆ‘ä»¬çš„simhashæŸ¥æ‰¾ã€‚é€šè¿‡é¡ºåºæŸ¥æ‰¾è‚¯å®šæ˜¯ä¸è¡Œçš„ï¼Œèƒ½å¦åƒhashmapä¸€æ ·å…ˆé€šè¿‡é”®å€¼å¯¹çš„æ–¹å¼å‡å°‘é¡ºåºæ¯”è¾ƒçš„æ¬¡æ•°ã€‚çœ‹ä¸‹å›¾ï¼š\n\n![](Simhash/simhashindex.png)\n\n**å­˜å‚¨**ï¼š\n1ã€å°†ä¸€ä¸ª64ä½çš„simhash codeæ‹†åˆ†æˆ4ä¸ª16ä½çš„äºŒè¿›åˆ¶ç ã€‚ï¼ˆå›¾ä¸Šçº¢è‰²çš„16ä½ï¼‰\n2ã€åˆ†åˆ«æ‹¿ç€4ä¸ª16ä½äºŒè¿›åˆ¶ç æŸ¥æ‰¾å½“å‰å¯¹åº”ä½ç½®ä¸Šæ˜¯å¦æœ‰å…ƒç´ ã€‚ï¼ˆæ”¾å¤§åçš„16ä½ï¼‰\n3ã€å¯¹åº”ä½ç½®æ²¡æœ‰å…ƒç´ ï¼Œç›´æ¥è¿½åŠ åˆ°é“¾è¡¨ä¸Šï¼›å¯¹åº”ä½ç½®æœ‰åˆ™ç›´æ¥è¿½åŠ åˆ°é“¾è¡¨å°¾ç«¯ã€‚ï¼ˆå›¾ä¸Šçš„ S1 â€” SNï¼‰\n\n**æŸ¥æ‰¾**ï¼š\n1ã€å°†éœ€è¦æ¯”è¾ƒçš„simhash codeæ‹†åˆ†æˆ4ä¸ª16ä½çš„äºŒè¿›åˆ¶ç ã€‚\n2ã€åˆ†åˆ«æ‹¿ç€4ä¸ª16ä½äºŒè¿›åˆ¶ç æ¯ä¸€ä¸ªå»æŸ¥æ‰¾simhashé›†åˆå¯¹åº”ä½ç½®ä¸Šæ˜¯å¦æœ‰å…ƒç´ ã€‚\n2ã€å¦‚æœæœ‰å…ƒç´ ï¼Œåˆ™æŠŠé“¾è¡¨æ‹¿å‡ºæ¥é¡ºåºæŸ¥æ‰¾æ¯”è¾ƒï¼Œç›´åˆ°simhashå°äºä¸€å®šå¤§å°çš„å€¼ï¼Œæ•´ä¸ªè¿‡ç¨‹å®Œæˆã€‚\n\n**åŸç†**ï¼š\nå€Ÿé‰´hashmapç®—æ³•æ‰¾å‡ºå¯ä»¥hashçš„keyå€¼ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„simhashæ˜¯å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼Œè¿™ä¸ªç®—æ³•çš„ç‰¹ç‚¹æ˜¯åªè¦ç›¸ä¼¼çš„å­—ç¬¦ä¸²åªæœ‰ä¸ªåˆ«çš„ä½æ•°æ˜¯æœ‰å·®åˆ«å˜åŒ–ã€‚é‚£è¿™æ ·æˆ‘ä»¬å¯ä»¥æ¨æ–­ä¸¤ä¸ªç›¸ä¼¼çš„æ–‡æœ¬ï¼Œè‡³å°‘æœ‰16ä½çš„simhashæ˜¯ä¸€æ ·çš„ã€‚å…·ä½“é€‰æ‹©16ä½ã€8ä½ã€4ä½ï¼Œå¤§å®¶æ ¹æ®è‡ªå·±çš„æ•°æ®æµ‹è¯•é€‰æ‹©ï¼Œè™½ç„¶æ¯”è¾ƒçš„ä½æ•°è¶Šå°è¶Šç²¾å‡†ï¼Œä½†æ˜¯ç©ºé—´ä¼šå˜å¤§ã€‚åˆ†ä¸º4ä¸ª16ä½æ®µçš„å­˜å‚¨ç©ºé—´æ˜¯å•ç‹¬simhashå­˜å‚¨ç©ºé—´çš„4å€ã€‚ä¹‹å‰ç®—å‡º5000wæ•°æ®æ˜¯ 382 Mbï¼Œæ‰©å¤§4å€1.5Gå·¦å³ï¼Œè¿˜å¯ä»¥æ¥å—ï¼šï¼‰\n\né€šè¿‡è¿™æ ·è®¡ç®—ï¼Œæˆ‘ä»¬çš„simhashæŸ¥æ‰¾è¿‡ç¨‹å…¨éƒ¨é™åˆ°äº†1æ¯«ç§’ä»¥ä¸‹ã€‚å°±åŠ äº†ä¸€ä¸ªhashæ•ˆæœè¿™ä¹ˆå‰å®³ï¼Ÿæˆ‘ä»¬å¯ä»¥ç®—ä¸€ä¸‹ï¼ŒåŸæ¥æ˜¯5000wæ¬¡é¡ºåºæ¯”è¾ƒï¼Œç°åœ¨æ˜¯å°‘äº†2çš„16æ¬¡æ–¹æ¯”è¾ƒï¼Œå‰é¢16ä½å˜æˆäº†hashæŸ¥æ‰¾ã€‚åé¢çš„é¡ºåºæ¯”è¾ƒçš„ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ 2^16 = 65536ï¼Œ 5000w/65536 = 763 æ¬¡ã€‚ã€‚ã€‚ã€‚å®é™…æœ€åé“¾è¡¨æ¯”è¾ƒçš„æ•°æ®ä¹Ÿæ‰ 763æ¬¡ï¼æ‰€ä»¥æ•ˆç‡å¤§å¤§æé«˜ï¼\n\nåˆ°ç›®å‰ç¬¬ä¸€ç‚¹é™åˆ°3.6æ¯«ç§’ã€æ”¯æŒ5000wæ•°æ®ç›¸ä¼¼åº¦æ¯”è¾ƒåšå®Œäº†ã€‚è¿˜æœ‰ç¬¬äºŒç‚¹åŒä¸€æ—¶åˆ»å‘å‡ºçš„æ–‡æœ¬å¦‚æœé‡å¤ä¹Ÿåªèƒ½ä¿ç•™ä¸€æ¡å’ŒçŸ­æ–‡æœ¬ç›¸è¯†åº¦æ¯”è¾ƒæ€ä¹ˆè§£å†³ã€‚å…¶å®ä¸Šé¢çš„é—®é¢˜è§£å†³äº†ï¼Œè¿™ä¸¤ä¸ªå°±ä¸æ˜¯ä»€ä¹ˆé—®é¢˜äº†ã€‚\n\n- ä¹‹å‰çš„è¯„ä¼°ä¸€ç›´éƒ½æ˜¯æŒ‰ç…§çº¿æ€§è®¡ç®—æ¥ä¼°è®¡çš„ï¼Œå°±ç®—æœ‰å¤šçº¿ç¨‹æäº¤ç›¸ä¼¼åº¦è®¡ç®—æ¯”è¾ƒï¼Œæˆ‘ä»¬æä¾›ç›¸ä¼¼åº¦è®¡ç®—æœåŠ¡å™¨ä¹Ÿéœ€è¦çº¿æ€§è®¡ç®—ã€‚æ¯”å¦‚åŒæ—¶å®¢æˆ·ç«¯å‘é€è¿‡æ¥ä¸¤æ¡éœ€è¦æ¯”è¾ƒç›¸ä¼¼åº¦çš„è¯·æ±‚ï¼Œåœ¨æœåŠ¡å™¨è¿™è¾¹éƒ½è¿›è¡Œäº†ä¸€ä¸ªæ’é˜Ÿå¤„ç†ï¼Œä¸€ä¸ªæ¥ç€ä¸€ä¸ªï¼Œç¬¬ä¸€ä¸ªå¤„ç†å®Œäº†åœ¨å¤„ç†ç¬¬äºŒä¸ªï¼Œç­‰åˆ°ç¬¬ä¸€ä¸ªå¤„ç†å®Œäº†ä¹Ÿå°±åŠ å…¥äº†simhashåº“ã€‚æ‰€ä»¥åªè¦æœåŠ¡ç«¯åŠ äº†é˜Ÿåˆ—ï¼Œå°±ä¸å­˜åœ¨åŒæ—¶è¯·æ±‚ä¸èƒ½åˆ¤æ–­çš„æƒ…å†µã€‚\n- simhashå¦‚ä½•å¤„ç†çŸ­æ–‡æœ¬ï¼Ÿæ¢ä¸€ç§æ€è·¯ï¼Œsimhashå¯ä»¥ä½œä¸ºå±€éƒ¨æ•æ„Ÿå“ˆå¸Œç¬¬ä¸€æ¬¡è®¡ç®—ç¼©å°æ•´ä¸ªæ¯”è¾ƒçš„èŒƒå›´ï¼Œç­‰åˆ°æˆ‘ä»¬åªæœ‰æ¯”è¾ƒ700å¤šæ¬¡æ¯”è¾ƒæ—¶ï¼Œå°±ç®—ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰ç²¾å‡†åº¦é«˜è®¡ç®—å¾ˆæ…¢çš„ç¼–è¾‘è·ç¦»ä¹Ÿå¯ä»¥æå®šã€‚å½“ç„¶å¦‚æœè§‰å¾—æ…¢äº†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä½™å¼¦å¤¹è§’ç­‰æ•ˆç‡ç¨å¾®é«˜ç‚¹çš„ç›¸ä¼¼åº¦ç®—æ³•ã€‚\n\n\n\n# é€‚ç”¨æƒ…å†µ\n\né€šè¿‡å¤§é‡æµ‹è¯•ï¼Œsimhashç”¨äºæ¯”è¾ƒå¤§æ–‡æœ¬ï¼Œæ¯”å¦‚500å­—ä»¥ä¸Šæ•ˆæœéƒ½è¿˜è›®å¥½ï¼Œè·ç¦»å°äº3çš„åŸºæœ¬éƒ½æ˜¯ç›¸ä¼¼ï¼Œè¯¯åˆ¤ç‡ä¹Ÿæ¯”è¾ƒä½ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯å¾®åšä¿¡æ¯ï¼Œæœ€å¤šä¹Ÿå°±140ä¸ªå­—ï¼Œä½¿ç”¨simhashçš„æ•ˆæœå¹¶ä¸é‚£ä¹ˆç†æƒ³ã€‚çœ‹å¦‚ä¸‹å›¾ï¼Œåœ¨è·ç¦»ä¸º3æ—¶æ˜¯ä¸€ä¸ªæ¯”è¾ƒæŠ˜ä¸­çš„ç‚¹ï¼Œåœ¨è·ç¦»ä¸º10æ—¶æ•ˆæœå·²ç»å¾ˆå·®äº†ï¼Œä¸è¿‡æˆ‘ä»¬æµ‹è¯•çŸ­æ–‡æœ¬å¾ˆå¤šçœ‹èµ·æ¥ç›¸ä¼¼çš„è·ç¦»ç¡®å®ä¸º10ã€‚å¦‚æœä½¿ç”¨è·ç¦»ä¸º3ï¼ŒçŸ­æ–‡æœ¬å¤§é‡é‡å¤ä¿¡æ¯ä¸ä¼šè¢«è¿‡æ»¤ï¼Œå¦‚æœä½¿ç”¨è·ç¦»ä¸º10ï¼Œé•¿æ–‡æœ¬çš„é”™è¯¯ç‡ä¹Ÿéå¸¸é«˜ã€‚\n\n![](Simhash/simhash2.png)\n\n# python å·¥å…·ï¼ˆSimhashï¼‰\n\nå®‰è£…\n\n```shell\npip install Simhash\n```\n\næŸ¥çœ‹simhashå€¼\n\n``` python\nfrom simhash import Simhash\nprint '%x' % Simhash(u'I am very happy'.split()).value\n# 9f8fd7efdb1ded7f\n```\n\nè®¡ç®—ä¸¤ä¸ªsimhashå€¼è·ç¦»\n\n```python\nhash1 = Simhash(u'I am very happy'.split())\nhash2 = Simhash(u'I am very sad'.split())\nprint hash1.distance(hash2)\n# 5\n```\n\nSimhashIndexä½¿ç”¨\n\n```python\nfrom simhash import Simhash, SimhashIndex\n# å»ºç«‹ç´¢å¼•\ndata = {\n    u'1': u'How are you I Am fine . blar blar blar blar blar Thanks .'.lower().split(),\n    u'2': u'How are you i am fine .'.lower().split(),\n    u'3': u'This is simhash test .'.lower().split(),\n}\nobjs = [(id, Simhash(sent)) for id, sent in data.items()]\nindex = SimhashIndex(objs, k=10)  # kæ˜¯å®¹å¿åº¦ï¼›kè¶Šå¤§ï¼Œæ£€ç´¢å‡ºçš„ç›¸ä¼¼æ–‡æœ¬å°±è¶Šå¤š\n# æ£€ç´¢\ns1 = Simhash(u'How are you . blar blar blar blar blar Thanks'.lower().split())\nprint index.get_near_dups(s1)\n# å¢åŠ æ–°ç´¢å¼•\nindex.add(u'4', s1)\n```\n\n# å‚è€ƒèµ„æ–™\n\n[æµ·é‡æ•°æ®ç›¸ä¼¼åº¦è®¡ç®—ä¹‹simhashå’Œæµ·æ˜è·ç¦»](http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity.html)\n\n[æµ·é‡æ•°æ®ç›¸ä¼¼åº¦è®¡ç®—ä¹‹simhashçŸ­æ–‡æœ¬æŸ¥æ‰¾](http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity2-html.html)\n\n[simhashåŸç†åŠä½¿ç”¨](https://blog.csdn.net/qq_16912257/article/details/72156277)\n\n","tags":["IR"],"categories":["Machine Learning"]},{"title":"å­—ä¸²ç›¸ä¼¼åº¦-ç¼–è¾‘è·ç¦»","url":"%2Fblog%2Fedit-distance.html","content":"\nå­—ç¬¦ä¸²ç¼–è¾‘è·ç¦» å³ Levenshtein è·ç¦»\n\npythonåº“å®‰è£…ï¼š\n\nâ€‹\tpip install python-Levenshtein\n\nä½¿ç”¨\n\n```python\nimport Levenshtein\nstr1=\"abc\"\nstr2=\"bac\"\n```\n\n## edit distance(Levenshteinè·ç¦»)\n\nä¸€ä¸ªå­—ä¸²è½¬åŒ–æˆå¦ä¸€ä¸ªå­—ä¸²æœ€å°‘çš„æ“ä½œæ¬¡æ•°(åŒ…æ‹¬æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢) \n\n```python\nLevenshtein.distance(str1,str2)\n```\n\n## hamming distance(æ±‰æ˜è·ç¦»)\n\nè¦æ±‚str1å’Œstr2å¿…é¡»é•¿åº¦ä¸€è‡´ ä¸¤ä¸ªç­‰é•¿å­—ä¸²ä¹‹é—´å¯¹åº”ä½ç½®ä¸Šä¸åŒå­—ç¬¦çš„ä¸ªæ•° \n\n```python\nLevenshtein.hamming(str1, str2)\n```\n\n## Levenshtein ratio(è±æ–‡æ–¯å¦æ¯”)\n\nè®¡ç®—å…¬å¼ $$r = \\frac{sum - ldist}{sum}$$, è¡¨ç¤ºä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ã€‚\n\nå…¶ä¸­$\\text{sum}$æ˜¯æŒ‡str1 å’Œ str2 å­—ä¸²çš„é•¿åº¦æ€»å’Œï¼Œ$\\text{ldist}$æ˜¯**ç±»ç¼–è¾‘è·ç¦»** .å…¶ä¸­ï¼šåˆ é™¤ã€æ’å…¥ä¾ç„¶+1ï¼Œä½†æ˜¯æ›¿æ¢+2 ã€‚\n\nè¿™æ ·è®¾è®¡çš„ç›®çš„ï¼šratio('a', 'c')ï¼Œ$sum=2$,å¦‚æœæŒ‰ç¼–è¾‘è·ç¦»è®¡ç®—$ldist$ï¼Œåˆ™$ratio =\\frac{2-1}2 = 0.5$,ä½†æ˜¯â€™a','c'æ²¡æœ‰é‡åˆï¼Œæ˜¾ç„¶ä¸åˆç®—ï¼Œä½†æ˜¯æ›¿æ¢æ“ä½œ+2ï¼Œå°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\n```\nLevenshtein.ratio(str1, str2)\n```\n\n\n\npython-Levenshtein è¿˜æä¾›äº†Jaro distance å’Œ [Jaroâ€“Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) çš„æ–‡æœ¬è·ç¦»çš„è®¡ç®—ã€‚ç”¨çš„æ—¶å€™å†æŸ¥ã€‚\n\n\n\nå‚è€ƒèµ„æ–™\n\nhttp://www.coli.uni-saarland.de/courses/LT1/2011/slides/Python-Levenshtein.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["IR"],"categories":["Machine Learning"]},{"title":"æ¡ä»¶ç‹¬ç«‹æ€§","url":"%2Fblog%2FConditional-Independence.html","content":"\n# æ¡ä»¶ç‹¬ç«‹æ€§\n\n## å®šä¹‰\n\nå¤šå˜é‡æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µå°±æ˜¯æ¡ä»¶ç‹¬ç«‹æ€§(conditional independence)ã€‚\n\nè€ƒè™‘ä¸‰ä¸ªå˜é‡$a$, $b$, $c$ï¼Œ å¹¶ä¸”å‡è®¾ç»™å®š$b$, $c$çš„æ¡ä»¶ä¸‹$a$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸ä¾èµ–äº$b$çš„å€¼ï¼Œå³ï¼š\n$$\np(a| b, c) = p(a |c)\n$$\næ­¤æ—¶æˆ‘ä»¬è¯´ï¼Œç»™å®š$c$çš„æ¡ä»¶ä¸‹ï¼Œ$a$æ¡ä»¶ç‹¬ç«‹äºbã€‚\n\næ¨è®ºï¼šç»™å®š$c$çš„æ¡ä»¶ä¸‹ï¼Œå¯¹äº$a$æ¡ä»¶ç‹¬ç«‹äºb ï¼Œæœ‰ä¸‹è¯•æˆç«‹ï¼š\n$$\n\\begin {aligned}\np(a,b|c)  &= p(a|b,c)p(b|c) \\ \\ \\ \\mathrm{:product \\ rule}\\\\ \n& =p(a|c)p(b|c)  \\ \\ \\ \\mathrm{:conditional\\  independence}\n\\end {aligned}\n$$\nå½“ä½¿ç”¨æ¦‚ç‡æ¨¡å‹æ—¶ï¼Œæ¡ä»¶ç‹¬ç«‹æ€§èµ·ç€é‡è¦çš„ä½œç”¨ï¼Œå®ƒç®€åŒ–äº†æ¨¡å‹çš„ç»“æ„ï¼Œé™ä½äº†æ¨¡å‹çš„è®­ç»ƒå’Œæ¨æ–­çš„è®¡ç®—é‡ã€‚\n\n## è¯æ˜\n\nåœ¨å›¾æ¨¡å‹ä¸­ï¼Œå¯¹äºæ˜¯ä¸‰ä¸ªå˜é‡$a$, $b$, $c$æ¥è¯´è¦è¯æ˜ given $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹ä¸$b$ã€‚éœ€è¦$c$æ˜¯å¦è¢«è§‚å¯Ÿåˆ°åˆ†ä¸¤ç§æƒ…å†µè®¨è®ºï¼š\n\nå¦‚æœ$c$è¢«è§‚å¯Ÿåˆ°äº†ï¼Œ given $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹ä¸$b$ ç­‰ä»·äº\n$$\np(a,b|c)  =p(a|c)p(b|c)\n$$\nå¦‚æœ$c$æ˜¯éšå˜é‡ï¼Œ given $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹ä¸$b$ç­‰ä»·äº\n$$\np(a,b)  =p(a)p(b)\n$$\nå³ï¼Œæ­¤æ—¶éœ€è¦marginalizing with respect to $c$ã€‚\n\n# è´å¶æ–¯ç½‘æ¡ä»¶ç‹¬ç«‹æ€§ï¼ˆD-separationï¼‰\n\nå¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œæ¡ä»¶ç‹¬ç«‹æ€§èƒ½å¤Ÿå¤§å¤§çš„ç®€åŒ–æ¨¡å‹ç»“æ„é™ä½æ¨¡å‹çš„è®­ç»ƒå’Œæ¨æ–­çš„è®¡ç®—é‡ã€‚\n\nå¯¹äºæœ‰å‘å›¾æ¥è¯´ï¼Œå½“æˆ‘ä»¬æ‹¿åˆ°ä¸€ä¸ªä½¿ç”¨è‹¥å¹²ä¸ªå˜é‡çš„æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯è¡¨ç¤ºçš„è¿™äº›å˜é‡çš„è”åˆåˆ†å¸ƒæ—¶ï¼Œæ€ä¹ˆç¡®å®šå…¶ä¸­å“ªäº›å˜é‡ä¹‹é—´æ˜¯æ¡ä»¶ç‹¬ç«‹çš„å‘¢ï¼Ÿæœ‰ä¸¤ç§æ–¹æ¡ˆï¼š\n\n- ä½¿ç”¨æ¦‚ç‡å…¬å¼æ¨å¯¼ã€‚è¯¥æ–¹æ³•å·²ç»éªŒè¯æ˜¯éå¸¸å¤æ‚çš„ä¸€ä¸ªè¿‡ç¨‹ã€‚\n- d-separationã€‚æœ¬æ–‡æ‰€è¦è®ºè¿°çš„å†…å®¹ã€‚\n\nå›¾æ¨¡å‹çš„ä¸€ä¸ªé‡è¦ç‰¹å¾æ˜¯ï¼Œå›¾ä¸­æ‰€æœ‰å˜é‡ä¹‹é—´çš„æ¡ä»¶ç‹¬ç«‹æ€§å¯ä»¥ç›´æ¥ä»å›¾ä¸­è¯»å‡ºæ¥ï¼Œä¸éœ€è¦è¿›è¡Œä»»ä½•è®¡ç®—ã€‚ä»å›¾ä¸­è¯»å‡ºæ¡ä»¶ç‹¬ç«‹æ€§çš„æ–¹æ³•å«åšd-separationï¼Œå…¶ä¸­â€dâ€œå³directedã€‚\n\n## ä¸‰ä¸ªä¾‹å­\n\nä¸‹é¢é¦–å…ˆä½¿ç”¨ä¸‰ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜d-separationä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€‚ è¿™ä¸‰ä¸ªä¾‹å­éƒ½æ˜¯è®¨è®º **given $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹ä¸$b$**æ˜¯å¦æˆç«‹çš„ã€‚\n\n### tail-2-tail\n\n![](Conditional-Independence/tail-2-tail1.png)\n\n$$\n\\begin {aligned}\np(a,b,c) &= p(a|c)p(b|c)p(c) \n\\\\\np(a,b|c) & = \\frac{p(a,b,c)}{p(c)} \\\\\n&= p(a|c)p(b|c)\n\\end {aligned}\n$$\n\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ æˆç«‹ã€‚\n\n\n![](Conditional-Independence/tail-2-tail0.png)\n\n$$\n\\begin {aligned}\np(a,b,c) &= p(a|c)p(b|c)p(c) \\\\\n\\\\\np(a,b) &=\\sum_c p(a,b,c)=\\sum_c p(a|c)p(b|c) p(c) \\neq p(a)p(b)\n\\end {aligned}\n$$\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ ä¸æˆç«‹ã€‚\n\n### head-2-tail\n\n\n\n![](Conditional-Independence/head-2-tail1.png)\n\n$$\n\\begin {aligned}\np(a,b,c)&=p(a)p(c|a)p(b|c)\\\\\n\\\\\np(a,b|c)&= \\frac{p(a,b,c)}{p(c)} \\\\\n&= \\frac{p(a)p(c|a)p(b|c)}{p(c)} \\\\\n&= p(a|c)p(b|c)\n\\end {aligned}\n$$\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ æˆç«‹ã€‚\n\n\n\n\n![](Conditional-Independence/head-2-tail0.png)\n\n$$\n\\begin {aligned}\np(a,b,c)&=p(a)p(c|a)p(b|c)\\\\\n\\\\\np(a,b)&=p(a)\\sum_c p(c|a)p(b|c)=p(a)p(b|a) \\neq p(a)p(b)\n\\end {aligned}\n$$\n\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ ä¸æˆç«‹ã€‚\n\n### head-2-head\n\n\n\n![](Conditional-Independence/head-2-head1.png)\n$$\n\\begin {aligned}\np(a,b,c)&=p(a)p(b)p(c|a,b) \\\\\n\\\\\np(a,b|c) & =\\quad \\frac{p(a,b,c)}{p(c)} \\\\\n&= \\frac{p(a)p(b)p(c|a,b)}{p(c)} \\neq p(a|c)p(b|c)\n\\end {aligned}\n$$\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ ä¸æˆç«‹ã€‚\n\n\n\n![](Conditional-Independence/head-2-head0.png)\n\n$$\n\\begin {aligned}\np(a,b,c)&=p(a)p(b)p(c|a,b) \\\\\n\\\\\np(a,b)&=\\sum_c p(a,b,c) = p(a)p(b)\n\\end {aligned}\n$$\ngiven $c$ï¼Œ $a$æ¡ä»¶ç‹¬ç«‹äº$b$ æˆç«‹ã€‚\n\nä½œä¸ºåŸå› çš„å¤šä¸ªå› ç´ (a,b)ï¼Œå³ä½¿å®ƒä»¬ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œç¡®å®šç»“æœ(c)ä¹‹åè¿™äº›åŸå› å°±å¯èƒ½å˜çš„ç›¸å…³äº†ã€‚å½“ç»“æœ(c)ä¸ä½œä¸ºè§‚æµ‹å˜é‡çš„æ—¶å€™ï¼ŒåŸå› æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä¹Ÿå«åšè¾¹ç¼˜ç‹¬ç«‹ï¼ˆMarginal Independenceï¼‰.\n\nå¦‚æœä¸€ä¸ªèŠ‚ç‚¹$y$æ˜¯$x$çš„ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œåˆ™\n\nå¦‚æœå­˜åœ¨ä»ç»“ç‚¹$x$åˆ°ç»“ç‚¹$y$çš„ä¸€æ¡è·¯å¾„ï¼Œå…¶ä¸­è·¯å¾„çš„æ¯ä¸€æ­¥éƒ½æ²¿ç€ç®­å¤´çš„æ–¹å‘ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¯´ç»“ç‚¹$y$æ˜¯ç»“ç‚¹$x$çš„åä»£(descendant)ã€‚å¯ä»¥è¯æ˜ï¼Œåœ¨ç±»ä¼¼äºä¸Šå›¾ä¸­çš„head-2-headçš„è·¯å¾„ä¸­ï¼Œå¦‚æœç»“ç‚¹$c$æˆ–è€…å®ƒçš„ä»»æ„åä»£èŠ‚ç‚¹è¢«è§‚æµ‹åˆ°ï¼Œé‚£ä¹ˆè·¯å¾„ä¼šunblockedï¼Œå³aå’Œbæ˜¯æ¡ä»¶ç›¸å…³çš„ã€‚\n\n## æ€»ç»“\n\nä¸€ä¸ªtail-2-tailç»“ç‚¹æˆ–è€…head-2-tailç»“ç‚¹ä½¿å¾—ä¸€æ¡è·¯å¾„**æ²¡æœ‰è¢«é˜»éš”(unblocked)**ï¼Œå½“å®ƒè¢«è§‚æµ‹åˆ°æ—¶ï¼Œå®ƒå°±\né˜»éš”äº†è·¯å¾„ï¼›\n\nä¸€ä¸ªhead-2-headç»“ç‚¹å¦‚æœæ²¡æœ‰è¢«è§‚æµ‹åˆ°é‚£ä¹ˆå®ƒé˜»éš”äº†è·¯å¾„(block)ï¼Œå¦‚æœå®ƒè¢«è§‚æµ‹åˆ°æˆ–è€…ä»–çš„åä»£èŠ‚ç‚¹è¢«è§‚æµ‹åˆ°äº†ï¼Œ\né‚£ä¹ˆè·¯å¾„å°±**æ²¡æœ‰è¢«é˜»éš”(unblocked)**äº†ã€‚\n\n|             | unobserved  |       observed        |\n| :---------: | :---------: | :-------------------: |\n| tail-2-tail |  unblocked  |      **blocked**      |\n| head-2-tail |  unblocked  |      **blocked**      |\n| head-2-head | **blocked** | unblocked(descendant) |\n\n## å®šä¹‰\n\nd-separationç ”ç©¶çš„æ˜¯ï¼š\n\n> **given C**ï¼Œåˆ¤æ–­ A å’Œ B æ˜¯å¦æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ã€‚\n>\n> å…¶ä¸­Cæ˜¯æ‰€æœ‰è¢«è§‚æµ‹åˆ°çš„ç‚¹çš„å­é›†ã€‚\n\nä¸‹é¢ç»™å‡ºd-separationçš„å®šä¹‰ã€‚\n\nå¯¹äº DAG å›¾ Gï¼Œå¦‚æœAï¼ŒBï¼ŒCæ˜¯ä¸‰ä¸ª**é›†åˆ**ï¼ˆå¯ä»¥æ˜¯å•ç‹¬çš„èŠ‚ç‚¹æˆ–è€…æ˜¯èŠ‚ç‚¹çš„é›†åˆï¼‰Cæ˜¯ç”±è§‚æµ‹åˆ°çš„ç‚¹ç»„æˆçš„ï¼Œä¸ºäº†åˆ¤æ–­ A å’Œ B æ˜¯å¦å…³äº C æ¡ä»¶ç‹¬ç«‹çš„(å³ï¼Œåœ¨**ç»™å®š/è§‚æµ‹åˆ°**Cæ—¶ Aå’ŒBæ˜¯å¦æ¡ä»¶ç‹¬ç«‹)ï¼Œ æˆ‘ä»¬è€ƒè™‘Gä¸­æ‰€æœ‰Aå’ŒBä¹‹é—´çš„æ— å‘è·¯å¾„ ã€‚ å¯¹äºå…¶ä¸­çš„ä¸€æ¡è·¯å¾„ï¼Œå¦‚æœå¥¹æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ä¸­çš„ä»»æ„ä¸€æ¡ï¼Œåˆ™ç§°è¿™æ¡è·¯å¾„æ˜¯é˜»å¡ï¼ˆblockï¼‰çš„ï¼š\n\nï¼ˆ1ï¼‰è·¯å¾„ä¸­å­˜åœ¨æŸä¸ªèŠ‚ç‚¹ X æ˜¯ head-to-tial æˆ–è€…tail-to-tail èŠ‚ç‚¹ï¼Œå¹¶ä¸” X æ˜¯åŒ…å«åœ¨ C ä¸­çš„ï¼›\n\nï¼ˆ2ï¼‰è·¯å¾„ä¸­å­˜åœ¨æŸä¸ªèŠ‚ç‚¹ X æ˜¯ head-to-headèŠ‚ç‚¹ï¼Œå¹¶ä¸” X æˆ– Xçš„å„¿å­æ˜¯ä¸åŒ…å«åœ¨ C ä¸­çš„ï¼›\n\nå¦‚æœ Aï¼ŒB é—´æ‰€æœ‰çš„è·¯å¾„éƒ½æ˜¯é˜»å¡çš„ï¼Œé‚£ä¹ˆ Aï¼ŒB å°±æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ï¼›å¦åˆ™ï¼Œ Aï¼ŒB ä¸æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ã€‚\n\nä¸¾ä¾‹å¦‚ä¸‹ï¼š\n\n\n\n![](Conditional-Independence/d-separation.png)\n\n\n\nå¯¹äºå›¾(a)ï¼Œ $A= \\{a\\},B=\\{b\\},C=\\{c\\}$ ã€‚**given C**ï¼Œåˆ¤æ–­ A å’Œ B æ˜¯å¦æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ã€‚åˆ†ææ–¹æ³•å¦‚ä¸‹ï¼š\n\n1. ä»aåˆ°bçš„æ— å‘è·¯å¾„ä¸ŠåŒ…å«ä¸¤ä¸ªèŠ‚ç‚¹ï¼šeå’Œfï¼›\n2. eï¼šhead-to-headèŠ‚ç‚¹ï¼Œå…¶å­èŠ‚ç‚¹å±äºCï¼Œä¸ç¬¦åˆç¬¬ï¼ˆ2ï¼‰æ¡ï¼Œunblockedã€‚\n3. fï¼štail-to-tailèŠ‚ç‚¹ï¼Œfä¸å±äºCï¼Œä¸ç¬¦åˆç¬¬ï¼ˆ1ï¼‰æ¡ï¼Œunblockedã€‚\n\nç»“è®ºï¼šgiven Cï¼Œ A å’Œ B æ˜¯å¦æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ **ä¸æˆç«‹**ã€‚\n\nå¯¹äºå›¾(b)ï¼Œ $A= \\{a\\},B=\\{b\\},C=\\{f\\}$ ã€‚**given C**ï¼Œåˆ¤æ–­ A å’Œ B æ˜¯å¦æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ã€‚åˆ†ææ–¹æ³•å¦‚ä¸‹ï¼š\n\n1. ä»aåˆ°bçš„æ— å‘è·¯å¾„ä¸ŠåŒ…å«ä¸¤ä¸ªèŠ‚ç‚¹ï¼šeå’Œfï¼›\n2. eï¼šhead-to-headèŠ‚ç‚¹ï¼Œeä¸å…¶å­èŠ‚ç‚¹ä¸å±äºCï¼Œç¬¦åˆç¬¬ï¼ˆ2ï¼‰æ¡ï¼Œblockedã€‚\n3. fï¼štail-to-tailèŠ‚ç‚¹ï¼Œfå±äºCï¼Œç¬¦åˆç¬¬ï¼ˆ1ï¼‰æ¡ï¼Œblockedã€‚\n\nç»“è®ºï¼šgiven Cï¼Œ A å’Œ B æ˜¯å¦æ˜¯å…³äº C æ¡ä»¶ç‹¬ç«‹çš„ **æˆç«‹**ã€‚\n\n\n\n# é©¬å°”å¯å¤«éšæœºåœºçš„æ¡ä»¶ç‹¬ç«‹æ€§\n\nåœ¨æœ‰å‘å›¾çš„æƒ…å½¢ä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯ä»¥é€šè¿‡ä½¿â½¤è¢«ç§°ä¸ºd-separationçš„å›¾æ£€æµ‹â½…æ³•åˆ¤æ–­â¼€ä¸ªç‰¹å®šçš„æ¡ä»¶ç‹¬â½´æ€§è´¨æ˜¯å¦æˆâ½´ã€‚è¿™æ¶‰åŠåˆ°åˆ¤æ–­é“¾æ¥ä¸¤ä¸ªç»“ç‚¹é›†åˆçš„è·¯å¾„æ˜¯å¦è¢«â€œé˜»éš”â€(blocked)ã€‚å…¶ä¸­æ¶‰åŠåˆ°çš„ä¸‰ç§æƒ…å½¢(head-2-head, head-2-tail, tail-2-tail)ä¸­head-2-headæ˜¯æ¯”è¾ƒç‰¹æ®Šçš„(D-separationçš„ç¬¬äºŒä¸ªæ¡ä»¶)ã€‚\n\nå¯¹åº”äºâ½†å‘å›¾æ¨¡å‹ã€‚é€šè¿‡ç§»é™¤å›¾ä¸­é“¾æ¥çš„â½…å‘æ€§ï¼Œâ½—ç»“ç‚¹å’Œâ¼¦ç»“ç‚¹çš„â¾®å¯¹ç§°æ€§ä¹Ÿè¢«ç§»é™¤äº†ï¼Œå› æ­¤head-2-headç»“ç‚¹çš„ç‰¹æ®Šæ€§è´¨ä¹Ÿå°±ä¸å†å­˜åœ¨äº†ã€‚\n\n## ç¬¬ä¸€ç§æ–¹æ³•ï¼š\n\nå‡è®¾åœ¨â¼€ä¸ªâ½†å‘å›¾ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªç»“ç‚¹é›†åˆï¼Œè®°ä½œAï¼ŒBï¼Œ Cã€‚æˆ‘ä»¬è€ƒè™‘æ¡ä»¶ç‹¬â½´æ€§è´¨ï¼Œå…¶ä¸­Cæ˜¯è¢«è§‚æµ‹ç‚¹çš„é›†åˆã€‚\nè€ƒè™‘è¿æ¥é›†åˆAçš„ç»“ç‚¹å’Œé›†åˆBçš„ç»“ç‚¹çš„æ‰€æœ‰å¯èƒ½è·¯å¾„ã€‚å¦‚æœæ‰€æœ‰è¿™äº›è·¯å¾„ **éƒ½** é€šè¿‡äº†é›†åˆCä¸­çš„â¼€ä¸ªæˆ–å¤šä¸ªç»“ç‚¹ï¼Œé‚£ä¹ˆæ‰€æœ‰è¿™æ ·çš„è·¯å¾„éƒ½è¢«â€œé˜»éš”â€(blocked)ï¼Œå› æ­¤æ¡ä»¶ç‹¬â½´æ€§è´¨æˆâ½´ã€‚ å¦‚æœå­˜åœ¨â¾„å°‘â¼€æ¡æœªè¢«é˜»éš”çš„è·¯å¾„ï¼Œé‚£ä¹ˆæ¡\nä»¶ç‹¬â½´çš„æ€§è´¨ä¸æˆâ½´ã€‚\n\nè¿™ä¸dåˆ’åˆ†çš„å‡†åˆ™å®Œå…¨ç›¸åŒï¼Œå”¯â¼€çš„å·®åˆ«åœ¨äºæ²¡æœ‰å¤´åˆ°å¤´çš„ç°è±¡ã€‚å› æ­¤ï¼Œâ½†å‘å›¾çš„æ¡ä»¶ç‹¬â½´æ€§çš„æ£€æµ‹â½æœ‰å‘å›¾ç®€å•ã€‚\n\n## ç¬¬äºŒç§æ–¹æ³•ï¼š\n\nå¦â¼€ç§æ¡ä»¶ç‹¬â½´æ€§çš„æ£€æµ‹çš„â½…æ³•æ˜¯å‡è®¾ä»å›¾ä¸­æŠŠé›†åˆCä¸­çš„ç»“ç‚¹ä»¥åŠä¸è¿™äº›ç»“ç‚¹ç›¸è¿çš„é“¾æ¥å…¨éƒ¨åˆ é™¤ï¼Œå¦‚æœä¸å­˜åœ¨â¼€æ¡ä»Aä¸­ä»»æ„ç»“ç‚¹åˆ°Bä¸­ä»»æ„ç»“ç‚¹çš„è·¯å¾„ï¼Œé‚£ä¹ˆæ¡ä»¶ç‹¬â½´çš„æ€§è´¨â¼€å®šæˆâ½´ã€‚\n\nä¸¾ä¾‹ï¼š\n\n![](Conditional-Independence/markov-ci.png)\n\nå¦‚ä¸Šå›¾ï¼Œå…¶ä¸­ä»é›†åˆAä¸­çš„ä»»æ„ç»“ç‚¹åˆ°é›†åˆBä¸­çš„ä»»æ„ç»“ç‚¹çš„æ¯æ¡è·¯å¾„éƒ½é€šè¿‡é›†åˆCä¸­çš„â¾„å°‘â¼€ä¸ªç»“ç‚¹ã€‚æ‰€ä»¥given Cï¼ŒAä¸Bæ¡ä»¶ç‹¬ç«‹ã€‚\n\nç§»é™¤é›†åˆCä¸­çš„æ‰€æœ‰èŠ‚ç‚¹å’Œä¸ä¹‹ç›¸å…³çš„è¿æ¥ï¼Œä¸å­˜åœ¨â¼€æ¡ä»Aä¸­ä»»æ„ç»“ç‚¹åˆ°Bä¸­ä»»æ„ç»“ç‚¹çš„è·¯å¾„ï¼Œæ‰€ä»¥given Cï¼ŒAä¸Bæ¡ä»¶ç‹¬ç«‹ã€‚\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning\n\n[æ¦‚ç‡å›¾æ¨¡å‹4ï¼šè´å¶æ–¯ç½‘ç»œ](https://blog.csdn.net/github_36326955/article/details/69569032#141-d-%E5%88%86%E7%A6%BB)\n\n\n\n\n\n\n\n\n\n","tags":["Math"],"categories":["Machine Learning"]},{"title":"pythonä¸­ç±»çš„å®ä¾‹åŒ– __new__","url":"%2Fblog%2FInstance-New-in-Python.html","content":"\n\n\n\n\n`__new__()`æ˜¯åœ¨æ–°å¼ç±»ä¸­æ–°å‡ºç°çš„æ–¹æ³•ï¼Œåœ¨Python2.7ä»¥å‰çš„ç‰ˆæœ¬åœ¨å®šä¹‰ç±»æ—¶éƒ½è¦æ˜¾ç¤ºçš„ç»§æ‰¿objectæ‰èƒ½ä½¿ç”¨ã€‚\n\n**`__new__()`æ–¹æ³•å§‹ç»ˆéƒ½æ˜¯ç±»çš„é™æ€æ–¹æ³•ï¼Œå³ä½¿æ²¡æœ‰è¢«åŠ ä¸Šé™æ€æ–¹æ³•è£…é¥°å™¨ã€‚**`__new__`æ–¹æ³•æ¥å—çš„å‚æ•°è™½ç„¶ä¹Ÿæ˜¯å’Œ`__init__`ä¸€æ ·ï¼Œä½†`__init__`æ˜¯åœ¨ç±»å®ä¾‹åˆ›å»ºä¹‹åè°ƒç”¨ã€‚\n\nobjectç±»ä¸­å¯¹`__new__()`æ–¹æ³•çš„å®šä¹‰ï¼š\n\n```python\nclass object:\n  @staticmethod # known case of __new__\n  def __new__(cls, *more): # known special case of object.__new__\n    \"\"\" T.__new__(S, ...) -> a new object with type S, a subtype of T \"\"\"\n    pass\n```\n\n# **`__new__ `çš„ä½œç”¨**\n\nä¾ç…§Pythonå®˜æ–¹æ–‡æ¡£çš„è¯´æ³•:\n\n1. `__new__`æ–¹æ³•ä¸»è¦æ˜¯å½“ä½ ç»§æ‰¿ä¸€äº›**ä¸å¯å˜çš„class**æ—¶(æ¯”å¦‚int, str, tuple)ï¼Œ æä¾›ç»™ä½ ä¸€ä¸ªè‡ªå®šä¹‰è¿™äº›ç±»çš„å®ä¾‹åŒ–è¿‡ç¨‹çš„é€”å¾„ï¼›\n2. å®ç°è‡ªå®šä¹‰çš„metaclassã€‚\n\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç¬¬ä¸€ä¸ªåŠŸèƒ½ï¼Œå…·ä½“æˆ‘ä»¬å¯ä»¥ç”¨intæ¥ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼š\n\nå‡å¦‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ°¸è¿œéƒ½æ˜¯æ­£æ•°çš„æ•´æ•°ç±»å‹ï¼Œé€šè¿‡ç»§æ‰¿intï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå†™å‡ºè¿™æ ·çš„ä»£ç ã€‚\n\n```python\nclass PositiveInteger(int):\n  def __init__(self, value):\n    super(PositiveInteger, self).__init__(self, abs(value))\n\ni = PositiveInteger(-3)\nprint i\n\n# -3\n```\n\nä½†è¿è¡Œåä¼šå‘ç°ï¼Œç»“æœæ ¹æœ¬ä¸æ˜¯æˆ‘ä»¬æƒ³çš„é‚£æ ·ï¼Œæˆ‘ä»¬ä»»ç„¶å¾—åˆ°äº†-3ã€‚è¿™æ˜¯å› ä¸ºå¯¹äºintè¿™ç§ä¸å¯å˜çš„å¯¹è±¡ï¼Œæˆ‘ä»¬åªæœ‰é‡è½½å®ƒçš„`__new__`æ–¹æ³•æ‰èƒ½èµ·åˆ°è‡ªå®šä¹‰çš„ä½œç”¨ã€‚\n\n```python\nclass PositiveInteger(int):\n  def __new__(cls, value):\n    return super(PositiveInteger, cls).__new__(cls, abs(value))\n\ni = PositiveInteger(-3)\nprint i\n\n# 3\n```\n\nå…³äº[å®ç°è‡ªå®šä¹‰çš„metaclass]æˆ‘ä¼šåœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­å•ç‹¬è®ºè¿°ã€‚\n\n# ç±»çš„å®ä¾‹åŒ–\n\næ¥ä¸‹æ¥æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹ï¼ŒPythonè§£é‡Šå™¨æ˜¯å¦‚ä½•å®ä¾‹åŒ–ä¸€ä¸ªç±»çš„ï¼š\n1. å½“æˆ‘ä»¬å®ä¾‹åŒ–`A`ç±»å¯¹è±¡æ—¶ï¼ŒPythonä¸­é¦–å…ˆè°ƒç”¨çš„æ˜¯è¯¥`A`ç±»å¯¹è±¡çš„`__new__`æ–¹æ³•ï¼Œå¦‚æœè¯¥`A`ç±»å¯¹è±¡æ²¡æœ‰å®šä¹‰`__new__`æ–¹æ³•ï¼Œåˆ™å»çˆ¶ç±»ä¸­ä¾æ¬¡æŸ¥æ‰¾ï¼Œç›´åˆ°`object`ç±»ï¼›\n2. `object`ç±»æœ‰ä¸€ä¸ª`__new__`æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¥æ”¶ä¸€ä¸ªå‚æ•°(ä¸€èˆ¬ä¸ºç±»å¯¹è±¡)ï¼Œå°†è¯¥å‚æ•°è¿›è¡Œå®ä¾‹åŒ–å¹¶è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼›\n3. Pythonè§£é‡Šå™¨ä¼šå°†è°ƒç”¨`__new__`æ–¹æ³•å¹¶å°†`A`ç±»å¯¹è±¡ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥ï¼Œæœ€åä¼šè¿”å›ä¸€ä¸ªå®ä¾‹å¯¹è±¡(è¿™ä¸ªå¯¹è±¡å°±æ˜¯`A`ç±»çš„å®ä¾‹å¯¹è±¡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º`a1`)ï¼›`__new__()`å¿…é¡»è¦æœ‰è¿”å›å€¼ï¼Œè¿”å›å®ä¾‹åŒ–å‡ºæ¥çš„å®ä¾‹ï¼Œå¯ä»¥`return`çˆ¶ç±»`__new__()`å‡ºæ¥çš„å®ä¾‹ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å°†objectçš„`__new__()`å‡ºæ¥çš„å®ä¾‹è¿”å›ã€‚\n4. Pythonè§£é‡Šå™¨é»˜è®¤ä¼šè°ƒç”¨`a1`å¯¹è±¡çš„`__init__`æ–¹æ³•ï¼Œå¹¶å°†å‚æ•°ä¼ å…¥ã€‚\n\n## å®ä¾‹åŒ–ä¸¾ä¾‹\n\n### å®ä¾‹åŒ–è‡ªèº«\n\n\n```python\nmyclass = MyClass(*args, **kwargs)\n```\n\næ­£å¦‚ä»¥ä¸Šæ‰€ç¤ºï¼Œä¸€ä¸ªç±»å¯ä»¥æœ‰å¤šä¸ªä½ç½®å‚æ•°å’Œå¤šä¸ªå‘½åå‚æ•°ï¼Œè€Œåœ¨å®ä¾‹åŒ–å¼€å§‹ä¹‹åï¼Œåœ¨è°ƒç”¨ `__init__()`æ–¹æ³•ä¹‹å‰ï¼ŒPythoné¦–å…ˆè°ƒç”¨`__new__()`æ–¹æ³•ï¼š\n\n```python\ndef __new__(cls, *args, **kwargs):\n    return super(MyClass, cls).__new__(cls, *args, **kwargs)\n```\n\nç¬¬ä¸€ä¸ªå‚æ•°clsæ˜¯å½“å‰æ­£åœ¨å®ä¾‹åŒ–çš„ç±»ã€‚\n\nå¦‚æœï¼ˆæ–°å¼ï¼‰ç±»ä¸­æ²¡æœ‰é‡å†™`__new__()`æ–¹æ³•ï¼Œå³åœ¨å®šä¹‰æ–°å¼ç±»æ—¶æ²¡æœ‰é‡æ–°å®šä¹‰`__new__()`æ—¶ ï¼ŒPythoné»˜è®¤æ˜¯è°ƒç”¨è¯¥ç±»çš„ç›´æ¥çˆ¶ç±»çš„`__new__()`æ–¹æ³•æ¥æ„é€ è¯¥ç±»çš„å®ä¾‹ï¼Œå¦‚æœè¯¥ç±»çš„çˆ¶ç±»ä¹Ÿæ²¡æœ‰é‡å†™` __new__()`ï¼Œé‚£ä¹ˆå°†ä¸€ç›´æŒ‰æ­¤è§„çŸ©è¿½æº¯è‡³objectçš„`__new__()`æ–¹æ³•ï¼Œå› ä¸ºobjectæ˜¯æ‰€æœ‰æ–°å¼ç±»çš„åŸºç±»ã€‚\n\nè€Œå¦‚æœæ–°å¼ç±»ä¸­é‡å†™äº†`__new__()`æ–¹æ³•ï¼Œé‚£ä¹ˆä½ å¯ä»¥è‡ªç”±é€‰æ‹©**ä»»æ„ä¸€ä¸ªä¸è¯¥ç±»æœ‰ç»§æ‰¿å…³ç³»çš„ç±»**çš„`__new__()`æ–¹æ³•æ¥åˆ¶é€ å®ä¾‹ï¼ŒåŒ…æ‹¬è¿™ä¸ªæ–°å¼ç±»çš„æ‰€æœ‰å‰ä»£ç±»å’Œåä»£ç±»ï¼Œä½†æ˜¯**ä¸èƒ½é€ æˆé€’å½’æ­»å¾ªç¯**ã€‚\n\n### å®ä¾‹åŒ–ç¥–å…ˆç±»\n\n```python\nclass Foo(object):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __new__(cls, *args, **kwargs):\n        return object.__new__(cls, *args, **kwargs)\n    \nclass Child(Foo):\n    def __new__(cls, *args, **kwargs):\n        return object.__new__(cls, *args, **kwargs)  # ç›´æ¥è°ƒç”¨äº†çˆ¶ç±»çš„çˆ¶ç±»çš„ newæ–¹æ³•\n\nchild = Child()\n```\n\n### å®ä¾‹åŒ–åä»£ç±»\n\n```python\nclass Foo(object):\n    def __init__(self, *args, **kwargs):\n        self.name = 'Foo'\n        print 'Foo init'\n        \n    def __new__(cls, *args, **kwargs):\n        print 'Foo new'\n        return super(Foo, cls).__new__(Stranger, *args, **kwargs)   #æ­¤å¤„å®ä¾‹åŒ–çš„æ˜¯Stranger\n    \nclass Stranger(Foo):\n    def __init__(self, *args, **kwargs):\n        self.name = 'Stranger'\n        print 'Stranger init'\n    \n    def __str__(self):\n        return 'stranger'\n    \nfoo = Foo()  # æ­¤äº‹è°ƒç”¨äº†Fooçš„__new__ æ–¹æ³•å’Œ Strangerçš„__init__æ–¹æ³•\nprint foo.name\n\n# Foo new\n# Stranger init\n# Stranger\n```\n\n### å®ä¾‹åŒ–æ— ç»§æ‰¿å…³ç³»çš„ç±»\n\n> åœ¨[æ–‡ç« ](https://www.cnblogs.com/liunnis/p/4634417.html)ä¸­å®éªŒè¯´æ˜å¦‚æœ`__new__()`æ²¡æœ‰è¿”å›`cls`ï¼ˆå³å½“å‰ç±»ï¼‰çš„å®ä¾‹ï¼Œé‚£ä¹ˆå½“å‰ç±»çš„`__init__()`æ–¹æ³•æ˜¯ä¸ä¼šè¢«è°ƒç”¨çš„ã€‚å¦‚æœ`__new__()`è¿”å›å…¶ä»–ç±»çš„å®ä¾‹ï¼Œé‚£ä¹ˆåªä¼šè°ƒç”¨è¢«è¿”å›çš„é‚£ä¸ªç±»çš„æ„é€ æ–¹æ³•ã€‚\n\næˆ‘ä¹Ÿå°è¯•äº†è¿™ä¸ªå®éªŒï¼Œå®éªŒç»“è®ºä¸ä»¥ä¸Šè¯´æ³•ä¸åŒï¼š\n\nå¦‚æœ`__new__()`è¿”å›å…¶ä»–ç±»çš„å®ä¾‹ï¼Œé‚£ä¹ˆåªä¼šå®ä¾‹åŒ–å…¶ä»–ç±»ï¼Œä½†ä¸ä¼šè°ƒç”¨å…¶ä»–ç±»çš„`__init__`æ–¹æ³•ã€‚\n\n```python\nclass Foo(object):\n    def __init__(self, *args, **kwargs):\n        self.name = 'Foo'\n        print 'Foo init'\n        \n    def __new__(cls, *args, **kwargs):\n        print 'Foo new'\n        return super(Foo, cls).__new__(Stranger, *args, **kwargs)   #æ­¤å¤„å®ä¾‹åŒ–çš„æ˜¯Stranger\n    \nclass Stranger(object):\n    def __init__(self, *args, **kwargs):\n        self.name = 'Stranger'\n        print 'Stranger init'\n    \n    def __str__(self):\n        return 'stranger'\n    \nfoo = Foo()  # åªè°ƒç”¨äº†Fooçš„__new__æ–¹æ³•å®ä¾‹åŒ–äº†Strangerã€‚\nprint type(foo)\nprint foo.name  # å¯ä»¥å‘ç°nameæ²¡æœ‰è¢«èµ‹å€¼ï¼Œ__init__æ–¹æ³•æ²¡æœ‰è¢«è°ƒç”¨\n\n# Foo new\n# <class '__main__.Stranger'>\n# ---------------------------------------------------------------------------\n# AttributeError                            Traceback (most recent call last)\n# <ipython-input-38-15ce8af5d890> in <module>()\n#      20 foo = Foo()\n#      21 print type(foo)\n# ---> 22 print foo.name\n\n# AttributeError: 'Stranger' object has no attribute 'name'\n```\n\n# å‚è€ƒèµ„æ–™\n\n[Pythonä¸­çš„__new__()æ–¹æ³•ä¸å®ä¾‹åŒ–](https://www.cnblogs.com/liunnis/p/4634417.html)\n\n[å®ä¾‹è§£æPythonä¸­çš„__new__ç‰¹æ®Šæ–¹æ³•](https://www.jb51.net/article/85724.htm)\n\n\n\n\n\n","tags":["Python"],"categories":["Python"]},{"title":"Sum Rule and Product Rule in Probability","url":"%2Fblog%2FSum-Rule-and-Product-Rule-in-Probability.html","content":"\n# ä¸¾ä¾‹\n\nå‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªç›’å­ï¼Œä¸€ä¸ªçº¢è‰²çš„ï¼Œä¸€ä¸ªè“è‰²çš„ï¼Œçº¢ç›’å­ä¸­æœ‰2ä¸ªè‹¹æœå’Œ6ä¸ªæ©˜å­ï¼Œè“ç›’å­ä¸­æœ‰3ä¸ªè‹¹æœå’Œ1ä¸ªæ©˜å­ã€‚ç°åœ¨å‡å®šæˆ‘ä»¬éšæœºé€‰æ‹©ä¸€ä¸ªç›’å­ï¼Œä»è¿™ä¸ªç›’å­ä¸­æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€ä¸ªæ°´æœï¼Œè§‚å¯Ÿé€‰æ‹©äº†å“ªç§æ°´æœï¼Œç„¶åæ”¾å›ç›’å­ä¸­ã€‚æˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹å¾ˆå¤šæ¬¡ã€‚\n\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦é€‰æ‹©çš„ç›’å­çš„é¢œè‰²æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œè¿™ä¸ªéšæœºå˜é‡å¯ä»¥å–ä¸¤ä¸ªå€¼ä¸­çš„ä¸€ä¸ªï¼Œå³r(çº¢ç›’å­)æˆ–b(è“ç›’å­)ã€‚ç±»ä¼¼åœ°ï¼Œæ°´æœçš„ç§ç±»ä¹Ÿæ˜¯ä¸€ä¸ªéšæœºå˜é‡ ï¼Œå®ƒå¯ä»¥å–aï¼ˆè‹¹æœï¼‰æˆ–è€…oï¼ˆæ©˜å­ï¼‰ã€‚\n\n![](Sum-Rule-and-Product-Rule-in-Probability/box-fruit.png)\n\n# æ¨å¯¼\n\nè¿™ä¸ªä¾‹å­æ¶‰åŠåˆ°ä¸¤ä¸ªéšæœºå˜é‡$X$å’Œ$Y$ (å¦‚ç›’å­çš„é¢œè‰²å’Œæ°´æœçš„ç§ç±»)ã€‚æˆ‘ä»¬å‡è®¾$X$å¯ä»¥å–ä»»æ„çš„$x_i$ ï¼Œå…¶ä¸­$i = 1,\\dots, M$ ï¼Œå¹¶ä¸”$Y$ å¯ä»¥å–ä»»æ„çš„$y_j$ ï¼Œå…¶ä¸­$j = 1,\\dots,L$ã€‚è€ƒè™‘ä¸€å…±è¿›è¡Œ$N$ æ¬¡è¯•éªŒï¼Œå…¶ä¸­æˆ‘ä»¬å¯¹$X$å’Œ$Y $éƒ½è¿›è¡Œå–æ ·ï¼ŒæŠŠ$X = x_i$ ä¸”$Y = y_j$ çš„è¯•éªŒçš„æ•°é‡è®°ä½œ$n_{ij}$ ã€‚å¹¶ä¸”ï¼ŒæŠŠ$X$å–å€¼$x_i$ï¼ˆä¸$Y$ çš„å–å€¼æ— å…³ï¼‰çš„è¯•éªŒçš„æ•°é‡è®°ä½œ$c_i$ ï¼ŒæŠŠ$Y $å–å€¼$y_j$ çš„è¯•éªŒçš„æ•°é‡è®°ä½œ$r_j$ ã€‚\n\n![](Sum-Rule-and-Product-Rule-in-Probability/sum-product.png)\n\nå¯¹äºä¸¤ä¸ªéšæœºå˜é‡$X$å’Œ$Y$ï¼Œ$X$å¯èƒ½çš„å–å€¼ä¸º$\\{x_1, \\dots, x_M \\}$ï¼Œ $Y$å¯èƒ½å–å€¼ä¸º$\\{x_1, \\dots, x_L \\}$ã€‚\n\né‚£ä¹ˆ $X=x_i, Y=y_j$ çš„è”åˆæ¦‚ç‡ä¸º:\n$$\np(X=x_i, Y=y_j)=\\frac{n_{ij}}{N}\n$$\né‚£ä¹ˆ $X=x_i$ çš„æ¦‚ç‡ä¸º:\n$$\np(X=x_i)=\\frac{c_i}{N} = \\sum_{j=1}^L p(X=x_i, Y=y_j)\n$$\nè¿™æ˜¯æ¦‚ç‡çš„åŠ å’Œè§„åˆ™ï¼ˆ**sum rule**ï¼‰ã€‚$p(X=x_i)$ä¹Ÿè¢«ç§°ä¸ºè¾¹ ç¼˜ æ¦‚ ç‡(**marginal probability**)ï¼Œå› ä¸ºå®ƒé€šè¿‡æŠŠå…¶ä»–å˜é‡ï¼ˆæœ¬ä¾‹ä¸­çš„Y ï¼‰è¾¹ç¼˜åŒ–æˆ–è€…åŠ å’Œå¾—åˆ°ã€‚\n\nç»™å®š$X = x_i$ ï¼Œ$Y = y_j$ çš„æ¡ä»¶æ¦‚ç‡ï¼ˆconditional probabilityï¼‰ï¼š\n$$\np(Y = y_j | X = x_i)=\\frac{n_{ij}}{c_i}\n$$\né‚£ä¹ˆï¼š\n$$\np(X=x_i, Y=y_j)=\\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\cdot \\frac{c_i}{N} = p(Y = y_j | X = x_i)p(X=x_i)\n$$\nè¿™è¢«ç§°ä¸ºæ¦‚ç‡çš„ä¹˜ç§¯è§„åˆ™ï¼ˆproduct ruleï¼‰ã€‚\n\n# æ€»ç»“\n\n$$\n\\begin {aligned}\n\\rm{sum\\  rule} &  \\ \\ \\ \\ p(X)= \\sum_{Y} p(X, Y)\\\\\n\\rm{product rule} &  \\ \\ \\ \\ p(X,Y)=p(Y|X)p(X)\n\\end {aligned}\n$$\n\n\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning","tags":["Math"],"categories":["Math"]},{"title":"è´å¶æ–¯ç½‘-è´å¶æ–¯å›å½’","url":"%2Fblog%2FBayesian-Networks-regression.html","content":"\n# æ¦‚è¿°\n\næ¦‚ç‡åœ¨ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­èµ·ç€é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒçš„å›¾å½¢è¡¨ç¤ºè¿›è¡Œåˆ†æå¾ˆæœ‰å¥½å¤„ã€‚è¿™ç§æ¦‚ç‡åˆ†å¸ƒçš„å›¾å½¢è¡¨ç¤ºè¢«ç§°ä¸º**æ¦‚ç‡å›¾æ¨¡å‹**ï¼ˆprobabilistic graphical modelsï¼‰ã€‚æ¦‚ç‡æ¨¡å‹çš„è¿™ä¸­å›¾å½¢è¡¨ç¤ºæœ‰å¦‚ä¸‹æ€§è´¨ï¼š\n\n- å®ƒä»¬æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼å°†æ¦‚ç‡æ¨¡å‹çš„ç»“æ„å¯è§†åŒ–ï¼Œå¯ä»¥ç”¨äºè®¾è®¡æ–°çš„æ¨¡å‹ã€‚\n- é€šè¿‡è§‚å¯Ÿå›¾å½¢ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ·±åˆ»åœ°è®¤è¯†æ¨¡å‹çš„æ€§è´¨ï¼Œå¦‚æ¡ä»¶ç‹¬ç«‹æ€§ã€‚\n- åœ¨å¤æ‚æ¨¡å‹ä¸­ï¼Œå¤æ‚çš„è®¡ç®—å¯ä»¥è¡¨ç¤ºç§°ä¸ºå›¾çš„æ“ä½œã€‚ï¼ˆè¿™äº›å›¾çš„æ“ä½œå®é™…ä¸Šä»£è¡¨äº†å¤æ‚çš„æ•°æ®è¡¨è¾¾å¼çš„æ¨å¯¼ï¼‰\n\nä¸€ä¸ªå›¾æœ‰ä¸¤éƒ¨åˆ†ç»„æˆèŠ‚ç‚¹ï¼ˆnodesï¼‰å’Œè¿æ¥ï¼ˆlinksï¼‰ã€‚å…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºæ¨¡å‹ä¸­çš„å˜é‡ï¼Œè¿æ¥è¡¨ç¤ºèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚æ ¹æ®è¿æ¥æ˜¯å¦å…·æœ‰æ–¹å‘æ€§å¯ä»¥å°†æ¦‚ç‡å›¾æ¨¡å‹åˆ†ä¸ºä¸¤ç±»ï¼š\n\n1. **è´å¶æ–¯ç½‘**(Bayesian Networks)ï¼š è¿æ¥å…·æœ‰æ–¹å‘ï¼Œç”¨ç®­å¤´è¡¨ç¤ºæ–¹å‘ï¼Œè¿æ¥çš„æ–¹å‘ä¹Ÿè¡¨ç¤ºäº†å˜é‡ä¹‹é—´çš„æ¡ä»¶å…³ç³»ï¼Œå¦‚A-->Bå¯¹åº”æ¡ä»¶æ¦‚ç‡$p(B|A)$ã€‚è´å¶æ–¯ç½‘ä¹Ÿç§°ä¸ºæœ‰å‘å›¾æ¨¡å‹(directed graphical models)ã€‚æœ‰å‘å›¾å¯¹äºè¡¨è¾¾éšæœºå˜é‡ä¹‹é—´çš„å› æœå…³ç³»å¾ˆæœ‰ç”¨ã€‚\n2. **é©¬å°”ç§‘å¤«éšæœºåœº**(Markov random fields)ï¼š è¿æ¥æ— æ–¹å‘æ€§ï¼Œä¹Ÿç§°ä¸ºæ— å‘å›¾æ¨¡å‹(undirected graphical models)ã€‚æ— å‘å›¾å¯¹äºè¡¨ç¤ºéšæœºå˜é‡ä¹‹é—´çš„è½¯é™åˆ¶æ¯”è¾ƒæœ‰ç”¨ã€‚\n\nä¸ºäº†æ±‚è§£æ¨æ–­é—®é¢˜ï¼Œé€šå¸¸æ¯”è¾ƒæ–¹ä¾¿çš„åšæ³•æ˜¯æŠŠæœ‰å‘å›¾å’Œæ— å‘å›¾éƒ½è½¬åŒ–ä¸ºä¸€ä¸ªä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼Œè¢«ç§°ä¸º**å› å­å›¾**(factor graph)ã€‚\n\næœ¬æ–‡è®¨è®ºæœ‰è´å¶æ–¯ç½‘ã€‚\n\nè´å¶æ–¯ç½‘ç»œæ˜¯è´å¶æ–¯æ–¹æ³•çš„æ‰©å±•ã€‚å®ƒæè¿°çš„æ˜¯è´å¶æ–¯æ¨¡å‹ï¼Œæ¯”å¦‚è´å¶æ–¯çº¿æ€§å›å½’æ¨¡å‹ï¼Œè´å¶æ–¯é€»è¾‘å›å½’æ¨¡å‹ã€‚\n\n# æ•°å­¦è¡¨è¾¾å¼ä¸å›¾çš„å¯¹åº”\n\nå¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œå›¾æ¨¡å‹å°†æ•°å­¦è¡¨è¾¾å¼ä¸å›¾å¯¹åº”èµ·æ¥ï¼Œä»è€Œæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼å°†æ¦‚ç‡æ¨¡å‹çš„ç»“æ„å¯è§†åŒ–ã€‚\n\nåœ¨æœ‰å‘å›¾æ¨¡å‹ä¸­æ˜¯æ€æ ·å°†å¤æ‚çš„æ¦‚ç‡è¡¨è¾¾å¼å’Œå›¾å¯¹åº”èµ·æ¥çš„ï¼Ÿ\n\nç›´æ¥ä¸¾ä¾‹å¦‚ä¸‹ï¼š\n\n![](Bayesian-Networks-regression/Bayesian-Networks01.png)\n\næ ¹æ®ä¸Šå›¾å¯ä»¥ç›´æ¥å°†æ‰€æœ‰éšæœºå˜é‡çš„è”åˆæ¦‚ç‡åˆ†å¸ƒåˆ†è§£ä¸ºä¸‹å¼çš„å³è¾¹ï¼Œå¤šä¸ªå› å­çš„ä¹˜ç§¯ã€‚\n$$\np(x_1,x_2,x_3,x_4,x_5,x_6,x_7) = p(x_1)p(x_2)p(x3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,x_5)\n$$\n\nå…·ä½“ç†è®ºè¯·è§PRML 8.1ã€‚\n\nå¯¹äºæœ‰$K$ä¸ªèŠ‚ç‚¹çš„å›¾ï¼Œè¿™$K$ä¸ªèŠ‚ç‚¹çš„è”åˆåˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š\n$$\np(\\mathbf X) = \\prod_{k=1}^K p(x_k|pa_k)\n$$\nå…¶ä¸­$pa_k$æ˜¯èŠ‚ç‚¹$x_k$çš„æ‰€æœ‰çˆ¶èŠ‚ç‚¹çš„é›†åˆï¼Œ$\\mathbf XS = \\{ x_1, \\dots x_K\\}$ ã€‚\n\nè´å¶æ–¯ç½‘ç»œè¿™çš„å›¾å¿…é¡»æ˜¯æœ‰å‘æ— ç¯å›¾ã€‚\n\n# è´å¶æ–¯å›å½’çš„å›¾æ¨¡å‹\n\nå…ˆå›é¡¾ä¸€ä¸‹è´å¶æ–¯å›å½’ã€‚\n\nå‡è®¾è®­ç»ƒé›†æœ‰Nä¸ªæ ·æœ¬ï¼Œæ ·æœ¬é›†çš„ç‰¹å¾ç”¨$\\mathrm X$è¡¨ç¤ºï¼Œ$x_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬ã€‚æ ·æœ¬é›†çš„lableå€¼ç”¨$\\mathrm T$è¡¨ç¤ºï¼Œ$t_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬çš„lableå€¼ã€‚å³  $\\mathbf{X}\\equiv (x_{1} \\dots x_{N})^{\\mathrm{T}}$ï¼Œ$\\mathrm{T}=\\{t_{1} \\dots t_{N} \\}^{\\mathrm{T}}$ ï¼Œæ ·æœ¬é›†è¡¨ç¤ºä¸º$\\mathcal D = \\{\\mathrm X, \\mathrm{T} \\}$ ã€‚ åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒä¸€ä¸ªå›å½’æ¨¡å‹$y(x;\\mathrm w)$ ï¼Œä½¿ç”¨è¯¥æ¨¡å‹æ ¹æ®æ–°æ•°æ®çš„ç‰¹å¾é¢„æµ‹å…¶lableå€¼ã€‚\n\nçº¿æ€§å›å½’ï¼š$y(x ,\\mathrm w) = \\mathrm w ^{\\mathrm T} x $\n\nåœ¨å›å½’é—®é¢˜ä¸­ï¼Œè®¤ä¸ºlableå€¼$t$æœä»å‡å€¼ä¸º$y(x,\\mathrm w)$ï¼Œæ–¹å·®ä¸º$\\beta^{-1}$çš„**é«˜æ–¯åˆ†å¸ƒ**ã€‚\n$$\np(t|x,\\mathrm w, \\beta)=\\mathcal N(t|y(x , \\mathrm w),\\beta^{-1})\n$$\n$\\beta$ä¸ºé«˜æ–¯å™ªå£°ï¼Œååº”çš„æ˜¯æ ·æœ¬é›†çš„é‡‡æ ·è¯¯å·®å³å™ªå£°ã€‚\n\nè´å¶æ–¯å­¦æ´¾è®¤ä¸ºæ¨¡å‹ä¸­çš„å‚æ•°$\\mathrm w$æ˜¯ä¸€ä¸ªä¸ç¡®å®šçš„å€¼ï¼Œä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒå¯¹å…¶è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤„æˆ‘ä»¬å‡è®¾$\\mathrm{w}$çš„æ˜¯æœä»å‡å€¼ä¸º$0$æ–¹å·®ä¸º${\\alpha }^{-1}\\mathbf{I}$çš„é«˜æ–¯åˆ†å¸ƒ(ä¹Ÿå¯ä»¥è¿›è¡Œå…¶ä»–å‡è®¾ï¼Œå…¶ä»–æƒ…å†µå¯å‚è€ƒ[è´å¶æ–¯çº¿æ€§å›å½’ä¸è´å¶æ–¯é€»è¾‘å›å½’](https://weirping.github.io/blog/Bayesian-Probabilities-in-ML.html))ã€‚\n$$\np(\\mathrm{w}|\\alpha )=\\ \\mathcal{N}(\\mathrm{w}|0,{\\alpha }^{-1}\\mathbf{I})=(\\frac{\\alpha }{2\\pi })^{(M +1)/2}\\ \\exp \\{-\\frac{\\alpha }{2}\\mathrm{w}^{\\mathrm{T}}\\mathrm{w}\\}\n$$\næ€»ç»“ä¸€ä¸‹ä¸Šé¢æ¶‰åŠåˆ°åˆ°ç¬¦å·ï¼š\n\n| ç¬¦å·           | å«ä¹‰                  |\n| ------------ | ------------------- |\n| $x$ or $x_i$ | ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾             |\n| $\\mathbf X$  | æ ·æœ¬é›†çš„ç‰¹å¾              |\n| $t$ or $t_i$ | ä¸€ä¸ªæ ·æœ¬çš„lable          |\n| $\\mathrm T$  | æ ·æœ¬é›†çš„lable           |\n| $\\mathbf w$  | æ¨¡å‹çš„å‚æ•°               |\n| $\\beta$      | æ ·æœ¬è®°å¾—å™ªå£°              |\n| $\\alpha$     | $\\mathbf w$æ‰€æœä»åˆ†å¸ƒçš„å‚æ•° |\n\n\n\n**è´å¶æ–¯ç½‘ç»œè€ƒè™‘çš„ä¸»è¦æ˜¯éšæœºå˜é‡ã€‚ä¸ä¹‹ç­‰ä»·çš„æ˜¯æ‰€æœ‰éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒ**é‚£ä¹ˆåœ¨è´å¶æ–¯æ¨¡å‹ä¸­çš„éšæœºå˜é‡æœ‰å“ªäº›å‘¢ï¼Ÿ\n\nåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µåªæœ‰$\\mathbf w$å’Œ$\\mathrm T =(t_1,...,t_N)$æ˜¯éšæœºå˜é‡ï¼Œ  $\\mathbf{X}= (x_{1} \\dots x_{N})^{\\mathrm{T}}$ï¼Œ $\\beta$ å’Œ$\\alpha$è¢«ç§°ä¸ºdeterministic parametersï¼Œä»–ä»¬æ˜¯æ¨¡å‹çš„ï¼ˆè¶…ï¼‰å‚æ•°è€Œä¸æ˜¯éšæœºå˜é‡ã€‚\n\n## éšæœºå˜é‡çš„è´å¶æ–¯ç½‘\n\næ‰€æœ‰éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š\n$$\np(\\mathrm T,  \\mathbf w)=p(\\mathbf w)\\prod _{n=1}^{N}p(t_n|\\mathbf{w})\n$$\næ³¨æ„ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬ä¸­çš„lable $t$ éƒ½æ˜¯è”åˆåˆ†å¸ƒä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œä¹Ÿæ˜¯å›¾æ¨¡å‹çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚ä½¿ç”¨åœ†åœˆè¡¨ç¤ºéšæœºå˜é‡ï¼Œå…¶å›¾æ¨¡å‹è¡¨ç¤ºä¸ºå¦‚ä¸‹å¦‚æ‰€ç¤ºã€‚\n\n\n\n![](Bayesian-Networks-regression/line-bayesian01.png)\n\nå¯ä»¥çœ‹åˆ°ä¸Šå›¾ä¸­éœ€è¦æ˜¾ç¤ºé‡å¤è¡¨ç¤º$N$ä¸ª$t$èŠ‚ç‚¹ï¼Œå¤ªå¤æ‚äº†ã€‚å¯¹äºé‡å¤çš„èŠ‚ç‚¹å¯ä»¥æ”¹æˆä¸‹å›¾çš„è¡¨ç¤ºæ–¹æ³•ã€‚ä½¿ç”¨ä¸€ä¸ªæ–¹æ¡†(box)è¡¨ç¤ºé‡å¤èŠ‚ç‚¹ï¼Œå…¶ä¸­å³ä¸‹è§’çš„$N$ è¡¨ç¤ºé‡å¤æ¬¡æ•°ã€‚\n\n![](Bayesian-Networks-regression/line-bayesian02.png)\n\n## å¢åŠ æ¨¡å‹å‚æ•°\n\næœ‰æ—¶å€™æ˜¾ç¤ºçš„è¡¨è¾¾å‡ºæ¨¡å‹çš„å‚æ•°ï¼Œå¯¹äºé—®é¢˜çš„åˆ†ææ˜¯æœ‰å¸®åŠ©çš„ã€‚åŒ…å«æ¨¡å‹å‚æ•°çš„éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒè¡¨ç¤ºå¦‚ä¸‹ã€‚\n$$\np(\\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)=p(\\mathbf w|\\alpha )\\prod _{n=1}^Np(t_n|\\mathbf w,x_n,\\beta)\n$$\nåœ¨å›¾æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å‚æ•°è¡¨ç¤ºä¸ºå®å¿ƒå°åœ†ç‚¹ã€‚\n\n![](Bayesian-Networks-regression/line-bayesian03.png)\n\n## observed variables\n\nåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰çš„éšæœºå˜é‡ $\\mathrm T =(t_1,...,t_N)$ å¯¹äºæ¨¡å‹æ¥è¯´éƒ½æ˜¯å·²çŸ¥çš„ï¼Œå³è§‚æµ‹åˆ°çš„å˜é‡(observed variables)ã€‚ ç›¸åº”çš„ï¼Œ$\\mathbf w$ æ˜¯æœªè¢«è§‚æµ‹åˆ°çš„ï¼Œç§°ä¸ºéšå˜é‡(latent variable)ã€‚\n\nåœ¨è´å¶æ–¯ç½‘ä¸­ï¼Œè§‚æµ‹åˆ°çš„å˜é‡ä½¿ç”¨å®å¿ƒåœ†åœˆè¡¨ç¤ºï¼Œéšå˜é‡ä½¿ç”¨ç©ºå¿ƒåœ†åœˆè¡¨ç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](Bayesian-Networks-regression/line-bayesian04.png)\n\n## å¢åŠ é¢„æµ‹å˜é‡\n\næˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯å¯¹æ–°è¾“å…¥çš„å˜é‡è¿›è¡Œé¢„æµ‹ã€‚å‡è®¾ç»™å®šä¸€ä¸ªè¾“å¦‚å€¼$\\hat x$ï¼Œæˆ‘ä»¬æƒ³æ‰¾åˆ°ä»¥è§‚æµ‹æ•°æ®ä¸ºæ¡ä»¶çš„å¯¹åº”çš„$\\hat t$çš„æ¦‚ç‡åˆ†å¸ƒã€‚æè¿°è¿™ä¸ªé—®é¢˜çš„å›¾æ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](Bayesian-Networks-regression/line-bayesian05.png)\n\nè¿™ä¸ªæ¨¡å‹çš„æ‰€æœ‰éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒä¸º:\n$$\np(\\hat t,  \\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)=\\{\\prod _{n=1}^Np(t_n|\\mathbf w,x_n,\\beta)\\}p(\\mathbf w|\\alpha )p(\\hat t|\\hat x, \\mathbf w,\\beta)\n$$\n\n## æ€»ç»“\n\n- ä½¿ç”¨åœ†åœˆè¡¨ç¤ºéšæœºå˜é‡ï¼›\n\n- è§‚æµ‹åˆ°çš„å˜é‡ä½¿ç”¨å®å¿ƒåœ†åœˆè¡¨ç¤ºï¼Œéšå˜é‡ä½¿ç”¨ç©ºå¿ƒåœ†åœˆè¡¨ç¤ºï¼›\n\n- ä½¿ç”¨ä¸€ä¸ªæ–¹æ¡†(box)è¡¨ç¤ºé‡å¤èŠ‚ç‚¹ï¼Œå…¶ä¸­å³ä¸‹è§’çš„$N$ è¡¨ç¤ºé‡å¤æ¬¡æ•°ï¼›\n\n- æ¨¡å‹å‚æ•°è¡¨ç¤ºä¸ºå®å¿ƒå°åœ†ç‚¹ï¼Œè¿éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒä¸­æ˜¯æ¡ä»¶å˜é‡éƒ¨åˆ†ï¼Œå¦‚$p(\\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)$ï¼›\n\n\n# å‚æ•°çš„åéªŒåˆ†å¸ƒ\n\nå¯¹äºè®­ç»ƒæ•°æ®æ¥ä¹¦ï¼Œæ‰€æœ‰éšæœºå˜é‡çš„è”åˆåˆ†å¸ƒè¡¨ç¤ºå¦‚ä¸‹ï¼š\n$$\np(\\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)=p(\\mathbf w|\\alpha )\\prod _{n=1}^Np(t_n|\\mathbf w,x_n,\\beta)\n$$\næ ¹æ®è´å¶æ–¯å…¬å¼æœ‰å‚æ•°$\\mathbf w$çš„åéªŒåˆ†å¸ƒï¼š\n$$\np( \\mathbf w |\\mathrm T,  \\mathbf{X},\\alpha ,\\beta) = \\frac {p(\\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)}{p(\\mathrm T|\\mathbf{X},\\alpha ,\\beta)}\n$$\nå…¶ä¸­$\\mathrm T$æ˜¯è§‚å¯Ÿåˆ°çš„å˜é‡ï¼Œ$p(\\mathrm T|\\mathbf{X},\\alpha ,\\beta)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ‰€ä»¥ï¼š\n$$\np( \\mathbf w |\\mathrm T,  \\mathbf{X},\\alpha ,\\beta) \\propto p(\\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta) = p(\\mathbf w|\\alpha )\\prod _{n=1}^Np(t_n|\\mathbf w,x_n,\\beta)\n$$\n\n# é¢„æµ‹åˆ†å¸ƒ\n\nç”±å…¬å¼\n$$\np(\\hat t,  \\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta)=\\{\\prod _{n=1}^Np(t_n|\\mathbf w,x_n,\\beta)\\}p(\\mathbf w|\\alpha )p(\\hat t|\\hat x, \\mathbf w,\\beta)\n$$\nå¯¹äºæ–°æ•°æ®$\\hat t$åœ¨ç»™å®šè®­ç»ƒæ•°æ®é›†$\\{\\mathbf{X},\\mathrm T\\}$æ—¶çš„é¢„æµ‹åˆ†å¸ƒ\n$$\np(\\hat t | \\mathbf{X},\\mathrm T, \\alpha ,\\beta) = \\frac {p(\\hat t,  \\mathrm T | \\mathbf{X},\\alpha ,\\beta)}{p(\\mathrm T|\\mathbf{X},\\alpha ,\\beta)}\n$$\nå…¶ä¸­$\\mathrm T$æ˜¯è§‚å¯Ÿåˆ°çš„å˜é‡ï¼Œ$p(\\mathrm T|\\mathbf{X},\\alpha ,\\beta)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæ‰€ä»¥ï¼š\n$$\np(\\hat t | \\mathbf{X},\\mathrm T, \\alpha ,\\beta) \\propto p(\\hat t,  \\mathrm T | \\mathbf{X},\\alpha ,\\beta) = \\int p(\\hat t,  \\mathrm T,  \\mathbf w | \\mathbf{X},\\alpha ,\\beta) d{\\mathbf w}\n$$\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning\n\n\n\n","tags":["Math"],"categories":["Machine Learning"]},{"title":"Decorators in Python","url":"%2Fblog%2FDecorators-in-Python.html","content":"\næœ¬æ–‡å‚è€ƒä¹‹å‰é˜…è¯»è¿‡çš„æ–‡ç« [Python å‡½æ•°è£…é¥°å™¨](http://www.runoob.com/w3cnote/python-func-decorators.html) å’Œ[Pythonè¿›é˜¶](https://eastlakeside.gitbooks.io/interpy-zh/content/decorators/)ä¸­å…³äºè£…é¥°å™¨çš„å†…å®¹æ•´ç†è€Œå¾—ã€‚åŸæ–‡ä¸­å­˜åœ¨ä¸€äº›å†…å®¹æˆ‘å·²ç»è¾ƒä¸ºç†Ÿæ‚‰ï¼Œç¨å¾®å¸¦è¿‡ï¼Œé‡ç‚¹è®°å½•ä¸å¤ªç†Ÿæ‚‰çš„éƒ¨åˆ†ã€‚\n\nè£…é¥°å™¨(Decorators)æ˜¯ Python çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚ç®€å•åœ°è¯´ï¼šä»–ä»¬æ˜¯ä¿®æ”¹å…¶ä»–å‡½æ•°çš„åŠŸèƒ½çš„å‡½æ•°ã€‚ä»–ä»¬æœ‰åŠ©äºè®©æˆ‘ä»¬çš„ä»£ç æ›´ç®€çŸ­ï¼Œä¹Ÿæ›´Pythonicï¼ˆPythonèŒƒå„¿ï¼‰ã€‚\n\næœ¬æ–‡è®¨è®ºå¦‚ä½•å†™ä½ è‡ªå·±çš„è£…é¥°å™¨ã€‚åœ¨æ­£å¼å¼€å§‹ä¹‹å‰ï¼Œå…ˆæ˜ç¡®å‡ ä¸ªæ¦‚å¿µï¼š\n\n- åœ¨pythonä¸­ä¸€åˆ‡çš†æ˜¯å¯¹è±¡ã€‚å‡½æ•°ï¼Œç±»å¯¹äº‹å¯¹è±¡ï¼›\n- ä¸€ä¸ªå‡½æ•°å¯ä»¥æ˜¯å¦ä¸€ä¸ªå‡½æ•°çš„å‚æ•°ã€è¿”å›å€¼ï¼›\n- å‡½æ•°å†…éƒ¨å¯ä»¥å®šä¹‰å‡½æ•°\n- é—­åŒ…ï¼špythonä¸­çš„è£…é¥°å™¨å®é™…ä¸Šæ˜¯ä¸€ä¸ªé—­åŒ…ã€‚åµŒå¥—å®šä¹‰åœ¨éå…¨å±€ä½œç”¨åŸŸé‡Œé¢çš„å‡½æ•°èƒ½å¤Ÿè®°ä½å®ƒåœ¨è¢«å®šä¹‰çš„æ—¶å€™å®ƒæ‰€å¤„çš„å°é—­å‘½åç©ºé—´ã€‚\n\n# ç¬¬ä¸€ä¸ªè£…é¥°å™¨\n\n## è£…é¥°å™¨å‡½æ•°\n\n```python\ndef a_decorator(a_func):\n\n    def wrapTheFunction():\n        print(\"I am doing some boring work before executing a_func()\")\n        a_func()\n        print(\"I am doing some boring work after executing a_func()\")\n    return wrapTheFunction\n\ndef a_function_requiring_decoration():\n    print(\"I am the function which needs some decoration to remove my foul smell\")\n\na_function_requiring_decoration()\n#outputs: \"I am the function which needs some decoration to remove my foul smell\"\n\na_function_requiring_decoration = a_decorator(a_function_requiring_decoration)\n\na_function_requiring_decoration()\n#outputs:I am doing some boring work before executing a_func()\n#        I am the function which needs some decoration to remove my foul smell\n#        I am doing some boring work after executing a_func()\n```\n\nä¸Šé¢çš„ä»£ç å®šä¹‰äº†ä¸€ä¸ªéœ€è¦è£…é¥°çš„å‡½æ•°`a_function_requiring_decoration`ï¼Œç”¨å‡½æ•°`a_decorator`ä½œä¸ºè£…é¥°å™¨ï¼Œå¯¹`a_function_requiring_decoration`çš„è¡Œä¸ºè¿›è¡Œä¿®é¥°ã€‚\n\nè£…é¥°å™¨çš„ä½œç”¨å°±æ˜¯å¯¹ä¸€ä¸ªå‡½æ•°è¿›è¡Œå°è£…ï¼Œå¹¶ä¸”ç”¨è¿™æ ·æˆ–è€…é‚£æ ·çš„æ–¹å¼æ¥å¯¹å®ƒçš„è¡Œä¸ºè¿›è¡Œä¿®é¥°ã€‚\n\n## ä½¿ç”¨@ç¬¦å·\n\nç°åœ¨ä½ ä¹Ÿè®¸ç–‘æƒ‘ï¼Œæˆ‘ä»¬åœ¨ä»£ç é‡Œå¹¶æ²¡æœ‰ä½¿ç”¨@ç¬¦å·ï¼Ÿé‚£åªæ˜¯ä¸€ä¸ªç®€çŸ­çš„æ–¹å¼æ¥ç”Ÿæˆä¸€ä¸ªè¢«è£…é¥°çš„å‡½æ•°ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨@æ¥è¿è¡Œä¹‹å‰çš„ä»£ç ã€‚\n\n```python\ndef a_decorator(a_func):\n\n    def wrapTheFunction():\n        print(\"I am doing some boring work before executing a_func()\")\n        a_func()\n        print(\"I am doing some boring work after executing a_func()\")\n    return wrapTheFunction\n\n@a_decorator\ndef a_function_requiring_decoration():\n    print(\"I am the function which needs some decoration to remove my foul smell\")\n# ç­‰ä»·äº a_function_requiring_decoration = a_decorator(a_function_requiring_decoration)\n    \na_function_requiring_decoration()\n#outputs: I am doing some boring work before executing a_func()\n#         I am the function which needs some decoration to remove my foul smell\n#         I am doing some boring work after executing a_func()\n```\n\nå¯ä»¥å‘ç°@ç¬¦å·å’Œ`a_function_requiring_decoration = a_decorator(a_function_requiring_decoration)`æ˜¯ç­‰ä»·çš„ã€‚\n\n## å‡½æ•°å±æ€§\n\nå¦‚æœæˆ‘ä»¬è¿è¡Œå¦‚ä¸‹ä»£ç ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼š\n\n```python\nprint(a_function_requiring_decoration.__name__)\n# Output: wrapTheFunction\n```\n\nè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Ouputè¾“å‡ºåº”è¯¥æ˜¯â€œa_function_requiring_decorationâ€ã€‚è¿™é‡Œçš„å‡½æ•°è¢«warpTheFunctionæ›¿ä»£äº†ã€‚å®ƒé‡å†™äº†æˆ‘ä»¬å‡½æ•°çš„åå­—å’Œæ³¨é‡Šæ–‡æ¡£(docstring)ã€‚å¹¸è¿çš„æ˜¯Pythonæä¾›ç»™æˆ‘ä»¬ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯functools.wrapsã€‚æˆ‘ä»¬ä¿®æ”¹ä¸Šä¸€ä¸ªä¾‹å­æ¥ä½¿ç”¨functools.wrapsï¼š\n\n```python\nfrom functools import wraps\n\ndef a_decorator(a_func):\n\n    @wraps(a_func)\n    def wrapTheFunction():\n        print(\"I am doing some boring work before executing a_func()\")\n        a_func()\n        print(\"I am doing some boring work after executing a_func()\")\n    return wrapTheFunction\n\n@a_decorator\ndef a_function_requiring_decoration():\n    print(\"I am the function which needs some decoration to remove my foul smell\")\n\nprint(a_function_requiring_decoration.__name__)\n# Output: a_function_requiring_decoration\n```\n\n@wrapsæ¥å—ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œè£…é¥°ï¼Œå¹¶åŠ å…¥äº†å¤åˆ¶å‡½æ•°åç§°ã€æ³¨é‡Šæ–‡æ¡£ã€å‚æ•°åˆ—è¡¨ç­‰ç­‰çš„åŠŸèƒ½ã€‚è¿™å¯ä»¥è®©æˆ‘ä»¬åœ¨è£…é¥°å™¨é‡Œé¢è®¿é—®åœ¨è£…é¥°ä¹‹å‰çš„å‡½æ•°çš„å±æ€§ã€‚\n\n# åº”ç”¨åœºæ™¯\n\nç°åœ¨æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è£…é¥°å™¨åœ¨å“ªäº›åœ°æ–¹ç‰¹åˆ«è€€çœ¼ï¼Œä»¥åŠä½¿ç”¨å®ƒå¯ä»¥è®©ä¸€äº›äº‹æƒ…ç®¡ç†èµ·æ¥å˜å¾—æ›´ç®€å•ã€‚\n\n## æˆæƒ\n\nè£…é¥°å™¨èƒ½æœ‰åŠ©äºæ£€æŸ¥æŸä¸ªäººæ˜¯å¦è¢«æˆæƒå»ä½¿ç”¨ä¸€ä¸ªwebåº”ç”¨çš„ç«¯ç‚¹(endpoint)ã€‚å®ƒä»¬è¢«å¤§é‡ä½¿ç”¨äºFlaskå’ŒDjango webæ¡†æ¶ä¸­ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­æ¥ä½¿ç”¨åŸºäºè£…é¥°å™¨çš„æˆæƒï¼š\n\n```python\nfrom functools import wraps\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth = request.authorization\n        if not auth or not check_auth(auth.username, auth.password):\n            authenticate()\n        return f(*args, **kwargs)\n    return decorated\n```\n\n## æ—¥å¿—\n\næ—¥å¿—æ˜¯è£…é¥°å™¨è¿ç”¨çš„å¦ä¸€ä¸ªäº®ç‚¹ã€‚è¿™æ˜¯ä¸ªä¾‹å­ï¼š\n\n```python\nfrom functools import wraps\n\ndef logit(func):\n    @wraps(func)\n    def with_logging(*args, **kwargs):\n        print(func.__name__ + \" was called\")\n        return func(*args, **kwargs)\n    return with_logging\n\n@logit\ndef addition_func(x):\n   \"\"\"Do some math.\"\"\"\n   return x + x\n\n\nresult = addition_func(4)\n# Output: addition_func was called\n```\n\n# å¸¦å‚æ•°çš„è£…é¥°å™¨\n\næ¥æƒ³æƒ³è¿™ä¸ªé—®é¢˜ï¼Œéš¾é“`@wraps`ä¸ä¹Ÿæ˜¯ä¸ªè£…é¥°å™¨å—ï¼Ÿä½†æ˜¯ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªå‚æ•°ï¼Œå°±åƒä»»ä½•æ™®é€šçš„å‡½æ•°èƒ½åšçš„é‚£æ ·ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ä¹Ÿé‚£æ ·åšå‘¢ï¼Ÿ\n\næˆ‘ä»¬å›åˆ°æ—¥å¿—çš„ä¾‹å­ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåŒ…è£¹å‡½æ•°ï¼Œèƒ½è®©æˆ‘ä»¬æŒ‡å®šä¸€ä¸ªç”¨äºè¾“å‡ºçš„æ—¥å¿—æ–‡ä»¶ã€‚\n\n```python\nfrom functools import wraps\n\ndef logit(logfile='out.log'):  # æ­¤æ—¶æ˜¯ä¸‰å±‚å‡½æ•°åµŒå¥—\n    def logging_decorator(func):\n        @wraps(func)\n        def wrapped_function(*args, **kwargs):\n            log_string = func.__name__ + \" was called\"\n            print(log_string)\n            # æ‰“å¼€logfileï¼Œå¹¶å†™å…¥å†…å®¹\n            with open(logfile, 'a') as opened_file:\n                # ç°åœ¨å°†æ—¥å¿—æ‰“åˆ°æŒ‡å®šçš„logfile\n                opened_file.write(log_string + '\\n')\n            return func(*args, **kwargs)\n        return wrapped_function\n    return logging_decorator\n\n@logit()  # ç­‰ä»·äº myfunc1=logit()(myfunc1)\ndef myfunc1():\n    pass\n# ç­‰ä»·äº myfunc1=logit()(myfunc1)\n\nmyfunc1()\n# Output: myfunc1 was called\n# ç°åœ¨ä¸€ä¸ªå«åš out.log çš„æ–‡ä»¶å‡ºç°äº†ï¼Œé‡Œé¢çš„å†…å®¹å°±æ˜¯ä¸Šé¢çš„å­—ç¬¦ä¸²\n\n@logit(logfile='func2.log')  # ç­‰ä»·äº myfunc1=logit(logfile='func2.log')(myfunc1)\ndef myfunc2():\n    pass\n\nmyfunc2()\n# Output: myfunc2 was called\n# ç°åœ¨ä¸€ä¸ªå«åš func2.log çš„æ–‡ä»¶å‡ºç°äº†ï¼Œé‡Œé¢çš„å†…å®¹å°±æ˜¯ä¸Šé¢çš„å­—ç¬¦ä¸²\n```\n\n\n\n# è£…é¥°å™¨ç±»\n\nç°åœ¨æˆ‘ä»¬æœ‰äº†èƒ½ç”¨äºæ­£å¼ç¯å¢ƒçš„logitè£…é¥°å™¨ï¼Œä½†å½“æˆ‘ä»¬çš„åº”ç”¨çš„æŸäº›éƒ¨åˆ†è¿˜æ¯”è¾ƒè„†å¼±æ—¶ï¼Œå¼‚å¸¸ä¹Ÿè®¸æ˜¯éœ€è¦æ›´ç´§æ€¥å…³æ³¨çš„äº‹æƒ…ã€‚æ¯”æ–¹è¯´æœ‰æ—¶ä½ åªæƒ³æ‰“æ—¥å¿—åˆ°ä¸€ä¸ªæ–‡ä»¶ã€‚è€Œæœ‰æ—¶ä½ æƒ³æŠŠå¼•èµ·ä½ æ³¨æ„çš„é—®é¢˜å‘é€åˆ°ä¸€ä¸ªemailï¼ŒåŒæ—¶ä¹Ÿä¿ç•™æ—¥å¿—ï¼Œç•™ä¸ªè®°å½•ã€‚è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ç»§æ‰¿çš„åœºæ™¯ï¼Œä½†ç›®å‰ä¸ºæ­¢æˆ‘ä»¬åªçœ‹åˆ°è¿‡ç”¨æ¥æ„å»ºè£…é¥°å™¨çš„å‡½æ•°ã€‚\n\nå¹¸è¿çš„æ˜¯ï¼Œç±»ä¹Ÿå¯ä»¥ç”¨æ¥æ„å»ºè£…é¥°å™¨ã€‚é‚£æˆ‘ä»¬ç°åœ¨ä»¥ä¸€ä¸ªç±»è€Œä¸æ˜¯ä¸€ä¸ªå‡½æ•°çš„æ–¹å¼ï¼Œæ¥é‡æ–°æ„å»ºlogitã€‚\n\n```python\nfrom functools import wraps\n \nclass logit(object):\n    def __init__(self, logfile='out.log'):\n        self.logfile = logfile\n \n    def __call__(self, func):\n        @wraps(func)\n        def wrapped_function(*args, **kwargs):\n            log_string = func.__name__ + \" was called\"\n            print(log_string)\n            # æ‰“å¼€logfileå¹¶å†™å…¥\n            with open(self.logfile, 'a') as opened_file:\n                # ç°åœ¨å°†æ—¥å¿—æ‰“åˆ°æŒ‡å®šçš„æ–‡ä»¶\n                opened_file.write(log_string + '\\n')\n            # ç°åœ¨ï¼Œå‘é€ä¸€ä¸ªé€šçŸ¥\n            self.notify()\n            return func(*args, **kwargs)\n        return wrapped_function\n \n    def notify(self):\n        # logitåªæ‰“æ—¥å¿—ï¼Œä¸åšåˆ«çš„\n        pass\n```\n\nè¿™ä¸ªå®ç°æœ‰ä¸€ä¸ªé™„åŠ ä¼˜åŠ¿ï¼Œåœ¨äºæ¯”åµŒå¥—å‡½æ•°çš„æ–¹å¼æ›´åŠ æ•´æ´ï¼Œè€Œä¸”åŒ…è£¹ä¸€ä¸ªå‡½æ•°è¿˜æ˜¯ä½¿ç”¨è·Ÿä»¥å‰ä¸€æ ·çš„è¯­æ³•ï¼š\n\n```python\n@logit(logfile='out.log')  # ç­‰ä»·äº myfunc1=logit(logfile='out.log')(myfunc1)\ndef myfunc1():\n    pass\n```\n\nç°åœ¨ï¼Œæˆ‘ä»¬ç»™`logit`åˆ›å»ºå­ç±»ï¼Œæ¥æ·»åŠ emailçš„åŠŸèƒ½(è™½ç„¶emailè¿™ä¸ªè¯é¢˜ä¸ä¼šåœ¨è¿™é‡Œå±•å¼€)ã€‚\n\n```python\nclass email_logit(logit):\n    '''\n    ä¸€ä¸ªlogitçš„å®ç°ç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨å‡½æ•°è°ƒç”¨æ—¶å‘é€emailç»™ç®¡ç†å‘˜\n    '''\n    def __init__(self, email='admin@myproject.com', *args, **kwargs):\n        self.email = email\n        super(email_logit, self).__init__(*args, **kwargs)\n\n    def notify(self):\n        # å‘é€ä¸€å°emailåˆ°self.email\n        # è¿™é‡Œå°±ä¸åšå®ç°äº†\n        pass\n```\n\nä»ç°åœ¨èµ·ï¼Œ`@email_logit`å°†ä¼šå’Œ`@logit`äº§ç”ŸåŒæ ·çš„æ•ˆæœï¼Œä½†æ˜¯åœ¨æ‰“æ—¥å¿—çš„åŸºç¡€ä¸Šï¼Œè¿˜ä¼šå¤šå‘é€ä¸€å°é‚®ä»¶ç»™ç®¡ç†å‘˜ã€‚\n\n\n\n# å‚è€ƒèµ„æ–™\n\nhttps://eastlakeside.gitbooks.io/interpy-zh/content/decorators/\n\n\n\n\n\n","tags":["Python"],"categories":["Python"]},{"title":"OneHotEncoder in Sklearn","url":"%2Fblog%2FOneHotEncoder-in-Sklearn.html","content":"\næœ€è¿‘åœ¨æ˜¯æœ‰xgboostè®­ç»ƒæ•°æ®ã€‚åœ¨ç‰¹å¾é¢„å¤„ç†é˜¶æ®µä½¿ç”¨äº†`OneHotEncoder`æ¥å¤„ç†nominalç±»å‹çš„åˆ†ç±»ç‰¹å¾ï¼Œæ¨¡å‹è®­ç»ƒå¥½ä»¥åéœ€è¦åè¿‡æ¥åˆ†æç‰¹å¾ï¼Œé‚£ä¹ˆéœ€è¦å°†åŸå§‹æ•°æ®ä¸­çš„ç‰¹å¾ä¸ç¼–ç æ•°æ®çš„ç‰¹å¾å¯¹åº”èµ·æ¥ã€‚é‚£ä¹ˆ`OneHotEncoder`æ˜¯æ€ä¹ˆå¯¹åº”èµ·æ¥çš„å‘¢ï¼Ÿ\n\né€šè¿‡å…¶å®˜æ–¹çš„ä¾‹å­åˆ†æäº†ä¸€ä¸‹ï¼Œä¸‹é¢è®°å½•å¦‚ä¸‹ã€‚\n\n`original data`ï¼šåŸå§‹æ•°æ®ã€‚\n\n`encode data`ï¼šç»è¿‡one hotç¼–ç ä»¥åçš„æ•°æ®ã€‚\n\næœ¬æ–‡è¦åˆ†æçš„å°±æ˜¯`OneHotEncoder`æ€ä¹ˆå°†æ•°æ®ä»`original data`å˜æˆ`encode data`çš„ã€‚å…¶è§„åˆ™å¦‚ä¸‹ï¼š\n\n- ç¼–ç åçš„ç‰¹å¾prependåœ¨æ–°æ•°æ®ä¸Šï¼Œå¹¶åˆ é™¤åŸå§‹ç‰¹å¾ã€‚éåˆ†ç±»ç‰¹å¾ç›´æ¥ä¿ç•™\n- `original data`ä¸­æ¯ä¸ªåˆ†ç±»ç‰¹å¾å¯ä»¥ç¼–ç æˆæ–°ç‰¹å¾çš„ä¸ªæ•°ä¾æ¬¡è®°å½•åœ¨`n_values_`å±æ€§ä¸­\n- `original data`ä¸­æ¯ä¸ªåˆ†ç±»ç‰¹å¾åœ¨`encode data`ä¸­å¯¹åº”çš„æ–°çš„ç‰¹å¾ç»´åº¦é€‰å–æ–¹å¼è®°å½•åœ¨`feature_indices_`ä¸­\n\n\n\n\n\n```python\nfrom sklearn.preprocessing import OneHotEncoder\nori_data = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\nenc = OneHotEncoder(categorical_features=[1, 2])\nenc.fit(ori_data)\nprint 'categorical_features:', enc.categorical_features\nprint 'n_values:', enc.n_values_\nprint 'feature_indices:', enc.feature_indices_\nprint 'transform:', enc.transform([[1, 1, 1]]).toarray()\n\n# categorical_features: [1, 2]\n# n_values: [3 4]\n# feature_indices: [0 3 7]\n# transform: [[ 0.  1.  0.  0.  1.  0.  0.  1.]]\n```\n\næ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤º\n\n![](OneHotEncoder-in-Sklearn/onehotencode.png)\n\nå¦‚ä¸Šä¾‹å­ï¼š\n\n- `original data`ä¸­ç¬¬1ã€2ä¸¤ä¸ªç»´åº¦ä¸ºåˆ†ç±»ç‰¹å¾`categorical_features=[1, 2]`ã€‚\n- è¿™ä¸¤ä¸ªç‰¹å¾æ¯ä¸ªç‰¹å¾åŒ…å«çš„å¯èƒ½çš„å€¼çš„æ•°é‡åˆ†åˆ«ä¸º3å’Œ4ã€‚ `n_values: [3 4]`\n- äº§ç”Ÿçš„æ–°çš„ç‰¹å¾è¢«prependåœ¨`encode data`ä¸­é å‰çš„è‹¥å¹²åˆ—ä¸­ã€‚`feature_indices: [0 3 7]`:\n  - åŸå§‹ç‰¹å¾çš„ é¦–ä¸ªåˆ†ç±»ç‰¹å¾ Feature1 => EncodeData[:, 0: 3]\n  - åŸå§‹ç‰¹å¾çš„ ç¬¬äºŒä¸ªåˆ†ç±»ç‰¹å¾ Feature2 => EncodeData[:, 3: 7]\n\nå‚è€ƒèµ„æ–™\n\n[`sklearn.preprocessing`.OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#)\n\n","tags":["Sklearn"],"categories":["Sklearn"]},{"title":"Xgboost Rank in Sklearn","url":"%2Fblog%2Fxgboost-rank-in-sklearn.html","content":"\n\nxgboostæä¾›äº†pythonæ¥å£ï¼ŒåŒæ—¶éƒ¨åˆ†æ”¯æŒsklearnã€‚åœ¨åˆ†ç±»ä»»åŠ¡å’Œå›å½’ä»»åŠ¡ä¸­æä¾›äº†XGBClassifierå’ŒXGBRegressorä¸¤ä¸ªç±»ï¼Œè¿™ä¸¤ä¸ªç±»å¯ä»¥å½“åšsklearnä¸­çš„estimatorä½¿ç”¨ï¼Œä¸sklearnæ— ç¼è¡”æ¥ã€‚\n\nxgboostæ˜¯æ”¯æŒrankä»»åŠ¡çš„ï¼Œä½†æ˜¯å®ƒå´æ²¡æœ‰æä¾›rankåŠŸèƒ½çš„sklearnçš„æ”¯æŒã€‚è¿™å¯¹äºåƒæˆ‘è¿™æ ·çš„åšltrå¹¶ä¸”å¸¸ç”¨sklearnçš„å¼€å‘äººå‘˜æ˜¯ä½•ç­‰çš„ä¸çˆ½ã€‚\n\nè‡ªå·±åŠ¨æ‰‹ä¸°è¡£è¶³é£Ÿï¼Œå®ç°ä¸€ä¸‹ã€‚å‘½åä¸ºXGBRankerã€‚\n\n*note: åœ¨æˆ‘æ­£åœ¨åšè¿™ä¸ªäº‹çš„æ—¶å€™å‘ç°githubä¸Šå·²ç»æœ‰äººåœ¨åšè¿™ä¸ªäº‹äº†ï¼Œ[åœ°å€åœ¨æ­¤](https://github.com/bigdong89/xgboostExtension)ï¼Œ é€šè¿‡é˜…è¯»ä»–çš„ä»£ç æˆ‘å‘ç°å…³äºgroupçš„å¤„ç½®æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æƒ³æ³•æ˜¯ä¸€æ ·çš„ã€‚æ‰€ä»¥æˆ‘æœ‰ä¸€éƒ¨åˆ†ä»£ç ç›´æ¥å‚è€ƒäº†ä»–çš„ã€‚*\n\nä¸»è¦å·¥ä½œå¦‚ä¸‹ï¼š\n\n1. XGBRankerä¸­`fit`å’Œ`predict`çš„å®ç°ï¼ˆå‚è€ƒhttps://github.com/bigdong89/xgboostExtensionï¼‰\n2. ndcg_scoreçš„å®ç°\n3. æ”¯æŒGridSearchCV\n\n# groupé—®é¢˜\n\nåœ¨sklearnä¸­estimatorçš„ç­¾åæ˜¯è¿™æ ·çš„`estimater.fit(X, y)`å’Œ`estimator.predict(X, y)`ï¼Œå³ï¼Œä»»åŠ¡è®­ç»ƒæ•°æ®åˆ†ä¸ºç‰¹å¾`X`å’Œlable`y`ã€‚ ä½†æ˜¯åœ¨lträ»»åŠ¡ä¸­ï¼Œæ•°æ®åˆ†ä¸ºä¸‰éƒ¨åˆ†`X`ã€ `y`ã€ `group`ï¼Œå¦‚ä¸‹å¦‚æ‰€ç¤ºï¼š\n\n![](xgboost-rank-in-sklearn/dataset.png)\n\nä¸ºäº†ä¿æŒç­¾åä¸€è‡´ï¼Œæˆ‘çš„åšæ³•æ˜¯å°†`group`åˆå¹¶åˆ°featrueä¸­ï¼Œåœ¨XGBRankerçš„`fit`ï¼Œ`predict`ç­‰æ–¹æ³•ä¸­å†å°†å…¶åˆ†ç¦»ã€‚\n\n# ndcg_score\n\nè¯„ä»·ltræ¨¡å‹çš„ä¸€ä¸ªé‡è¦æ ‡å‡†å°±æ˜¯ndcgã€‚æˆ‘å®ç°äº†`ndcg_at_5_scoring, ndcg_at_3_scoring, ndcg_at_10_scoring` ä¸‰ä¸ªæŒ‡æ ‡ã€‚\n\nnote: æˆ‘æ²¡æœ‰ä½¿ç”¨ç»§æ‰¿`_BaseScorer`çš„æ–¹æ³•å®ç°ä»¥ä¸Šä¸‰ä¸ªscoringã€‚å¦‚æœæœ‰éœ€æ±‚æˆ‘åœ¨å¢åŠ ã€‚\n\n\n\n# GridSearchCV\n\nsklearnæä¾›äº†ä¸€äº›æ–¹ä¾¿çš„å·¥å…·ç”¨äºHyperparameter Tuningï¼Œå¦‚cross_val_scoreã€validation curveså’ŒGridSearchCVï¼Œå…¶ä¸­GridSearchCVæ˜¯æˆ‘å¸¸ç”¨çš„ä¸€ä¸ªå·¥å…·ã€‚\n\nå…¶å®åœ¨GridSearchCVä¸­çš„fitæ–¹æ³•ç­¾å`GridSearchCV.fit(X[, y, groups])`ä¸­å¯ä»¥çœ‹å‡ºGridSearchCVæ˜¯æ”¯æŒgroupså‚æ•°çš„ï¼Œé€šè¿‡é˜…è¯»ä»£ç å‘ç°è¯¥å‚æ•°æ˜¯ç”¨äºåƒ`GroupKFold`è¿™æ ·çš„æ•°æ®åˆ†å‰²ç±»åšäº¤å‰éªŒè¯çš„ï¼Œè¿™ä¸ªgroupså‚æ•°å¹¶æ²¡æœ‰ä¼ å¦‚estimaterä¸­ã€‚è¿™ä¹Ÿå¯ä»¥ä»ä¾§é¢è¯´æ˜sklearnä¸­çš„estimateræœ€åˆçš„è®¾è®¡ä¸Šåªè€ƒè™‘äº†å›å½’ï¼Œåˆ†ç±»å’Œèšç±»è¿™æ ·çš„ä¼ ç»Ÿä»»åŠ¡ï¼Œä¸ºè€ƒè™‘ltrç±»å‹çš„ä»»åŠ¡ã€‚\n\n# ä»£ç ä»‹ç»\n\n- ndcg_scorer.pyï¼šå®ç°`ndcg_at_5_scoring, ndcg_at_3_scoring, ndcg_at_10_scoring`\n- rank_GroupKFold.pyï¼šç”¨äºltrçš„äº¤å‰éªŒè¯æ•°æ®åˆ†å‰²æ–¹æ³•ï¼ŒåŸºäºGroupKFold.pyå®ç°ã€‚\n- rank_sklearn.py ï¼š XGBRankerå®ç°ç±»\n- rank_util.pyï¼šå·¥å…·ç±»ï¼Œç”¨äºç¬¬ä¸€åˆ—ä¸ºgroupä¿¡æ¯çš„æ•°æ®\n- xgbranker_sklearn_example.ipynbï¼š å®ä¾‹ä»£ç ï¼ŒåŒ…æ‹¬æ¨¡å‹è®­ç»ƒï¼Œé¢„æµ‹ï¼ŒGridSearchCVçš„ä½¿ç”¨\n\n\n\nä»£ç åœ°å€ï¼šhttps://github.com/Weirping/xgbranker_sklearn.git\n\n\n\n# å‚è€ƒ\n\nhttps://github.com/bigdong89/xgboostExtension\n","tags":["Sklearn"],"categories":["Sklearn"]},{"title":"Hyperparameter Tuning in Sklearn","url":"%2Fblog%2FHyperparameter-Tuning-in-Sklearn.html","content":"\næœ¬æ–‡æ•´ç†è®°å½•åœ¨sklearnä¸­æä¾›çš„æ¨¡å‹è°ƒå‚çš„å‡ ä¸ªå·¥å…·ã€‚æ¶‰åŠå†…å®¹æœ‰cross_val_scoreã€validation curveså’ŒGridSearchCVã€‚\n\nåœ¨æœºå™¨å­¦ä¹ ä¸­ä¸€èˆ¬å­˜åœ¨ä¸¤ç§ç±»å‹çš„å‚æ•°ï¼š\n\n- ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„å‚æ•°ï¼Œå¦‚é€»è¾‘å›å½’çš„æƒé‡å‚æ•°$W$ ã€‚\n- è¶…å‚æ•°(hyperparameters)ï¼Œå¦‚é€»è¾‘å›å½’ä¸­çš„æ­£åˆ™åŒ–ç³»æ•°ï¼Œå†³ç­–æ ‘ä¸­çš„depthå‚æ•°(æ§åˆ¶æ ‘çš„æ·±åº¦)ã€‚\n\nè¶…å‚æ•°æ˜¯åœ¨æ¨¡å‹è®­ç»ƒå‰äººä¸ºæŒ‡å®šçš„ï¼Œå®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šå†³å®šäº†æ¨¡å‹æ•ˆæœçš„å¥½åã€‚Skleanæä¾›äº†æ¥é€‰å–è¶…å‚æ•°çš„å·¥å…·ã€‚\n\n# äº¤å‰éªŒè¯-æ¨¡å‹æ•ˆæœçš„è¯„ä»·\n\né€‰å–è¶…å‚çš„æ ‡å‡†æ˜¯æ¨¡å‹æ•ˆæœï¼Œé€šå¸¸æˆ‘ä»¬å¯¹æ¨¡å‹æ•ˆæœçš„è¯„ä»·æ–¹æ³•å°±æ˜¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯„ä»·æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•å°±æ˜¯äº¤å‰éªŒè¯ã€‚\n\näº¤å‰éªŒè¯è®¾è®¡åˆ°ä¸¤ä¸ªæ–¹é¢ï¼š1. æ•°æ®é›†åˆ†å‰²ï¼› 2. æ³›åŒ–æŒ‡æ ‡è®¡ç®—ã€‚\n\n## æ•°æ®åˆ†å‰²æ–¹å¼\n\nnote: äº¤å‰éªŒè¯çš„æ–¹æ³•æ˜¯å’Œæ•°æ®é›†åˆ†å‰²æ–¹æ³•ç´§å¯†ç›¸è¿çš„ï¼Œæœ¬èŠ‚ä»‹ç»æ•°æ®åˆ†å‰²æ–¹å¼ä¹Ÿæ˜¯ä»‹ç»äº¤å‰éªŒè¯æ–¹æ³•ã€‚\n\nå¸¸ç”¨çš„äº¤å‰éªŒè¯æ–¹æ³•å¦‚ä¸‹ï¼š\n\n- holdout cross-validation ï¼šå°†æ•°æ®åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œæµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹æ•ˆæœ(æ³›åŒ–èƒ½åŠ›)\n  holdoutæ–¹æ³•çš„ä¸è¶³: å¯¹æ•°æ®åˆ†å‰²æ–¹æ³•æ•æ„Ÿï¼ŒåŸå§‹æ•°æ®çš„ä¸åŒåˆ†å‰²æ–¹æ³•å½±å“æ¨¡å‹æ•ˆæœã€‚\n\n- holdout cross-validation V2ï¼šå°†åŸå§‹æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°(è¶…å‚)ä¸åœçš„åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶ç”¨éªŒè¯é›†éªŒè¯æ¨¡å‹æ•ˆæœåªèƒ½æ‰¾åˆ°ä¸€ä¸ªè¾ƒå¥½çš„æ¨¡å‹ï¼Œæœ€åç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹æ•ˆæœ(æ³›åŒ–èƒ½åŠ›)ã€‚å…¶å®è¯¥æ–¹æ³•å·²ç»å…·æœ‰äº†æ¨¡å‹è°ƒä¼˜çš„åŠŸèƒ½ã€‚\n  holdoutæ–¹æ³•çš„ä¸è¶³: å¯¹æ•°æ®åˆ†å‰²æ–¹æ³•æ•æ„Ÿï¼ŒåŸå§‹æ•°æ®çš„ä¸åŒåˆ†å‰²æ–¹æ³•å½±å“æ¨¡å‹æ•ˆæœã€‚\n  ![](Hyperparameter-Tuning-in-Sklearn/holdout.png)\n\n- K-fold cross-validationï¼š éšæœºå°†æ•°æ®åˆ†å‰²ä¸º$k$ä¸ªfoldsï¼Œç”¨å…¶ä¸­çš„$k-1$ä¸ªä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹çš„ä¸€ä¸ªä½œä¸ºæµ‹è¯•é›†ã€‚é‚£ä¹ˆå¯ä»¥å¾—åˆ°è¯¥æ¨¡å‹åœ¨$k$ä¸ªæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚å–å…¶ç®—æ³•å¹³å‡å€¼åšä¸ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›\n  `from sklearn.model_selection import KFold`\n\n  ![](Hyperparameter-Tuning-in-Sklearn/kfold.png)\n\n- stratified k-fold cross-validation ï¼šç”¨åœ¨åˆ†ç±»ä¸ºé¢˜ä¸­ï¼Œä¸K-foldç›¸æ¯”stratified k-foldä¿è¯æ¯ä¸€ä¸ªfoldä¸­å„ç±»åˆ«çš„æ¯”ä¾‹å’Œæ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ¯”ä¾‹ç›¸åŒã€‚å®éªŒéªŒè¯è¯¥æ–¹æ³•ç›¸å¯¹äºK-foldèƒ½å¤Ÿæ›´å¥½çš„å¹³è¡¡bias and varianceã€‚\n  `from sklearn.cross_validation import StratifiedKFold`\n\n- leave-one-out(LOO) cross-validationï¼šæ˜¯K-foldçš„æç«¯å½¢å¼ï¼Œå°†K-foldä¸­çš„kç­‰äºæ ·æœ¬é‡ã€‚\n\n- **GroupKFold**ï¼š ç”¨äºæœç´¢é—®é¢˜ä¸­ï¼Œæœç´¢è®­ç»ƒæ•°æ®æ˜¯æŒ‰ç…§doc-query pairçš„å½¢å¼å­˜åœ¨çš„ï¼Œä¸€ä¸ªqueryå¯¹åº”è‹¥å¹²è¡Œï¼Œè¿™å°±æ˜¯ä¸€ä¸ªgroupã€‚åœ¨ltré—®é¢˜ä¸­æ¨¡å‹è¯„ä»·æ—¶è®¡ç®—çš„æœ€å°å•ä½å°±æ˜¯ä¸€ä¸ªqueryï¼Œå¦‚ndcgã€‚æ‰€ä»¥éœ€è¦ä¿è¯åœ¨è®­ç»ƒé›†åˆ†å‰²çš„æ—¶å€™ä¸€ä¸ªqueryå¯¹åº”çš„è¿™äº›è¡Œåˆ†å‰²åˆ°ç›¸åŒçš„foldå†…ã€‚\n  `from sklearn.model_selection import GroupKFold`\n\nsklearnä¸­æä¾›çš„æ•°æ®åˆ†å‰²æ–¹å¼å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| ç±»å                                       | æè¿°                                       |\n| ---------------------------------------- | ---------------------------------------- |\n| [`model_selection.GroupKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)([n_splits]) | K-fold iterator variant with non-overlapping groups. |\n| [`model_selection.GroupShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit)([â€¦]) | Shuffle-Group(s)-Out cross-validation iterator |\n| [`model_selection.KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)([n_splits, shuffle, â€¦]) | K-Folds cross-validator                  |\n| [`model_selection.LeaveOneGroupOut`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut)() | Leave One Group Out cross-validator      |\n| [`model_selection.LeavePGroupsOut`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut)(n_groups) | Leave P Group(s) Out cross-validator     |\n| [`model_selection.LeaveOneOut`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut)() | Leave-One-Out cross-validator            |\n| [`model_selection.LeavePOut`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut)(p) | Leave-P-Out cross-validator              |\n| [`model_selection.PredefinedSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit)(test_fold) | Predefined split cross-validator         |\n| [`model_selection.RepeatedKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold)([n_splits, â€¦]) | Repeated K-Fold cross validator.         |\n| [`model_selection.RepeatedStratifiedKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold)([â€¦]) | Repeated Stratified K-Fold cross validator. |\n| [`model_selection.ShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)([n_splits, â€¦]) | Random permutation cross-validator       |\n| [`model_selection.StratifiedKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)([n_splits, â€¦]) | Stratified K-Folds cross-validator       |\n| [`model_selection.StratifiedShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)([â€¦]) | Stratified ShuffleSplit cross-validator  |\n| [`model_selection.TimeSeriesSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit)([n_splits, â€¦]) | Time Series cross-validator              |\n\n\n\n##  æ³›åŒ–æŒ‡æ ‡è®¡ç®—\n\nä½¿ç”¨sklearnè®¡ç®—æ³›åŒ–æŒ‡æ ‡çš„æ–¹å¼\n\n1. sklearnæŒ‰ç…§ä»»åŠ¡çš„ä¸åŒåˆ†åˆ«å®šä¹‰äº†å¸¸ç”¨çš„æ¨¡å‹è¯„ä»·æŒ‡æ ‡ [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) ã€‚å¦‚**åˆ†ç±»ä»»åŠ¡**ä¸­å®šä¹‰çš„`metrics.precision_score` ã€`metrics.recall_score`ã€‚ **å›å½’ä»»åŠ¡**ä¸­å®šä¹‰çš„`metrics.mean_squared_error`å’Œ`metrics.r2_score` ã€‚èšç±»ä»»åŠ¡ä¸­å®šä¹‰çš„`metrics.mutual_info_score` ç­‰ã€‚\n   é¢„å®šä¹‰çš„metricsè¿˜æœ‰å…¶å¯¹åº”çš„å­—ç¬¦ä¸²åç§°ï¼Œå…¶å¯¹åº”å…³ç³»è§ [é¢„å®šä¹‰çš„scoringå‚æ•°](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) ï¼Œè¿™äº›å­—ç¬¦ä¸²å¸¸ç”¨è¯­ scoringå‚æ•°ã€‚\n\n2. scoreå‡½æ•°ã€‚sklearnä¸­çš„estimatoréƒ½ä¼šç”¨ä¸€ä¸ªé»˜è®¤çš„scoreå‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥ç”¨äºè®¡ç®—é¢„å®šä¹‰çš„æ³›åŒ–æŒ‡æ ‡ã€‚\n   å¦‚ä¸‹é¢çš„ä¾‹å­æ‰€ç¤ºï¼Œlræ¨¡å‹é»˜è®¤çš„scoreå‡½æ•°å°±æ˜¯`accuracy_score` ã€‚\n\n   ```python\n   import numpy as np\n   from sklearn.datasets import load_iris\n   from sklearn.model_selection import StratifiedKFold\n   from sklearn.preprocessing import StandardScaler\n   from sklearn.pipeline import Pipeline\n   from sklearn.linear_model import LogisticRegression\n   from sklearn.metrics import accuracy_score\n   from sklearn.metrics import precision_score\n\n   np.random.seed(0)\n   iris = load_iris()\n   X, y = iris.data, iris.target\n   indices = np.arange(y.shape[0])\n   np.random.shuffle(indices)\n   X, y = X[indices], y[indices]\n   y = y > 1\n   pipe_lr = Pipeline([('scl', StandardScaler()),\n                       ('clf', LogisticRegression(penalty='l2', random_state=0, C=0.7))])\n   kfold = StratifiedKFold(n_splits = 5)\n   scores = []\n   for i, (train_ind, test_ind) in enumerate(kfold.split(X=X, y=y)):\n       trainX, trainy = X[train_ind], y[train_ind]\n       testX, testy = X[test_ind], y[test_ind]\n       pipe_lr.fit(trainX, trainy)\n       print \"[CV %d] default score %f\" % (i, pipe_lr.score(testX, testy))\n       test_pred = pipe_lr.predict(testX)\n       acc_score = accuracy_score(testy, test_pred)\n       print \"[CV %d] accuracy score %f\" % (i, acc_score)\n       pre_score = precision_score(testy, test_pred)\n       print \"[CV %d] precision score %f\" % (i, pre_score)\n       scores.append(acc_score)\n   print \"mean accuracy score %f\" % np.mean(scores)\n\n   # [CV 0] default score 1.000000\n   # [CV 0] accuracy score 1.000000\n   # [CV 0] precision score 1.000000\n   # [CV 1] default score 0.833333\n   # [CV 1] accuracy score 0.833333\n   # [CV 1] precision score 0.777778\n   # [CV 2] default score 1.000000\n   # [CV 2] accuracy score 1.000000\n   # [CV 2] precision score 1.000000\n   # [CV 3] default score 1.000000\n   # [CV 3] accuracy score 1.000000\n   # [CV 3] precision score 1.000000\n   # [CV 4] default score 0.933333\n   # [CV 4] accuracy score 0.933333\n   # [CV 4] precision score 0.833333\n   # mean accuracy score 0.953333\n   ```\n\n   â€‹\n\n3. `cross_val_score` \n   ä¸Šé¢ä¸¤ç§æ–¹å¼éƒ½éœ€è¦ä½¿ç”¨å¾ªç¯æ‰èƒ½å¾—åˆ°æ¯ä¸€ç»„æµ‹è¯•é›†çš„scoreã€‚cross_val_scoreæ˜¯å¯¹ä»¥ä¸Šä¸¤ç§æ–¹å¼çš„ç®€åŒ–ï¼Œç”¨æˆ·åªéœ€è¦ä¼ å…¥cv(äº¤å‰éªŒè¯åˆ†å‰²å™¨)å’Œscoring(ä½¿ç”¨çš„æ³›åŒ–è¯¯å·®è¯„ä»·æ–¹å¼)ï¼Œç›´æ¥è¿”å›æ¯ä¸€ç»„æµ‹è¯•é›†çš„scoreã€‚\n\n   ```python\n   import numpy as np\n   from sklearn.datasets import load_iris\n   from sklearn.model_selection import StratifiedKFold\n   from sklearn.preprocessing import StandardScaler\n   from sklearn.pipeline import Pipeline\n   from sklearn.linear_model import LogisticRegression\n   from sklearn.model_selection import cross_val_score\n\n   np.random.seed(0)\n   iris = load_iris()\n   X, y = iris.data, iris.target\n   indices = np.arange(y.shape[0])\n   np.random.shuffle(indices)\n   X, y = X[indices], y[indices]\n   y = y > 1\n\n   pipe_lr = Pipeline([('scl', StandardScaler()),\n                       ('clf', LogisticRegression(penalty='l2', random_state=0, C=0.7))])\n\n   kfold = StratifiedKFold(n_splits = 5)\n\n   scores = cross_val_score(estimator=pipe_lr, \n                            X=X, \n                            y=y, \n                            #cv=5,  # å¯ä»¥ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œè¯¥ä¾‹ä¸­ä½¿ç”¨ 5-foldçš„äº¤å‰éªŒè¯\n                            cv=kfold, # ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ª splitå¯¹è±¡\n                            scoring=None, # Noneï¼šä½¿ç”¨estimatorçš„scoreå‡½æ•°ï¼›stringï¼šé¢„å®šä¹‰çš„metricsï¼›callableï¼šè‡ªå®šä¹‰metricsï¼Œç­¾åéœ€ä¸ºscorer(estimator, X, y)\n                            n_jobs=1)\n   print 'CV accuracy scores: %s' % scores\n   print \"mean accuracy score %f +/- %f\" % (np.mean(scores) ,np.std(scores))\n\n   # CV accuracy scores: [ 1.          0.83333333  1.          1.          0.93333333]\n   # mean accuracy score 0.953333 +/- 0.065320\n   ```\n\n# GridSearchCV\n\näº¤å‰éªŒè¯æ˜¯è¯„ä¼°å…·æœ‰ç›¸åŒè¶…å‚çš„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œå³ï¼Œä¸€ä¸ªäº¤å‰éªŒè¯ä¸­ä½¿ç”¨çš„æ¨¡å‹è¶…å‚æ˜¯ä¸€æ ·çš„ã€‚æ‰€ä»¥å¯¹äºåŒä¸€ä¸ªæ¨¡å‹æ¥è¯´ï¼Œäº¤å‰éªŒè¯å®é™…ä¸Šå°±æ˜¯è¯„ä»·ä¸€ç»„è¶…å‚å¥½åçš„ã€‚\n\nä¸‹é¢ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œä¸€ä¸ªæ¨¡å‹æœ‰ä¸‰ä¸ªè¶…å‚æ•°ï¼šparam1, param2, param3ã€‚ä¸ºäº†è¯´æ˜æ–¹ä¾¿ï¼Œå‡è®¾ä»–ä»¬éƒ½æ˜¯å»æ•´æ•°ã€‚\n\nè¿™ä¸‰ä¸ªè¶…å‚æ•°å¯èƒ½çš„å–å€¼èŒƒå›´æ˜¯ï¼š\n\n- param1 : [1, 3]\n- param2 : [1:2]\n- param3 : [2: 3]\n\né‚£ä¹ˆå°±æœ‰ $3 \\times 2 \\times 2 = 12$ ä¸­å¯èƒ½çš„å‚æ•°ç»„åˆã€‚æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯å¯»æ‰¾å…¶ä¸­çš„ä¸€ä¸ªç»„åˆä½¿æ¨¡å‹æ•ˆæœæœ€å¥½ã€‚å¦‚æœä½¿ç”¨`cross_val_score`ï¼Œæˆ‘ä»¬éœ€è¦äººå·¥ç©·ä¸¾æ¯ä¸€ç§ç»„åˆï¼Œè®¡ç®—æ¯ä¸€ç§å¯èƒ½çš„`mean score` ï¼Œæ‰¾å‡ºmean scoreæœ€å°çš„ä¸€ç»„ã€‚\n\nsklearnå·²ç»å¸®æˆ‘ä»¬å®ç°äº†ä»¥ä¸Šæ–¹æ³•ï¼Œå³[`model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) ã€‚\n\n> estimatorï¼šæ‰€ä½¿ç”¨çš„åˆ†ç±»å™¨ï¼Œå¦‚estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10), å¹¶ä¸”ä¼ å…¥é™¤éœ€è¦ç¡®å®šæœ€ä½³çš„å‚æ•°ä¹‹å¤–çš„å…¶ä»–å‚æ•°ã€‚æ¯ä¸€ä¸ªåˆ†ç±»å™¨éƒ½éœ€è¦ä¸€ä¸ªscoringå‚æ•°ï¼Œæˆ–è€…scoreæ–¹æ³•ã€‚\n>\n> param_gridï¼šå€¼ä¸ºå­—å…¸æˆ–è€…åˆ—è¡¨ï¼Œå³éœ€è¦æœ€ä¼˜åŒ–çš„å‚æ•°çš„å–å€¼ï¼Œparam_grid =param_test1ï¼Œparam_test1 = {'n_estimators':range(10,71,10)}ã€‚\n>\n> scoring :å‡†ç¡®åº¦è¯„ä»·æ ‡å‡†ï¼Œ\n>\n> - Noneï¼š é»˜è®¤è¿™æ—¶éœ€è¦ä½¿ç”¨estimatorçš„scoreå‡½æ•°ï¼›\n> - stringï¼šä½¿ç”¨é¢„å®šä¹‰scoreï¼Œ å‚è€ƒ [é¢„å®šä¹‰çš„scoringå‚æ•°](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)ï¼Œ å¦‚scoring='roc_auc'ï¼Œ\n> - callable: éœ€è¦å…¶å‡½æ•°ç­¾åå½¢å¦‚ï¼šscorer(estimator, X, y)ï¼›\n>\n> cv :äº¤å‰éªŒè¯å‚æ•°ï¼Œé»˜è®¤Noneï¼Œä½¿ç”¨ä¸‰æŠ˜äº¤å‰éªŒè¯ã€‚æŒ‡å®šfoldæ•°é‡ï¼Œé»˜è®¤ä¸º3ï¼Œä¹Ÿå¯ä»¥æ˜¯yieldè®­ç»ƒ/æµ‹è¯•æ•°æ®çš„ç”Ÿæˆå™¨ã€‚\n>\n> refit :é»˜è®¤ä¸ºTrue,ç¨‹åºå°†ä¼šä»¥äº¤å‰éªŒè¯è®­ç»ƒé›†å¾—åˆ°çš„æœ€ä½³å‚æ•°ï¼Œé‡æ–°å¯¹æ‰€æœ‰å¯ç”¨çš„è®­ç»ƒé›†ä¸å¼€å‘é›†è¿›è¡Œï¼Œä½œä¸ºæœ€ç»ˆç”¨äºæ€§èƒ½è¯„ä¼°çš„æœ€ä½³æ¨¡å‹å‚æ•°ã€‚å³åœ¨æœç´¢å‚æ•°ç»“æŸåï¼Œç”¨æœ€ä½³å‚æ•°ç»“æœå†æ¬¡fitä¸€éå…¨éƒ¨æ•°æ®é›†ã€‚\n>\n> iid:é»˜è®¤True,ä¸ºTrueæ—¶ï¼Œé»˜è®¤ä¸ºå„ä¸ªæ ·æœ¬foldæ¦‚ç‡åˆ†å¸ƒä¸€è‡´ï¼Œè¯¯å·®ä¼°è®¡ä¸ºæ‰€æœ‰æ ·æœ¬ä¹‹å’Œï¼Œè€Œéå„ä¸ªfoldçš„å¹³å‡ã€‚\n>\n> verboseï¼šæ—¥å¿—å†—é•¿åº¦ï¼Œintï¼šå†—é•¿åº¦ï¼Œ0ï¼šä¸è¾“å‡ºè®­ç»ƒè¿‡ç¨‹ï¼Œ1ï¼šå°‘é‡ä¿¡æ¯ï¼Œ>1ï¼šå¯¹æ¯ä¸ªå­æ¨¡å‹éƒ½è¾“å‡ºã€‚å»ºè®®ç”¨2\n>\n> n_jobs: å¹¶è¡Œæ•°ï¼Œintï¼šä¸ªæ•°,-1ï¼šè·ŸCPUæ ¸æ•°ä¸€è‡´, 1:é»˜è®¤å€¼ã€‚\n\n```python\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = svm.SVC()\ngsearch = GridSearchCV(svc, parameters, scoring='accuracy',)\ngsearch.fit(iris.data, iris.target)\nprint gsearch.best_score_\nprint gsearch.best_params_\nprint gsearch.best_estimator_\nprint gsearch.grid_scores_ # æ²¡ç»„å‚æ•°å¯¹åº”çš„score\n\n# 0.98\n# {'kernel': 'linear', 'C': 1}\n# SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n#   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n#   max_iter=-1, probability=False, random_state=None, shrinking=True,\n#   tol=0.001, verbose=False)\n# [mean: 0.98000, std: 0.01602, params: {'kernel': 'linear', 'C': 1}, mean: 0.97333, std: 0.00897, params: {'kernel': 'rbf', 'C': 1}, mean: 0.97333, std: 0.03697, params: {'kernel': 'linear', 'C': 10}, mean: 0.98000, std: 0.01601, params: {'kernel': 'rbf', 'C': 10}]\n```\n\n# nested cross-validation\n\nè¯¥æ–¹æ³•ç”¨äºæ¯”è¾ƒä¸¤ç§æ¨¡å‹æ•ˆæœã€‚\n\nå…¶æ€è·¯å¦‚ä¸‹å›¾æ‰€ç¤º\n\n![](Hyperparameter-Tuning-in-Sklearn/nested-cross-validation.png)\n\nå¤–éƒ¨å¾ªç¯æ˜¯ä¸€ä¸ª`cross_val_score` ï¼Œå†…éƒ¨å¾ªç¯æ˜¯GridSearchCVã€‚æ¯ä¸€ä¸ªå†…éƒ¨å¾ªç¯åªä½¿ç”¨å¤–éƒ¨çš„è®­ç»ƒé›†ä½œä¸ºå…¨éƒ¨æ•°æ®ã€‚\n\nå…¶å¥½å¤„å¦‚ä¸‹\n\n> in a nice study on the bias in error estimation, Varma and Simon concluded that the true error of the estimate is almost unbiased relative to the test set when nested cross-validation is used\n\nä¸¾ä¾‹å¦‚ä¸‹ï¼šæ¯”è¾ƒSVMå’ŒDecisionTreeåœ¨irisæ•°æ®é›†ä¸Šåšåˆ†ç±»ä»»åŠ¡çš„æ•ˆæœ\n\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(0)\niris = load_iris()\nX, y = iris.data, iris.target\nindices = np.arange(y.shape[0])\nnp.random.shuffle(indices)\nX, y = X[indices], y[indices]\n\npipe_svc = Pipeline([('scl', StandardScaler()),\n            ('clf', SVC(random_state=1))])\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nparam_grid = [{'clf__C': param_range, \n               'clf__kernel': ['linear']},\n                 {'clf__C': param_range, \n                  'clf__gamma': param_range, \n                  'clf__kernel': ['rbf']}]\n\ngs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid,\n                  scoring='accuracy', \n                  cv=10, \n                  n_jobs=-1)\nscores = cross_val_score(gs, X, y, scoring='accuracy', cv=5)\nprint('CV SVM accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n\n\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n                  scoring='accuracy', \n                  cv=5)\nscores = cross_val_score(gs, \n                         X, \n                         y, \n                         scoring='accuracy',\n                         cv=5)\nprint('CV DecisionTree accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n\n# CV SVM accuracy: 0.960 +/- 0.039\n# CV DecisionTree accuracy: 0.947 +/- 0.027\n```\n\nå¯ä»¥å‘ç°SVMçš„æ•ˆæœæ›´å¥½ã€‚\n\n# validation curves\n\n[`model_selection.validation_curve`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve)\n\nvalidation curveså¯ä»¥ç†è§£ä¸ºä¸€å¼ å›¾è¡¨ï¼Œå…¶çºµåæ ‡ä¸ºæ¨¡å‹performance (score) ï¼Œè¡Œåæ ‡ä¸ºæ¨¡å‹çš„**ä¸€ä¸ªå‚æ•°**ã€‚å…¶è¡¨ç¤ºçš„æ˜¯éšç€å‚æ•°çš„å˜åŒ–ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¾¾åˆ°çš„æ•ˆæœã€‚\nä½œç”¨ï¼šæ¨¡å‹è°ƒå‚(åªèƒ½é’ˆå¯¹ä¸€ä¸ªç‰¹å¾)\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import validation_curve\n\nnp.random.seed(0)\niris = load_iris()\nX, y = iris.data, iris.target\nindices = np.arange(y.shape[0])\nnp.random.shuffle(indices)\nX, y = X[indices], y[indices]\ny = y > 1\n\npipe_lr = Pipeline([('scl', StandardScaler()),\n                    ('clf', LogisticRegression(penalty='l2', random_state=0, C=0.7))])\n\nparam_range = np.logspace(-1, 6, 7)\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, \n                X=X, \n                y=y, \n                param_name='clf__C', \n                param_range=param_range,\n                cv=10)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, \n         color='blue', marker='o', \n         markersize=5, label='training accuracy')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color='blue')\n\nplt.plot(param_range, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='validation accuracy')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig('./figures/validation_curve.png', dpi=300)\nplt.show()\n```\n\n![](Hyperparameter-Tuning-in-Sklearn/validation-curves.png)\n\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\nPython Machine Learning\n\n [`sklearn.model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n\n","tags":["Sklearn"],"categories":["Machine Learning"]},{"title":"é©¬æ°è·ç¦»ä¸å…¶æ¨å¯¼","url":"%2Fblog%2FMahalanobis-distance.html","content":"\næœ¬æ–‡æ›¾å‘è¡¨åœ¨[åšå®¢å›­](https://www.cnblogs.com/Weirping/articles/6613013.html)ä¸­ï¼Œé‡æ–°ä¿®è®¢å‘è¡¨äºæ­¤ã€‚\n\né©¬æ°è·ç¦»å°±æ˜¯ç”¨äºåº¦é‡ä¸¤ä¸ªåæ ‡ç‚¹ä¹‹é—´çš„è·ç¦»å…³ç³»ï¼Œè¡¨ç¤ºæ•°æ®çš„åæ–¹å·®è·ç¦»ã€‚ä¸å°ºåº¦æ— å…³çš„(scale-invariant)ï¼Œå³ç‹¬ç«‹äºæµ‹é‡å°ºåº¦ã€‚\n\n# åŸºæœ¬æ€æƒ³ï¼ˆintuitionï¼‰\n\nå¦‚ä¸‹å›¾çš„è¿‡ç¨‹ï¼ˆä»¥ä¸¤ä¸ªç»´åº¦ä½œä¸ºä¾‹å­ï¼‰ï¼Œæ­¤ä¾‹çš„æ•°æ®é‡å¿ƒä¸ºåŸç‚¹ï¼Œ$P1$,$P2$åˆ°åŸç‚¹çš„æ¬§æ°è·ç¦»ç›¸åŒï¼Œä½†ç‚¹$P2$åœ¨$y$è½´ä¸Šç›¸å¯¹åŸç‚¹æœ‰è¾ƒå¤§çš„å˜å¼‚ï¼Œè€Œç‚¹$P1$åœ¨$x$è½´ä¸Šç›¸å¯¹åŸç‚¹æœ‰è¾ƒå°çš„å˜å¼‚ã€‚æ‰€ä»¥$P1$ç‚¹è·åŸç‚¹çš„**ç›´è§‚è·ç¦»**æ˜¯æ¯”$P2$ç‚¹çš„å°çš„ã€‚\n\n![Mahalanobis distance img 1](Mahalanobis-distance/p1.png)\n\né©¬æ°è·ç¦»å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ƒå°†ç›´è§‚è·ç¦»å’Œæ¬§å¼è·ç¦»ç»Ÿä¸€ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œ **å®ƒå…ˆå°†æ•°æ®ä¸åŒç»´åº¦ä¸Šçš„æ–¹å·®ç»Ÿä¸€ï¼ˆå³å„ç»´åº¦ä¸Šçš„æ–¹å·®ç›¸åŒï¼‰ï¼Œæ­¤æ—¶çš„æ¬§å¼è·ç¦»å°±æ˜¯ç›´è§‚è·ç¦»**ã€‚\n\n![Mahalanobis distance img 2](Mahalanobis-distance/p2.png)\n\nå¦‚å›¾ï¼šç»Ÿä¸€æ–¹å·®åçš„å›¾ï¼Œ$\\hat{P1}$åˆ°åŸç‚¹çš„è·ç¦»å°äº$\\hat{P2}$ã€‚$P1$åˆ°åŸç‚¹çš„æ¬§å¼è·ç¦»å’Œ$P2$çš„ç›¸åŒã€‚**ä»¥ä¸Šæ‰€è¯´çš„ç›´è§‚è·ç¦»å°±æ˜¯é©¬æ°è·ç¦» **ã€‚ä½†æ˜¯ï¼Œå¦‚æœä¸åŒç»´åº¦ä¹‹é—´å…·æœ‰ç›¸å…³æ€§ï¼Œåˆ™å‹ç¼©çš„æ•ˆæœå°±ä¸å¥½äº†ã€‚å¦‚ä¸‹å›¾åªåœ¨æ¨ªå‘å’Œçºµå‘ä¸Šå‹ç¼©ï¼Œåˆ™è¾¾ä¸åˆ°ä¸Šå›¾çš„å‹ç¼©æ•ˆæœã€‚\n\n![Mahalanobis distance img 3](Mahalanobis-distance/p3.png)\n\n![Mahalanobis distance img 3](Mahalanobis-distance/p4.png)\n\næ‰€ä»¥åœ¨$F1$æ–¹å‘å’Œ$F2$æ–¹å‘ä¸Šå‹ç¼©æ•°æ®æ‰èƒ½è¾¾åˆ°è¾ƒå¥½çš„æ•ˆæœã€‚æ‰€ä»¥éœ€è¦å°†åŸå§‹æ•°æ®åœ¨**XYåæ ‡ç³»**ä¸­çš„åæ ‡ è¡¨ç¤ºåœ¨**Fåæ ‡ç³»**ä¸­ã€‚ç„¶åå†åˆ†åˆ«æ²¿ç€åæ ‡è½´å‹ç¼©æ•°æ®ã€‚\n\næ‰€ä»¥ï¼Œè®¡ç®—æ ·æœ¬æ•°æ®çš„é©¬æ°è·ç¦»åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š\n1. åæ ‡æ—‹è½¬\n2. æ•°æ®å‹ç¼©\n\n**åæ ‡æ—‹è½¬çš„ç›®æ ‡**ï¼šä½¿æ—‹è½¬åçš„å„ä¸ªç»´åº¦ä¹‹é—´çº¿æ€§æ— å…³ï¼Œæ‰€ä»¥è¯¥æ—‹è½¬è¿‡ç¨‹å°±æ˜¯ä¸»æˆåˆ†åˆ†æçš„è¿‡ç¨‹ã€‚\n**æ•°æ®å‹ç¼©çš„ç›®æ ‡**ï¼šæ‰€ä»¥å°†ä¸åŒçš„ç»´åº¦ä¸Šçš„æ•°æ®å‹ç¼©æˆä¸ºæ–¹å·®éƒ½æ˜¯1çš„çš„æ•°æ®é›†ã€‚\n\n# æ¨å¯¼è¿‡ç¨‹\n\næœ‰ä¸€ä¸ªåŸå§‹çš„å¤šç»´æ ·æœ¬æ•°æ®$X_{n \\times m}$(måˆ—ï¼Œnè¡Œ):\n\n$$\n\\begin {matrix}\nx_{11} & x_{12} &  \\cdots  & x_{1m}  \\\\\nx_{21} & x_{22} &  \\cdots  & x_{2m}  \\\\\n\\vdots  &  \\vdots  &  \\ddots  &  \\vdots   \\\\\nx_{n1} & x_{n2} &  \\cdots  & x_{nm}  \\\\\n\\end {matrix}\n$$\n\nå…¶ä¸­æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼ˆå…±$n$ä¸ªï¼‰ï¼›$X_i$è¡¨ç¤ºæ ·æœ¬çš„ç¬¬$i$ä¸ªç»´åº¦ï¼ˆå…±$m$ä¸ªï¼‰ $X_i=(x_{1i}, x_{2i}, \\dots, x_{ni})^{\\mathrm{T}}$ ï¼Œä»¥ä¸Šå¤šç»´æ ·æœ¬æ•°æ®è®°ä¸º${\\bf{X}} = \\left( {X_1}, {X_2} \\cdots {X_m} \\right)$ã€‚æ ·æœ¬çš„æ€»ä½“å‡å€¼ä¸º$\\bf{\\mu }_{\\bf{X}} = \\left( \\mu _{X1}, \\mu _{X2} \\cdots \\mu _{Xm} \\right)$ã€‚å…¶åæ–¹å·®ä¸ºï¼š\n$$\n\\bf{\\Sigma }_\\bf{X} \n= E\\left\\{ (\\bf{X} - \\bf{\\mu}_{\\bf{X}})^{\\bf{T}}(\\bf{X} - \\bf{\\mu}_{\\bf{X}}) \\right\\}\n= {\\frac 1n}{(\\bf{X} - \\bf{\\mu}_{\\bf{X}})^{\\bf{T}}}(\\bf{X} - \\bf{\\mu}_{\\bf{X}})\n$$\n\n\nåæ–¹å·®çŸ©é˜µè¡¨ç¤ºæ ·æœ¬æ•°æ®å„ç»´åº¦ä¹‹é—´çš„å…³ç³»çš„ã€‚**å…¶ä¸­næ˜¯æ ·æœ¬çš„æ•°é‡**\n\nå‡è®¾å°†åŸå§‹æ•°æ®é›†$\\bf{X}$é€šè¿‡åæ ‡æ—‹è½¬çŸ©é˜µ$\\bf{U}$æ—‹è½¬åˆ°æ–°çš„åæ ‡ç³»ç»Ÿä¸­å¾—åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®é›†$\\bf{F}$ã€‚ï¼ˆå…¶å®$\\bf{X}$å’Œ$\\bf{F}$è¡¨ç¤ºçš„æ˜¯åŒä¸€ç»„æ ·æœ¬æ•°æ®é›†ï¼Œåªæ˜¯ç”±äºå…¶åæ ‡å€¼ä¸åŒï¼Œä¸ºäº†æ˜“äºåŒºåˆ†ç”¨äº†ä¸¤ä¸ªå­—æ¯è¡¨ç¤ºï¼‰\n$$\n\\bf{F}^\\bf{T} = \n(\\bf{F}_\\bf{1}\\bf{,}\\bf{F}_\\bf{2} \\cdots \\bf{F}_\\bf{m})^\\mathrm{T} = \\bf{U}\\bf{X}^\\mathrm{T}\n$$\næ–°æ•°æ®é›†$\\bf{F}$çš„å‡å€¼è®°ä¸º$\\mu _F = \\mu_{F1},\\mu_{F2} \\cdots \\mu_{Fm})$ , $\\mu _F = U\\mu_X$\n\nç”±äºå°†æ•°æ®é›†æ—‹è½¬åæ•°æ®çš„å„ç»´åº¦ä¹‹é—´æ˜¯ä¸ç›¸å…³çš„ï¼Œæ‰€ä»¥æ–°æ•°æ®é›†${\\bf{F}}$çš„åæ–¹å·®çŸ©é˜µ${\\Sigma_F}$åº”è¯¥ä¸ºå¯¹è§’é˜µã€‚\nç”±äº:\n$$\n(\\bf{F} - \\bf{\\mu}_{\\bf{F}})^{\\mathrm{T}} = \\bf{U}(\\bf{X} - \\bf{\\mu}_{\\bf{X}})^{\\mathrm{T}} \\\\\n(\\bf{F} - \\bf{\\mu}_{\\bf{F}}) = (\\bf{X} - \\bf{\\mu}_{\\bf{X}})\\bf{U}^{\\mathrm{T}}\n$$\næ‰€ä»¥ï¼š\n$$\n\\begin {aligned}\n\\bf{\\Sigma}_{\\bf{F}} &= \\bf{E}\\left\\{\\left(\\bf{F} - \\bf{\\mu}_{\\bf{F}} \\right)^{\\bf{T}}\\left(\\bf{F} - \\bf{\\mu }_{\\bf{F}} \\right) \\right\\} \\\\\n& = \\frac{\\bf{1}}{\\bf{n}}\\left(\\bf{F} - \\bf{\\mu}_{\\bf{F}} \\right)^{\\bf{T}}\\left(\\bf{F} - \\bf{\\mu}_{\\bf{F}} \\right) \\\\\n&= \\frac{\\bf{1}}{\\bf{n}}\\bf{U}\\left(\\bf{X} - \\bf{\\mu }_{\\bf{X}} \\right)^{\\bf{T}}\\left(\\bf{X} - \\bf{\\mu }_{\\bf{X}} \\right)\\bf{U}^{\\bf{T}}  \\\\\n& = \\bf{U}\\bf{\\Sigma }_X\\bf{U}^{\\bf{T}}  \\\\\n& = \\left( {\\matrix{   {\\lambda 1} & {} & {} & {}  \\cr \n{} & {\\lambda 2} & {} & {}  \\cr   \n{} & {} &  \\ddots  & {}  \\cr\n{} & {} & {} & {\\lambda m}  \\cr  } } \\right)\n\\end {aligned}\n$$\n\nå…¶ä¸­ $\\sqrt{\\lambda_i}$ å°±æ˜¯ç¬¬$i$ä¸ªç»´åº¦çš„æ–¹å·®ã€‚ \n\nç”±äº$\\bf \\Sigma_{\\bf X}$æ˜¯å®å¯¹è§’é˜µï¼Œæ‰€ä»¥$\\bf U$æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µ $\\bf U^{\\bf T} = \\bf U^{-1}$ã€‚\n\nä»¥ä¸Šæ˜¯å‡†å¤‡çŸ¥è¯†ï¼Œä¸‹é¢æ¨å¯¼ä¸€ä¸ªæ ·æœ¬ç‚¹$\\bf{x} = \\left( x_1,x_2 \\cdots x_m \\right)$åˆ°é‡å¿ƒ$\\bf{\\mu }_{\\bf X} = \\left( \\mu _{X1},\\mu _{X2} \\cdots \\mu _{Xm} \\right)$çš„é©¬æ°è·ç¦»ã€‚ç­‰ä»·äºæ±‚ç‚¹ $\\bf{f} = (f_1,f_2 \\cdots f_m)$ **å‹ç¼©åçš„åæ ‡**å€¼åˆ°æ•°æ®é‡å¿ƒå‹ç¼©åçš„åæ ‡å€¼ $\\bf{\\mu }_\\bf{F} = \\left( \\mu _{F1},\\mu _{F2} \\cdots \\mu _{Fm} \\right)$çš„æ¬§å¼è·ç¦»ã€‚\n\n$$\n\\begin {aligned}\n& d^2(\\bf{f},\\bf{\\mu }_{\\bf{F}} ) = (\\frac {f_1 - \\mu_{F1}}{\\sqrt{\\lambda_1}})^2 + (\\frac{f_2 - \\mu_{F2}}{\\sqrt{\\lambda_2}})^2 +  \\cdots  + (\\frac{f_m - \\mu_{Fm}}{\\sqrt{\\lambda_m}})^2  \\\\   \n& = ( f_1 - \\mu_{F1},{f_2} - \\mu_{F2} \\cdots f_m - \\mu_{Fm} )\n\\left (\n\t\\begin  {matrix}\n\t\t{\\frac 1{\\lambda_1}} & {} & {} & {}    \\\\      \n\t\t{} & {\\frac 1 {\\lambda _2}} & {} & {}  \\\\     \n\t\t{} & {} &  \\ddots & {}                 \\\\      \n\t\t{} & {} & {} & {\\frac 1 {\\lambda _m}}  \\\\   \n\t\\end {matrix} \n\\right) \n\\left(\n\t\\begin {matrix}\n\t\t{f_1 - \\mu_{F1}}  \\\\      \n\t\t{f_2 - \\mu_{F2}}  \\\\  \n\t\t\\vdots            \\\\  \n\t\t{f_m - \\mu_{Fm}}  \\\\  \n\t\\end {matrix} \n\\right)  \\\\   \n& = (\\bf{f} - \\bf{\\mu}_{\\bf{F}})(\\bf{U}\\bf{\\Sigma}_X \\bf{U}^{\\bf{T}})^{-1}(\\bf{f} - \\bf{\\mu}_{\\bf{F}})^T  \\\\   \n&   \\\\   \n& = (\\bf{x} - \\bf{\\mu}_X)\\bf{U}^T(\\bf{U}\\bf{\\Sigma}_X \\bf{U}^\\bf{T})^{-1}\\bf{U}(\\bf{x} - \\bf{\\mu}_X )^T  \\\\   \n&   \\\\   \n& = (\\bf{x} - \\bf{\\mu}_X )\\bf{U}^T \\bf{U}\\bf{\\Sigma}_X^{-1}\\bf{U}^{\\bf{T}}\\bf{U}(\\bf{x} - \\bf{\\mu}_X )^T  \\\\   \n&   \\\\   \n& = (\\bf{x} - \\bf{\\mu }_X )\\bf{\\Sigma}_X^{-1}(\\bf{x} - \\bf{\\mu }_X)^T \\\\\n\\end {aligned}\n$$\nè¿™å°±æ˜¯é©¬æ°è·ç¦»çš„çš„è®¡ç®—å…¬å¼äº†ã€‚\n\nå¦‚æœ$\\bf{x}$æ˜¯åˆ—å‘é‡\n$$\n{d^2} = \\left( \\bf{x} - \\bf{\\mu }_X \\right)^T\\bf{\\Sigma }_X^{-1}\\left( \\bf{x} - \\bf{\\mu }_X \\right)\n$$\n\nå¦‚æœå¹¶æŠŠä¸Šæ–‡çš„é‡å¿ƒ$\\bf{\\mu }_{\\bf{X}} = \\left( \\mu _{X1},\\mu _{X2} \\cdots \\mu_{Xm} \\right)$æ”¹ä¸ºä»»æ„ä¸€ä¸ªæ ·æœ¬ç‚¹$\\bf{y}$ï¼Œåˆ™å¯ä»¥å¾—åˆ°$\\bf{x}$å’Œ$\\bf{y}$ä¸¤ä¸ªæ ·æœ¬ç‚¹ä¹‹é—´çš„é©¬æ°è·ç¦»å…¬å¼ä¸ºï¼š\n$$\nd^2 = \\left( \\bf{x} - \\bf{y} \\right)^{\\bf{T}}\\bf{\\Sigma }_{\\bf{X}}^{-1}\\left( \\bf{x} - \\bf{y} \\right)\n$$\n\n# å‚è€ƒèµ„æ–™\n\nç»†è¯´é©¬æ°è·ç¦»\n\n\n\n\n\n","tags":["Math"],"categories":["Math"]},{"title":"Vimä¸­æ­£åˆ™è¡¨è¾¾å¼æ±‡æ€»","url":"%2Fblog%2FRegex-in-Vim.html","content":"\nåœ¨windowsç³»ç»Ÿä¸‹ç”¨notepad++ å¤„ç†æ–‡æœ¬æ—¶æœ€é•¿ç”¨çš„å°±æ˜¯æ­£åˆ™è¡¨è¾¾å¼äº†ï¼Œä½†æ˜¯åœ¨linuxçš„vimä¸­ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„æ—¶å€™å’Œnotepad++ ä¸­æ˜¯æœ‰æ‰€ä¸åŒçš„ã€‚ è¿™äº›åŒºåˆ«ä¹Ÿä¸èƒ½å…¨éƒ½è®°ä½ï¼Œæ¯æ¬¡ç”¨çš„æ—¶å€™åœ¨ä¸Šç½‘æœç´¢ä¸¥é‡å½±å“æ•ˆç‡ã€‚ æ‰€ä»¥æŠŠä¸€äº›åŸºæœ¬çš„çŸ¥è¯†ç‚¹è®°å½•ä¸‹æ¥ï¼Œé‚æˆæ­¤æ–‡ã€‚\n\n\n# æŸ¥æ‰¾æ›¿æ¢\n\nvim ä¸­æ­£åˆ™è¡¨è¾¾å¼ä¸»è¦ç”¨åœ¨ æŸ¥æ‰¾æ›¿æ¢ ä¸­ã€‚å…¶è¯­æ³•å¦‚ä¸‹\næŸ¥æ‰¾ï¼š /pattern\næ›¿æ¢ï¼š [range]s/{pattern}/{string}/[flags]\nä¾‹å­ï¼š\n\n    :1,10s/from/to/ è¡¨ç¤ºåœ¨ç¬¬1åˆ°ç¬¬10è¡Œï¼ˆåŒ…å«ç¬¬1ï¼Œç¬¬10è¡Œï¼‰ä¹‹é—´æœç´¢æ›¿æ¢\n    :10s/from/to/ è¡¨ç¤ºåªåœ¨ç¬¬10è¡Œæœç´¢æ›¿æ¢\n    :%s/from/to/ è¡¨ç¤ºåœ¨æ‰€æœ‰è¡Œä¸­æœç´¢æ›¿æ¢\n    1,$s/from/to/ åŒä¸Š\n\nflags æœ‰å¦‚ä¸‹å››ä¸ªé€‰é¡¹\nc confirmï¼Œæ¯æ¬¡æ›¿æ¢å‰è¯¢é—®ï¼›\ne errorï¼Œ ä¸æ˜¾ç¤ºé”™è¯¯ï¼›\ng globleï¼Œä¸è¯¢é—®ï¼Œæ•´è¡Œæ›¿æ¢ã€‚å¦‚æœä¸åŠ gé€‰é¡¹ï¼Œåˆ™åªæ›¿æ¢æ¯è¡Œçš„ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ï¼›\ni ignoreï¼Œå¿½ç•¥å¤§å°å†™\nè¿™äº›é€‰é¡¹å¯ä»¥åˆå¹¶ä½¿ç”¨ï¼Œå¦‚cgiè¡¨ç¤ºä¸åŒºåˆ†å¤§å°å†™ï¼Œæ•´è¡Œæ›¿æ¢ï¼Œæ›¿æ¢å‰è¯¢é—®\n\n# vimä¸­çš„æ­£åˆ™è¡¨è¾¾å¼\n\n## å…ƒå­—ç¬¦\n```\n    . åŒ¹é…ä»»æ„å­—ç¬¦\n    [abc] åŒ¹é…æ–¹æ‹¬å·ä¸­çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œå¯ç”¨-è¡¨ç¤ºå­—ç¬¦èŒƒå›´ã€‚å¦‚[a-z0-9]åŒ¹é…å°å†™å­—æ¯å’Œæ•°å­—\n    [^abc] åŒ¹é…é™¤æ–¹æ‹¬å·ä¸­å­—ç¬¦ä¹‹å¤–çš„ä»»æ„å­—ç¬¦\n    \\d åŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œç­‰åŒäº[0-9]\n    \\D åŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—ä¹‹å¤–çš„ä»»æ„å­—ç¬¦ï¼Œç­‰åŒäº[^0-9]\n    \\x åŒ¹é…åå…­è¿›åˆ¶æ•°å­—ï¼Œç­‰åŒäº[0-9A-Fa-f]\n    \\X åŒ¹é…åå…­è¿›åˆ¶æ•°å­—ä¹‹å¤–çš„ä»»æ„å­—ç¬¦ï¼Œç­‰åŒäº[^0-9A-Fa-f]\n    \\l åŒ¹é…[a-z]\n    \\L åŒ¹é…[^a-z]\n    \\u åŒ¹é…[A-Z]\n    \\U åŒ¹é…[^A-Z]\n    \\w åŒ¹é…å•è¯å­—æ¯ï¼Œç­‰åŒäº[0-9A-Za-z_]\n    \\W åŒ¹é…å•è¯å­—æ¯ä¹‹å¤–çš„ä»»æ„å­—ç¬¦ï¼Œç­‰åŒäº[^0-9A-Za-z_]\n    \\t åŒ¹é…<TAB>å­—ç¬¦\n    \\s åŒ¹é…ç©ºç™½å­—ç¬¦ï¼Œç­‰åŒäº[\\t]\n    \\S åŒ¹é…éç©ºç™½å­—ç¬¦ï¼Œç­‰åŒäº[^\\t]\n```\n## vimä¸­è¡¨ç¤ºæ•°é‡çš„å…ƒå­—ç¬¦\næ³¨æ„ï¼šä¸‹é¢é™¤äº† `*` ä»¥å¤– å…¶ä»–éƒ½éœ€è¦åœ¨å‰é¢åŠ ä¸Š `\\`\n```\n    *  *  åŒ¹é…0-ä»»æ„ä¸ª\n    \\+ åŒ¹é…1-ä»»æ„ä¸ª\n    \\? åŒ¹é…0-1ä¸ª\n    \\{n,m} åŒ¹é…n-mä¸ª\n    \\{n}   åŒ¹é…nä¸ª\n    \\{n,}  åŒ¹é…n-ä»»æ„ä¸ª\n    \\{,m}  åŒ¹é…0-mä¸ª\n```\n\n## éœ€è½¬ä¹‰çš„å­—ç¬¦\n\n```\n    \\* åŒ¹é…* å­—ç¬¦\n    .  åŒ¹é…. å­—ç¬¦\n    \\/ åŒ¹é… / å­—ç¬¦\n    \\  åŒ¹é… \\ å­—ç¬¦\n    \\[ åŒ¹é… [ å­—ç¬¦\n    \\] åŒ¹é… ] å­—ç¬¦\n```\n\n## æ›¿æ¢å˜é‡\næ³¨æ„ ï¼š vimä¸­æ›¿æ¢å˜é‡éœ€è¦åœ¨æ‹¬å·å‰é¢åŠ ä¸Š `\\`\nåœ¨æ­£åˆ™å¼ä¸­ä»¥`\\(`å’Œ `\\)`æ‹¬èµ·æ¥çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œåœ¨åé¢ä½¿ç”¨çš„æ—¶å€™å¯ä»¥ç”¨` \\1`ã€`\\2`ç­‰å˜é‡æ¥è®¿é—®`\\(`å’Œ`\\)`ä¸­çš„å†…å®¹\n\n## å¤šé€‰ä¸€åŒ¹é…\n```\n    åœ¨ä¸€ä¸ªæŸ¥æ‰¾æ¨¡å¼ä¸­ï¼Œ\"æˆ–\" è¿ç®—ç¬¦æ˜¯ \"\\|\"ã€‚ä¾‹å¦‚:\n    /foo\\|bar\n    è¿™ä¸ªå‘½ä»¤åŒ¹é…äº† \"foo\" æˆ– \"bar\"ã€‚æ›´å¤šçš„æŠ‰æ‹©å¯ä»¥è¿åœ¨åé¢:\n    /one\\|two\\|three\n    åŒ¹é… \"one\"ï¼Œ\"two\" æˆ– \"three\"ã€‚\n    å¦‚è¦åŒ¹é…å…¶å¤šæ¬¡é‡å¤ï¼Œé‚£ä¹ˆæ•´ä¸ªæŠ‰æ‹©ç»“æ„é¡»ç½®äº \"\\(\" å’Œ \"\\)\" ä¹‹é—´:\n    /\\(foo\\|bar\\)\\+\n    è¿™ä¸ªå‘½ä»¤åŒ¹é… \"foo\"ï¼Œ\"foobar\"ï¼Œ\"foofoo\"ï¼Œ\"barfoobar\"ï¼Œç­‰ç­‰ã€‚\n    å†ä¸¾ä¸ªä¾‹å­:\n    /end\\(if\\|while\\|for\\)\n    è¿™ä¸ªå‘½ä»¤åŒ¹é… \"endif\"ï¼Œ\"endwhile\" å’Œ \"endfor\"ã€‚\n```\n\n## vimä¸­çš„ç‰¹æ®Šå­—ç¬¦\n**å›è½¦** \nè¾“å…¥æ–¹å¼ï¼š `ctrl+v+enter`\nå›è½¦åœ¨vimçš„è¾“å…¥æ–¹æ³•æ˜¯\n\nâ€‹\tå…ˆæŒ‰`ctrl+v`,å¾—åˆ°`^` \n\nâ€‹\tå†æŒ‰`enter`.å¾—åˆ°^M\n\nå› æ­¤æŠŠå…¨æ–‡ä»¶æ‰€æœ‰stræ¢æˆstrå›è½¦çš„è¯­å¥æ˜¯:\n: % s/str/str^M/g\n\n\n\n# å‚è€ƒ\n\n[VimæŸ¥æ‰¾æ›¿æ¢ & æ­£åˆ™è¡¨è¾¾å¼](https://blog.csdn.net/u014015972/article/details/50688837)","tags":["Vim"],"categories":["Vim"]},{"title":"Skewness and Kurtosis","url":"%2Fblog%2FSkewness-and-Kurtosis.html","content":"\nååº¦å’Œå³°åº¦éƒ½æ˜¯ç»Ÿè®¡é‡ \n\nååº¦Skewness(ä¸‰é˜¶) ï¼šä¸‰é˜¶ä¸­å¿ƒè·é™¤ä»¥æ ‡å‡†å·®çš„ä¸‰æ¬¡æ–¹ã€‚æè¿°åˆ†å¸ƒåç¦»å¯¹ç§°æ€§ç¨‹åº¦çš„ä¸€ä¸ªç‰¹å¾æ•°ã€‚\n\nå³°åº¦Kurtosis (å››é˜¶) ï¼šå››é˜¶ä¸­å¿ƒçŸ©é™¤ä»¥æ ‡å‡†å·®çš„å¹³æ–¹ å‡å»ä¸‰ã€‚ ç”¨æ¥åæ˜ é¢‘æ•°åˆ†å¸ƒæ›²çº¿é¡¶ç«¯å°–å³­æˆ–æ‰å¹³ç¨‹åº¦çš„æŒ‡æ ‡ã€‚\n\n# skew\n\næ˜¯ç ”ç©¶æ•°æ®åˆ†å¸ƒå¯¹ç§°çš„ç»Ÿè®¡é‡ã€‚é€šè¿‡å¯¹ååº¦ç³»æ•°çš„æµ‹é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ¤å®šæ•°æ®åˆ†å¸ƒçš„ä¸å¯¹ç§°ç¨‹åº¦ä»¥åŠæ–¹å‘ã€‚\n\nå…·ä½“æ¥è¯´ï¼Œå¯¹äºéšæœºå˜é‡Xï¼Œæˆ‘ä»¬å®šä¹‰ååº¦ä¸ºå…¶çš„ä¸‰é˜¶æ ‡å‡†ä¸­å¿ƒçŸ©:\n$$\n\\mathrm{Skew}(\\mathbf{X}) = E[(\\frac{\\mathbf{X}-\\mu}{\\sigma})^3] = \\frac{E[(\\mathbf{X}-\\mu)^3]}{(E[(\\mathbf{X}-\\mu)^2])^{3/2}}=\\frac{k_3}{k_2^{3/2}}\n$$\nå…¶ä¸­ï¼š $\\mu$ä¸ºéšæœºå˜é‡å‡å€¼ï¼Œ$\\sigma$ä¸ºéšæœºå˜é‡æ ‡å‡†å·®\n\nè€Œå¯¹äºæ ·æœ¬çš„ååº¦ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç®€è®°ä¸ºSKï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºçŸ©ä¼°è®¡ï¼Œå¾—åˆ°æœ‰ï¼š\n$$\n\\mathrm{SK} =\\frac{m_3}{m_2^{3/2}} = \\frac{\\frac{1}{n}\\sum(x_i-\\bar x)^3}{[\\frac{1}{n}\\sum(x_i-\\bar x)^2]^{3/2}}\n$$\nå…¶ä¸­ï¼š $\\bar x$ä¸ºæ ·æœ¬å‡å€¼ï¼Œ$m_3$ä¸ºæ ·æœ¬ä¸‰é˜¶ä¸­å¿ƒçŸ©ï¼Œ$m_2$ä¸ºæ ·æœ¬çš„äºŒé˜¶ä¸­å¿ƒçŸ©ã€‚**æ ·æœ¬ååº¦çš„è®¡ç®—ç»“æœéƒ½å±äºæœ‰åä¼°è®¡ã€‚**\n\nååº¦çš„è¡¡é‡æ˜¯ç›¸å¯¹äºæ­£æ€åˆ†å¸ƒæ¥è¯´ï¼Œæ­£æ€åˆ†å¸ƒçš„ååº¦ä¸º0ã€‚å› æ­¤æˆ‘ä»¬è¯´ï¼Œè‹¥æ•°æ®åˆ†å¸ƒæ˜¯å¯¹ç§°çš„ï¼Œååº¦ä¸º0ã€‚\n\nè‹¥ååº¦>0ï¼Œåˆ™å¯è®¤ä¸ºé«˜å³°åœ¨å·¦ï¼Œåˆ†å¸ƒä¸ºå³åï¼Œå³åˆ†å¸ƒæœ‰ä¸€æ¡é•¿å°¾åœ¨å³ï¼›\n\nè‹¥ååº¦<0ï¼Œåˆ™å¯è®¤ä¸ºé«˜å³°åœ¨å³ï¼Œåˆ†å¸ƒä¸ºå·¦åï¼Œå³åˆ†å¸ƒæœ‰ä¸€æ¡é•¿å°¾åœ¨å·¦ã€‚\n\nè‹¥ååº¦ = 0 - mean = median, the distribution is symmetrical around the mean.\n\nåŒæ—¶ååº¦çš„ç»å¯¹å€¼è¶Šå¤§ï¼Œè¯´æ˜åˆ†å¸ƒçš„åç§»ç¨‹åº¦è¶Šä¸¥é‡ã€‚\n\n![](Skewness-and-Kurtosis/skew.png)\n\n![](Skewness-and-Kurtosis/skew2.png)\n\n\n\n# Kurtosis\n\nå³°åº¦ï¼ŒKurtosisï¼Œæ˜¯ç ”ç©¶æ•°æ®åˆ†å¸ƒé™¡å³­æˆ–å¹³æ»‘çš„ç»Ÿè®¡é‡ï¼Œé€šè¿‡å¯¹å³°åº¦ç³»æ•°çš„æµ‹é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ¤å®šæ•°æ®åˆ†å¸ƒç›¸å¯¹äºæ­£æ€åˆ†å¸ƒè€Œè¨€æ˜¯æ›´é™¡å³­(peakedness)è¿˜æ˜¯å¹³ç¼“(flattening)ã€‚\n\nå¯¹äºéšæœºå˜é‡Xï¼Œæˆ‘ä»¬å®šå³°åº¦ä¸ºå…¶çš„å››é˜¶æ ‡å‡†ä¸­å¿ƒçŸ©:\n$$\n\\mathrm{Kurtosis}(\\mathbf{X}) = E[(\\frac{\\mathbf{X}-\\mu}{\\sigma})^4] = \\frac{E[(\\mathbf{X}-\\mu)^4]}{(E[(\\mathbf{X}-\\mu)^2])^{2}}\n$$\næ­¤æ—¶æ­£å¤ªåˆ†å¸ƒçš„å³°åº¦ç³»æ•°æ˜¯3ï¼Œä½†æ˜¯ä¸ºäº†æ¯”è¾ƒèµ·æ¥æ–¹ä¾¿ï¼Œå¾ˆå¤šè½¯ä»¶ï¼ˆspssï¼Œpythonä¸­çš„pandaså·¥å…·ï¼‰å°†å³°åº¦ç³»æ•°å‡å»3ï¼Œå³ä½¿ç”¨å¦‚ä¸‹å…¬å¼è®¡ç®—ã€‚å¯¹äºæ ·æœ¬çš„å³°åº¦ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç®€è®°ä¸ºKï¼Œå¯é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—æ ·æœ¬çš„å³°åº¦ç³»æ•°ï¼š\n$$\n\\mathrm{K} =\\frac{m_4}{m_2^{2}} - 3 = \\frac{\\frac{1}{n}\\sum(x_i-\\bar x)^4}{[\\frac{1}{n}\\sum(x_i-\\bar x)^2]^{2}} -3\n$$\nä¸Šå¼çš„åˆ†å­åˆ†æ¯éƒ½ä¸æ˜¯æ— åä¼°è®¡é‡ã€‚\n\n- Kurtosis > 3: Leptokurtic distribution, sharper than a normal distribution, with values concentrated around the mean and longer tails.This means high probability for extreme values.\n- Kurtosis < 3: Platykurtic distribution, flatter than a normal distribution with a wider peak. The probability for extreme values is less than for a normal distribution, and the values are wider spread around the mean.\n- Kurtosis = 3: Mesokurtic distribution - normal distribution for example.\n\n**å³°åº¦å…¶å®æ˜¯ä¸€ä¸ªç›¸å¯¹äºæ­£æ€åˆ†å¸ƒçš„å¯¹æ¯”é‡ï¼Œæ­£æ€åˆ†å¸ƒçš„å³°åº¦ç³»æ•°ä¸º3ï¼Œä½†æ˜¯ä¸ºäº†æ¯”è¾ƒèµ·æ¥æ–¹ä¾¿ï¼Œå¾ˆå¤šè½¯ä»¶ï¼ˆspssï¼Œpythonä¸­çš„pandaså·¥å…·ï¼‰å°†å³°åº¦ç³»æ•°å‡å»3ï¼Œæ­¤æ—¶æ­£æ€åˆ†å¸ƒå³°åº¦å€¼å®šä¸º0ã€‚è€Œå‡åŒ€åˆ†å¸ƒçš„å³°åº¦ä¸º-1.2ï¼ŒæŒ‡æ•°åˆ†å¸ƒçš„å³°åº¦ä¸º6ã€‚ **\n\n![](Skewness-and-Kurtosis/kretosis.png)\n\n\n\nå¸¸è§åˆ†å¸ƒçš„ skewness å’Œ kurtosis å€¼è§ä¸‹è¡¨\n\n![](Skewness-and-Kurtosis/dist-kretosis.png)\n\nnoteï¼šæ­¤è¡¨ä¸­åˆ—å‡ºçš„ kurtosis æ˜¯æœªå‡å» 3 çš„ç‰ˆæœ¬ã€‚\n\n# å‚è€ƒèµ„æ–™\n\n### [Skewness - Wikipedia](https://en.wikipedia.org/wiki/Skewness)\n\n### [Kurtosis - Wikipedia](https://en.wikipedia.org/wiki/Kurtosis)","tags":["Math"],"categories":["Math"]},{"title":"è´å¶æ–¯çº¿æ€§å›å½’ä¸è´å¶æ–¯é€»è¾‘å›å½’","url":"%2Fblog%2FBayesian-Probabilities-in-ML.html","content":"\nåœ¨æœºå™¨å­¦ä¹ ä¸­ç»å¸¸ä¼šé‡åˆ°æ¦‚ç‡é—®é¢˜ï¼Œè€Œåœ¨æ¦‚ç‡é—®é¢˜ä¸­ç»å¸¸å‡ºç°çš„å°±æ˜¯é¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾ã€‚\n\né¢‘ç‡å­¦æ´¾ï¼šä½¿ç”¨éšæœºäº‹ä»¶çš„å‘ç”Ÿçš„é¢‘ç‡æè¿°æ¦‚ç‡çš„æ–¹æ³•ï¼Œå°±æ˜¯é€šå¸¸è¯´çš„å¤å…¸æ¦‚å‹ï¼Œæˆ–è€…ç§°ä¸ºé¢‘ç‡å­¦æ´¾ã€‚å®ƒè¯•å›¾ä»äº‹ä»¶çš„æ•´ä½“æ¥å»ºæ¨¡æ•´ä¸ªäº‹ä»¶ã€‚å¦‚æƒ³è¦è®¡ç®—æŠ›æ·ä¸€æšç¡¬å¸æ—¶æ­£é¢æœä¸Šçš„æ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦ä¸æ–­åœ°æŠ›æ·ç¡¬å¸ï¼Œå½“æŠ›æ·æ¬¡æ•°è¶‹å‘æ— ç©·æ—¶æ­£é¢æœä¸Šçš„é¢‘ç‡å³ä¸ºæ­£é¢æœä¸Šçš„æ¦‚ç‡ã€‚å…¶ä¸­æœ€é‡è¦çš„å°±æ˜¯ä¸æ–­çš„é‡å¤ã€‚\n\nè´å¶æ–¯å­¦æ´¾ï¼šè´å¶æ–¯å­¦æ´¾è®¤ä¸ºæ¦‚ç‡æ˜¯å¯¹äº‹ä»¶ä¸ç¡®å®šæ€§çš„å®šé‡æè¿°ã€‚å¦‚æˆ‘ä»¬æƒ³è·å¾—è¿™ä¸ªäº‹ä»¶çš„æ¦‚ç‡\"whether the Arctic ice cap will have disappeared by the end of the century.\" ï¼Œæˆ‘ä»¬ä¸å¯èƒ½é€šè¿‡é‡å¤è¯•éªŒæ¥è®¡ç®—ã€‚è¿™æ—¶è´å¶æ–¯è§‚ç‚¹å°±æ´¾ä¸Šç”¨åœºäº†ï¼Œé¦–å…ˆæˆ‘ä»¬ä¼šé€šè¿‡ä»¥å¾€çš„ç»éªŒå¯¹åŒ—æå†°å·çš„èåŒ–é€Ÿåº¦è¿›è¡Œä¸€ä¸ªåˆç†çš„ä¼°è®¡ï¼Œå½“æœ‰æ–°çš„è§‚æµ‹æ•°æ®å‡ºç°çš„æ—¶å€™ï¼ˆå¦‚æ°”è±¡å«æ˜Ÿçš„è§‚å¯Ÿæ•°æ®ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ–°çš„æ•°æ®å¯¹åŸæœ‰é¢„ä¼°è¿›è¡Œçº æ­£ã€‚\n\nå…³äºé¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾å¯ä»¥å‚è€ƒ[é¢‘ç‡å­¦æ´¾(Frequentists) è´å¶æ–¯å­¦æ´¾(Bayesians)](https://blog.csdn.net/u012116229/article/details/24636001) ã€‚\n\næœ¬æ–‡åŸºäºçº¿æ€§å›å½’é—®é¢˜å’Œé€»è¾‘å›å½’é—®é¢˜å™è¿°è´å¶æ–¯ç†è®ºåœ¨æœºå™¨å­¦ä¹ çš„åº”ç”¨ã€‚\n\n# å˜é‡å®šä¹‰\n\nä¸ºäº†åæ–‡è®ºè¿°æ–¹ä¾¿ï¼Œé¦–å…ˆå¯¹ä¸‹æ–‡ä¸­æåˆ°çš„çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’åšå¦‚ä¸‹è¯´æ˜ã€‚\n\nå‡è®¾è®­ç»ƒé›†æœ‰Nä¸ªæ ·æœ¬ï¼Œæ ·æœ¬é›†çš„ç‰¹å¾ç”¨$\\mathrm X$è¡¨ç¤ºï¼Œ$x_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬ã€‚æ ·æœ¬é›†çš„lableå€¼ç”¨$\\mathrm T$è¡¨ç¤ºï¼Œ$t_i$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬çš„lableå€¼ã€‚å³  $\\mathbf{X}\\equiv (x_{1} \\dots x_{N})^{\\mathrm{T}}$ï¼Œ$\\mathrm{T}=\\{t_{1} \\dots t_{N} \\}^{\\mathrm{T}}$ ï¼Œæ ·æœ¬é›†è¡¨ç¤ºä¸º$\\mathcal D = \\{\\mathrm X, \\mathrm{T} \\}$ ã€‚\n\nåŸºäºè¯¥æ•°æ®é›†è®­ç»ƒä¸€ä¸ªæ¨¡å‹$y(x;\\mathrm w)$ ï¼Œä½¿ç”¨è¯¥æ¨¡å‹æ ¹æ®æ–°æ•°æ®çš„ç‰¹å¾é¢„æµ‹å…¶lableå€¼ã€‚\n\næœ¬æ–‡åªè®ºè¿°çº¿æ€§å›å½’é—®é¢˜å’Œé€»è¾‘å›å½’é—®é¢˜ï¼Œå…¶å½¢å¼å¦‚ä¸‹ï¼š\n\nçº¿æ€§å›å½’ï¼š$y(x ,\\mathrm w) = \\mathrm w ^{\\mathrm T} x $\n\né€»è¾‘å›å½’ï¼š$y(x , \\mathrm w) = \\frac {1}{1+ e ^{ - \\mathrm w ^{\\mathrm T} x} }$\n\nåœ¨å›å½’é—®é¢˜ä¸­ï¼Œè®¤ä¸ºç›®æ ‡å€¼$t$æœä»å‡å€¼ä¸º$y(x,\\mathrm w)$ï¼Œæ–¹å·®ä¸º$\\beta^{-1}$çš„**é«˜æ–¯åˆ†å¸ƒ**ã€‚\n$$\np(t|x,\\mathrm w, \\beta)=\\mathcal N(t|y(x , \\mathrm w),\\beta^{-1}) \\tag 1\n$$\n$\\beta$ä¸ºé«˜æ–¯å™ªå£°ï¼Œååº”çš„æ˜¯æ ·æœ¬é›†çš„é‡‡æ ·è¯¯å·®å³å™ªå£°ã€‚\n\nåœ¨é€»è¾‘å›å½’é—®é¢˜ä¸­ï¼Œç›®æ ‡å€¼$t$æœä»**Bernoulli distributionï¼ˆä¼¯åŠªåŠ›åˆ†å¸ƒï¼Œ0-1åˆ†å¸ƒï¼‰**ã€‚$t$çš„å–å€¼ä¸º0æˆ–1ã€‚é€»è¾‘å›å½’ä¸­ä½¿ç”¨çš„sigmoidå‡½æ•°ï¼š\n$$\ny = \\sigma(a) \\equiv \\frac 1{1+exp(-a)}\n$$\nçš„ä¸€ä¸ªé‡è¦ç‰¹æ€§å°±æ˜¯ï¼šå…¶è¾“å‡ºå€¼è¡¨ç¤ºçš„æ˜¯æ¦‚ç‡ï¼Œå³ï¼š$y(x , \\mathrm w)$è¡¨ç¤ºå–$t=1$çš„æ¦‚ç‡ï¼Œè€Œ$(1-y(x , \\mathrm w))$è¡¨ç¤ºå–$t=0$çš„æ¦‚ç‡ã€‚æ‰€ä»¥å¯¹äºä»»ä½•ä¸€ä¸ªæ ·æœ¬$\\{ x,t \\}$ æ»¡è¶³å‚æ•°ä¸º$y(x , \\mathrm w)$çš„ä¼¯åŠªåŠ›åˆ†å¸ƒ:\n$$\np(t|x, \\mathrm w)=y(x, \\mathrm w)^t \\{1-y(x , \\mathrm w)\\}^{(1-t)} \\tag 2\n$$\n\n\n#  è´å¶æ–¯æ¦‚ç‡\n\nåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè´å¶æ–¯å­¦æ´¾è®¤ä¸ºæ¨¡å‹ä¸­çš„å‚æ•°$\\mathrm w$æ˜¯ä¸€ä¸ªä¸ç¡®å®šçš„å€¼ï¼Œä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒå¯¹å…¶è¿›è¡Œå»ºæ¨¡ã€‚\n\né¦–å…ˆæˆ‘ä»¬å¯¹$\\mathrm{w}$å¯èƒ½çš„åˆ†å¸ƒåšä¸€ä¸ªå‡è®¾ï¼Œè¿™ä¸ªå‡è®¾æ˜¯åŸºäºç»éªŒçš„ã€å’Œè§‚æµ‹æ•°æ®æ— å…³çš„ï¼Œè¿™ä¸ªåˆ†å¸ƒå³ä¸ºå…ˆéªŒåˆ†å¸ƒ$p(\\mathrm{w})$ã€‚ä¾æ®è®­ç»ƒæ•°æ®é›†çº æ­£åçš„$\\mathrm{w}$çš„æ¦‚ç‡åˆ†å¸ƒä¸ºåéªŒåˆ†å¸ƒ$p(\\mathrm{w} | D)$ã€‚æŒ‰ç…§è´å¶æ–¯å…¬å¼å¯¹åéªŒæ¦‚ç‡åˆ†è§£ï¼Œå³ï¼š\n\n$$\np(\\mathrm{w}|\\mathcal{D})=\\frac{p(\\mathcal{D}|\\mathrm{w})p(\\mathrm{w})}{p(\\mathcal{D})}\n$$\n\n$\\mathrm{w}$çš„åéªŒåˆ†å¸ƒ$p(\\mathrm{w}| D)$å¯ä»¥åˆ†è§£ä¸ºä¸‰éƒ¨åˆ†:\n1. $p(\\mathrm{w})$ ï¼šå…ˆéªŒåˆ†å¸ƒï¼ˆpriorï¼‰ï¼Œæ˜¯å…³äº$\\mathrm{w}$çš„å‡½æ•°ï¼Œä¾èµ–äºå…ˆéªŒçŸ¥è¯†ã€‚\n2. $p(\\mathcal{D}|\\mathrm{w})$ ï¼šä¼¼ç„¶å‡½æ•°ï¼ˆlikelihoodï¼‰ï¼Œæ˜¯å…³äº$\\mathrm{w}$çš„å‡½æ•°ã€‚è¡¨ç¤ºå¯¹äº$\\mathrm{w}$çš„ä¸åŒå€¼ï¼Œæ•°æ®é›†$\\mathcal D$è¢«è§‚æµ‹åˆ°çš„æ¦‚ç‡ã€‚\n     noteï¼Œä¼¼ç„¶å‡½æ•°ä¸æ˜¯å…³äº$\\mathrm{w}$çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼Œæ‰€ä»¥ä¼¼ç„¶å‡½æ•°å¯¹wç§¯åˆ†ä¸æ˜¯1ï¼Œå®é™…ä¸Šæ˜¯$\\mathcal D$çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ã€‚\n3. $p(\\mathcal{D})$ ï¼šå½’ä¸€åŒ–é¡¹ï¼Œç”¨äºä¿è¯å…¬å¼å³è¾¹å¯¹$\\mathrm{w}$ç§¯åˆ†æ˜¯1ï¼Œå³ï¼Œä¿è¯åéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚$p(\\mathcal{D})=\\int p(\\mathcal{D}|\\mathrm{w})p(\\mathrm{w})\\mathrm{dw}$ã€‚å¯¹äºå›ºå®šæ•°æ®é›†æ¥è¯´ã€‚\n\n\né€šè¿‡ä»¥ä¸Šåˆ†æå¯çŸ¥ï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w}|\\mathcal{D}) \\varpropto p(\\mathcal{D}|\\mathrm{w})p(\\mathrm{w}) \\\\\n& posterior \\varpropto likelihood \\times prior\n\\end{aligned}\n$$\n\nåŸºäºè´å¶æ–¯æ¦‚ç‡æˆ‘ä»¬å¯ä»¥åšä¸¤ä»¶äº‹ï¼š\n\n1. ç¡®å®šå‚æ•°çš„åˆ†å¸ƒ\n2. ç¡®å®šæ–°æ ·æœ¬é¢„æµ‹å€¼çš„åˆ†å¸ƒ\n\n# å‚æ•°åˆ†å¸ƒ\n\nä¸‹é¢è®ºè¿°ä½¿ç”¨è´å¶æ–¯æ–¹æ³•è®¡ç®—å‚æ•°$\\mathrm{w}$çš„åéªŒåˆ†å¸ƒçš„æ–¹æ³•ã€‚\n\n>  è‹¥å‚æ•°çš„å…ˆéªŒæ¦‚ç‡ä¸æ¨¡å‹ä¼¼ç„¶æ¦‚ç‡å…±è½­ï¼Œé‚£ä¹ˆå‚æ•°çš„åéªŒåˆ†å¸ƒæ¦‚ç‡ä¸å…ˆéªŒæ¦‚ç‡æœä»ç›¸åŒçš„åˆ†å¸ƒã€‚æ¯”å¦‚è¯´ï¼šå…ˆéªŒæ¦‚ç‡æœä»Betaåˆ†å¸ƒï¼Œä¼¼ç„¶æ¦‚ç‡æœä»äºŒé¡¹åˆ†å¸ƒï¼Œè¿™æ—¶å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒä¸ä¼¼ç„¶æ¦‚ç‡åˆ†å¸ƒå…±è½­äº†ï¼Œé‚£ä¹ˆåéªŒæ¦‚ç‡ä¹Ÿæœä»Betaåˆ†å¸ƒã€‚\n\nä¹Ÿå°±æ˜¯è¯´åœ¨ä½¿ç”¨è´å¶æ–¯å…¬å¼æ—¶ï¼Œå¦‚æœé€‰æ‹©çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒä¸ä¼¼ç„¶æ¦‚ç‡åˆ†å¸ƒå…±è½­ï¼Œé‚£åéªŒæ¦‚ç‡åˆ†å¸ƒå°±å¾ˆå®¹æ˜“è®¡ç®—å‡ºæ¥äº†ã€‚\n\nä½†ç°å®æ˜¯ï¼Œå®ƒä»¬äºŒè€…ä¹‹é—´ç»å¸¸æ˜¯ä¸å…±è½­çš„ï¼Œä»è€Œå‡ºç°äº†ä¸‰ç§å¸¸ç”¨çš„è¿‘ä¼¼è®¡ç®—æ–¹æ³•ï¼š\n\n- ç‚¹ä¼°è®¡(Point Estimate--MAPæ–¹æ³•)\n- æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼æ–¹æ³•(Laplace approximation)\n- é‡‡æ ·æ³•(Sampling--Metropolis-Hastings)\n\n\næœ¬æ–‡ä¸»è¦è®²è§£ä¸‰ç§æƒ…å†µï¼šå…ˆéªŒä¸åéªŒåŒåˆ†å¸ƒçš„æƒ…å†µã€ç‚¹ä¼°è®¡(Point Estimate--MAPæ–¹æ³•)ã€æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼æ–¹æ³•(Laplace approximation)\n\n## å…ˆéªŒä¸åéªŒåŒåˆ†å¸ƒçš„æƒ…å†µ\n\nnoteï¼Œåœ¨LDAæ¨¡å‹ä¸­ï¼Œå…ˆéªŒåˆ†å¸ƒå’Œä¼¼ç„¶åˆ†å¸ƒæ˜¯å…±è½­ï¼Œåˆ†åˆ«ä¸ºMultinomialåˆ†å¸ƒå’Œ Dirichletåˆ†å¸ƒã€‚\n\nå¦‚ä¸Šæ‰€è¿°ï¼Œå½“å…ˆéªŒåˆ†å¸ƒä¸ä¼¼ç„¶å‡½æ•°å…±è½­ï¼Œå³åéªŒåˆ†å¸ƒæ˜¯åŒåˆ†å¸ƒæ—¶ï¼Œæ˜¯æ€ä¹ˆæ±‚è§£å‚æ•°çš„åéªŒåˆ†å¸ƒå‘¢ï¼Ÿä¸‹é¢ä»¥å›å½’é—®é¢˜ä¸ºä¾‹ä»‹ç»ã€‚\n\né¦–å…ˆå‡è®¾å‚æ•°$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒéƒ½æœä»æ­£æ€åˆ†å¸ƒï¼Œå¦‚ä¸‹ï¼š\n\n$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒä¸å›å½’é—®é¢˜çš„ä¼¼ç„¶å‡½æ•°ä¸ºï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w})=\\ \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{0},\\mathbf{S}_{0}) \\\\ \n& p(\\mathcal{D}| \\mathrm{w})=\\ p(\\mathrm{T}|\\mathbf X,\\mathrm w, \\beta)=\\prod_{i=1}^N \\mathcal N(t_i|y(x_i , \\mathrm w),\\beta^{-1}) \\\\\n\\end {aligned}\n$$\nåˆ™åéªŒåˆ†å¸ƒä¸ºï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w}|\\mathcal{D})= \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{N},\\mathbf{S}_{N}) \\\\\n\\end {aligned} \\tag 3\n$$\nå…¶ä¸­ï¼š\n$$\n\\begin {aligned}\n& \\mathbf{m}_{N}= \\mathbf{S}_{N}(\\mathbf{S}_{0}^{-1}\\mathbf{m}_{0}+\\beta \\mathrm{X}^{\\mathrm{T}}\\mathrm{T}) \\\\\n& \\mathbf{S}_{N}^{-1}=\\ \\mathbf{S}_{0}^{-1}+\\beta \\mathrm{X}^{\\mathrm{T}}\\mathrm{X} \\\\\n\\end {aligned} \n$$\nå…¶æ±‚è§£æ–¹æ³•æ¥æºäºã€ŠPattern Recognition and Machine Learningã€‹2.2.3 Bayesâ€™ theorem for Gaussian variablesã€‚å…¶ç»“è®ºå¦‚ä¸‹ï¼š\n\n![](Bayesian-Probabilities-in-ML\\Bayes-theorem-for-Gaussian-variables.png)\n\n\n\n## MAP\n\n**åéªŒæ¦‚ç‡å…¬å¼è¡¨ç¤ºäº† åœ¨ç»™å®šæ•°æ®é›†$\\mathcal D$çš„å‰æä¸‹ï¼Œ$\\mathrm w$çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ŒMAPæ–¹æ³•è®¤ä¸º$\\mathrm w$æœ€å¯èƒ½çš„å–å€¼å°±æ˜¯èƒ½ä½¿åéªŒæ¦‚ç‡$p(\\mathrm w| \\mathcal D)$æœ€å¤§çš„$\\mathrm w$ ã€‚è¯¥æ–¹æ³•ç§°ä¸º maximum posterior ï¼Œå³ MAPã€‚**\n\n### çº¿æ€§å›å½’MAP\n\n**å…ˆéªŒæ¦‚ç‡**\n\nç›¸å¯¹äºä¸Šé¢$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒçš„ä¸€èˆ¬åŒ–å‡è®¾ï¼Œåœ¨MAPä¸­æˆ‘ä»¬å‡è®¾$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒæ˜¯æœä»å‡å€¼ä¸º$0$æ–¹å·®ä¸º${\\alpha }^{-1}\\mathbf{I}$çš„é«˜æ–¯åˆ†å¸ƒï¼Œ å³ä¸Šæ–‡ä¸­çš„ $\\mathbf{m}_0 = 0, \\mathbf{S}_0 = {\\alpha }^{-1}\\mathbf{I}$:\n$$\np(\\mathrm{w}|\\alpha )=\\ \\mathcal{N}(\\mathrm{w}|0,{\\alpha }^{-1}\\mathbf{I})=(\\frac{\\alpha }{2\\pi })^{(M +1)/2}\\ \\exp \\{-\\frac{\\alpha }{2}\\mathrm{w}^{\\mathrm{T}}\\mathrm{w}\\} \\tag 5\n$$\n\nå…¶ä¸­ï¼š$\\alphaâ€‹$ä¸ºç²¾åº¦ï¼Œ$M+1â€‹$ä¸ºå‘é‡$\\mathrm wâ€‹$çš„ç»´åº¦ã€‚\n\n**ä¼¼ç„¶å‡½æ•°**\n\n$$\np(\\mathrm{T}|\\mathbf X,\\mathrm w, \\beta)=\\prod_{i=1}^N \\mathcal N(t_i|y(x_i , \\mathrm w),\\beta^{-1}) \\tag 6\n$$\n**åéªŒæ¦‚ç‡**\n\n$$\n\\begin {aligned}\np(\\mathrm w| \\mathcal D) &\\varpropto  p(\\mathrm{T}|\\mathbf X,\\mathrm w, \\beta) \\times p(\\mathrm{w}|\\alpha ) \\\\\n\\end {aligned}  \\tag 8\n$$\nå¦‚æœå¥—ç”¨ä¸Šé¢çš„å…¬å¼:\n$$\n\\begin {aligned}\np(\\mathrm{w}|\\mathcal{D}) &= \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{N},\\mathbf{S}_{N}) \\\\\n\\mathrm{where}: & \\mathbf{m}_{N}\\quad =\\beta\\mathbf{S}_{N}\\mathbf{X}^{\\mathrm T}\\mathbf{T} \\\\\n& \\mathbf{S}_{N}^{-1}=\\alpha \\mathbf{I}+\\beta \\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\n\\end {aligned}\n$$\n\n\n**MAPè®¡ç®—**\n\nå°†å…¬å¼5ã€6å¸¦å…¥å…¬å¼8ï¼Œå–ä¸¤è¾¹å–è´Ÿå¯¹æ•°ï¼š \n$$\n\\begin {aligned}\n-\\ln p(\\mathrm w| \\mathcal D) &\\varpropto  -\\ln \\{p(\\mathrm{T}|\\mathbf X,\\mathrm w, \\beta) \\times p(\\mathrm{w}|\\alpha ) \\} \\\\\n& \\varpropto \\frac{\\beta }{2}\\sum _{n=1}^{N}\\{y(x_{n},\\mathrm{w})-t_{n}\\}^{2}+\\frac{\\alpha }{2}\\mathrm{w}^{\\mathrm{T }}\\mathrm{w} + \\mathrm{const}\n\\end {aligned}  \\tag 8\n$$\næœ€å¤§åŒ–åéªŒæ¦‚ç‡ ç­‰ä»·äº æœ€å°åŒ–åéªŒæ¦‚ç‡çš„è´Ÿå¯¹æ•° ç­‰ä»·äº æœ€å°åŒ–ä¸‹å¼:\n\n$$\n\\frac{\\beta }{2}\\sum _{n=1}^{N}\\{y(x_{n},\\mathrm{w})-t_{n}\\}^{2}+\\frac{\\alpha }{2}\\mathrm{w}^{\\mathrm{T }}\\mathrm{w}\n$$\nå¯ä»¥å‘ç°ï¼Œå¯¹äºå›å½’é—®é¢˜ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡(ML)æ±‚è§£æ—¶ï¼Œå…¶æŸå¤±å‡½æ•°çš„æ­£åˆ™åŒ–ç³»æ•°ä¸º$\\lambda =\\alpha / \\beta $æ—¶ï¼Œç­‰ä»·äºMAPæ±‚è§£æ–¹æ³•ã€‚\n\nnote: $\\beta$ æ˜¯æ ·æœ¬æ•°æ®çš„åæ–¹å·®çš„å€’æ•°ï¼ˆç²¾åº¦ï¼‰ï¼Œä¸æ ·æœ¬æ•°æ®æœ‰å…³çš„ï¼›$\\alpha$ æ˜¯è¶…å‚ã€‚\n\n### é€»è¾‘å›å½’MAP\n\n**å…ˆéªŒæ¦‚ç‡**\n\nç›¸å¯¹äºä¸Šé¢$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒçš„ä¸€èˆ¬åŒ–å‡è®¾ï¼Œåœ¨MAPä¸­æˆ‘ä»¬å‡è®¾$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒæ˜¯æœä»å‡å€¼ä¸º$0$æ–¹å·®ä¸º${\\alpha }^{-1}\\mathbf{I}$çš„é«˜æ–¯åˆ†å¸ƒï¼Œ å³ä¸Šæ–‡ä¸­çš„ $\\mathbf{m}_0 = 0, \\mathbf{S}_0 = {\\alpha }^{-1}\\mathbf{I}$:\n$$\np(\\mathrm{w}|\\alpha )=\\ \\mathcal{N}(\\mathrm{w}|0,{\\alpha }^{-1}\\mathbf{I})=(\\frac{\\alpha }{2\\pi })^{(M +1)/2}\\ \\exp \\{-\\frac{\\alpha }{2}\\mathrm{w}^{\\mathrm{T}}\\mathrm{w}\\} \\tag 5\n$$\nå…¶ä¸­ï¼š$\\alpha$ä¸ºç²¾åº¦ï¼Œ$M+1$ä¸ºå‘é‡$\\mathrm w$çš„ç»´åº¦ã€‚\n\n**ä¼¼ç„¶å‡½æ•°**\n$$\np(\\mathrm{T}|\\mathbf X, \\mathrm w)=\\prod_{i=1}^N y(x_i, \\mathrm w)^{t_i} \\{1-y(x_i , \\mathrm w)\\}^{(1-t_i)} \\tag 7\n$$\n**åéªŒæ¦‚ç‡**\n$$\n\\begin {aligned}\np(\\mathrm w| \\mathcal D) &\\varpropto  p(\\mathrm{T}|\\mathbf X, \\mathrm w) \\times p(\\mathrm{w}|\\alpha )\\\\\n\\end {aligned} \\tag 9\n$$\n**MAPè®¡ç®—**\n\nå°†å…¬å¼5ã€7å¸¦å…¥å…¬å¼9ï¼Œå–ä¸¤è¾¹å–è´Ÿå¯¹æ•°ï¼š\n$$\n\\begin {aligned}\n-\\ln p(\\mathrm w| \\mathcal D) &\\varpropto  -\\ln \\{p(\\mathrm{T}|\\mathbf X, \\mathrm w) \\times p(\\mathrm{w}|\\alpha ) \\} \\\\\n\\end {aligned} \\tag 9\n$$\næœ€å¤§åŒ–åéªŒæ¦‚ç‡ ç­‰ä»·äº æœ€å°åŒ–åéªŒæ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼Œèƒ½å¤Ÿä½¿åéªŒæ¦‚ç‡çš„è´Ÿå¯¹æ•°å–æœ€å°å€¼çš„$\\mathrm w$å³ä¸º$\\mathrm w$çš„æœ€ä¼˜è§£ã€‚\n\né€»è¾‘å›å½’çš„ä¼¼ç„¶å‡½æ•°æ²¡æœ‰æ­£æ€åˆ†å¸ƒçš„ç‰¹ç‚¹ï¼Œä¸èƒ½åˆ©ç”¨ã€ŠPattern Recognition and Machine Learningã€‹2.2.3 Bayesâ€™ theorem for Gaussian variablesçš„ç»“è®ºï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨ç‰›é¡¿æ³•æ±‚è§£ã€‚\n\n\n## Laplace approximation\n\nå¾ˆå¤šæ—¶å€™ï¼Œåœ¨ä½¿ç”¨è´å¶æ–¯æ–¹æ³•æ—¶ï¼Œä¼¼ç„¶å‡½æ•°å’Œå…ˆéªŒåˆ†å¸ƒä¸æ˜¯å…±è½­äº†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å¯ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼æ¥æ±‚è§£å‚æ•°çš„åéªŒåˆ†å¸ƒã€‚\n\næœ¬èŠ‚ä»¥é€»è¾‘å›å½’é—®é¢˜ä¸ºä¾‹æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼åœ¨è¯¥é—®é¢˜ä¸­çš„åº”ç”¨ã€‚\n\næ‹‰æ™®æ‹‰æ–¯å°±æ˜¯ä½¿ç”¨ä¸€ä¸ªæ­£æ€åˆ†å¸ƒæ¥è¿‘ä¼¼ä¸€ä¸ªæœªçŸ¥åˆ†å¸ƒï¼Œåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ˜¯ç”¨ä¸€ä¸ªæ­£æ€åˆ†å¸ƒæ¥è¿‘ä¼¼å‚æ•°$\\mathbf{w}$çš„åéªŒåˆ†å¸ƒçš„ã€‚\n\nå‡è®¾å‚æ•°$\\mathrm{w}$çš„å…ˆéªŒåˆ†å¸ƒä¸ºæ­£æ€åˆ†å¸ƒï¼Œå‚æ•°ä¸º$\\mathbf{m}_{0}$ å’Œ $\\mathbf{S}_{0}$ï¼Œåˆ™å…¶å…ˆéªŒåˆ†å¸ƒå’Œä¼¼ç„¶å‡½æ•°å¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w})=\\ \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{0},\\mathbf{S}_{0}) \\\\ \n& p(\\mathcal{D}| \\mathrm{w})=p(\\mathrm{T}|\\mathbf X, \\mathrm w)=\\prod_{i=1}^N y(x_i, \\mathrm w)^{t_i} \\{1-y(x_i , \\mathrm w)\\}^{(1-t_i)} \\\\\n\\end {aligned}\n$$\nå°†ä¸Šå¼å¸¦å…¥åéªŒåˆ†å¸ƒå…¬å¼$p(\\mathrm{w}|\\mathcal{D}) \\varpropto f(\\mathbf {w}) = p(\\mathcal{D}|\\mathrm{w})p(\\mathrm{w}) $å·¦è¾¹å¹¶å¯¹ä¸¤è¾¹å–å¯¹æ•°ä¸ºï¼š\n$$\n\\begin {aligned}\n\\ln p(\\mathbf w|\\mathcal{D}) &\\varpropto \\ln f(\\mathbf{w})= -\\frac{1}{2}(\\mathbf w - \\mathbf m_0)^{\\mathrm T} \\mathbf{S}_0^{-1}(\\mathbf w - \\mathbf m_0) \\\\\n&+\\sum_{n=1}^{N}\\{t_n \\ln y_n+(1-t_{n})\\ln (1-y_{n})\\}+\\mathrm{const}\n\\end {aligned}\n$$\nå…¶ä¸­:$y_n = y(x_n, \\mathbf w)$ã€‚ä¸ºäº†è¯´æ˜æ–¹ä¾¿ï¼Œå¢åŠ $f(\\mathbf {w}) = p(\\mathcal{D}|\\mathrm{w})p(\\mathrm{w})$ã€‚ \n\næˆ‘ä»¬æ˜¯ç”¨ä¸€ä¸ªæ­£æ€åˆ†å¸ƒæ¥è¿‘ä¼¼å‚æ•°$\\mathbf{w}$çš„åéªŒåˆ†å¸ƒçš„ï¼Œé‚£ä¹ˆåéªŒåˆ†å¸ƒçš„å‡å€¼åº”è¯¥æ˜¯èƒ½å¤Ÿä½¿åéªŒåˆ†å¸ƒæœ€å¤§çš„$\\mathbf{w}$çš„å€¼ï¼Œå³ä¸º$\\mathbf{w}_{MAP}$ã€‚åéªŒåˆ†å¸ƒçš„æ–¹å·®å³ä¸º$\\ln f(\\mathbf w)$åœ¨$\\mathbf{w}_{MAP}$å¤„äºŒé˜¶å¯¼æ•°çš„å€’æ•°ï¼Œå³ï¼š\n$$\n\\mathbf{S}_{N}=-\\nabla \\nabla \\ln f(\\mathbf{w})=\\ \\mathbf{S}_{0}^{-1}+\\sum _{n=1}^{N}y_{n}(1-y_{n})x_nx_n^{\\mathrm T}\n$$\n$\\mathbf{w}$çš„åéªŒåˆ†å¸ƒå³ä¸ºï¼š\n$$\np(\\mathrm{w}|\\mathcal D)=\\ \\mathcal{N}(\\mathbf{w}|\\mathbf{m}_{MAP},\\mathbf{S}_{N}) \\\n$$\n\n# é¢„æµ‹åˆ†å¸ƒ\n\nå®é™…åº”ç”¨ä¸­æˆ‘ä»¬æ›´å…³æ³¨çš„æ˜¯å¯¹æ–°æ•°æ®çš„é¢„æµ‹å€¼ï¼Œå³ï¼Œå¯¹äºæ–°æ•°æ®é¢„æµ‹å€¼çš„åéªŒåˆ†å¸ƒ$p(t|x,\\mathcal{D}, \\mathbf{m}_0, \\mathbf{S}_0, \\beta)$ï¼Œè€Œä¸å¤ªå…³æ³¨æ¨¡å‹å‚æ•°çš„å€¼åˆ°åº•æ˜¯å¤šå°‘ã€‚\n\nå¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œçº¿æ€§å›å½’é—®é¢˜ä¸­æˆ‘ä»¬å‡è®¾é¢„æµ‹å€¼æœä»æ­£æ€åˆ†å¸ƒå³:\n$$\np(t|x,\\mathrm w, \\beta)=\\mathcal N(t|y(x , \\mathrm w),\\beta^{-1})\\tag 1\n$$\nå¯¹äºå‚æ•°$ \\mathrm w$çš„**å…ˆéªŒåˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒéƒ½ä¸ºæ­£æ€åˆ†å¸ƒ**çš„æƒ…å†µï¼Œ$ \\mathrm w$çš„åéªŒåˆ†å¸ƒä¸ºï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w}|\\mathcal{D})= \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{N},\\mathbf{S}_{N}) \\\\\n\\end {aligned} \\tag 3\n$$\nå…¶ä¸­ï¼š\n$$\n\\begin {aligned}\n& \\mathbf{m}_{N}= \\mathbf{S}_{N}(\\mathbf{S}_{0}^{-1}\\mathbf{m}_{0}+\\beta \\mathrm{X}^{\\mathrm{T}}\\mathrm{T}) \\\\\n& \\mathbf{S}_{N}^{-1}=\\ \\mathbf{S}_{0}^{-1}+\\beta \\mathrm{X}^{\\mathrm{T}}\\mathrm{X} \\\\\n\\end {aligned}\n$$\næ³¨æ„ï¼šä¸Šé¢çš„åéªŒåˆ†å¸ƒåªæ˜¯æ ‡è®°çš„ç®€åŒ–ï¼Œå¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š\n$$\n\\begin {aligned}\n& p(\\mathrm{w}|\\mathcal{D},  \\mathbf{m}_0, \\mathbf{S}_0, \\beta) =  p(\\mathrm{w}|\\mathcal{D})= \\mathcal{N}(\\mathrm{w}|\\mathbf{m}_{N},\\mathbf{S}_{N}) \\\\\n\\end {aligned}\n$$\nå¯¹äºæ–°æ•°æ®çš„è§‚æµ‹å€¼$x$ï¼Œå…¶é¢„æµ‹å€¼çš„(åéªŒ)åˆ†å¸ƒä¸ºï¼šä¸‹é¢çš„åˆ†è§£éœ€è¦å‚è€ƒè´å¶æ–¯ç½‘çš„ç†è®º(æ¦‚ç‡å›¾æ¨¡å‹)åˆ†è§£ã€‚\n$$\np(t|x,\\mathcal{D}, \\mathbf{m}_0, \\mathbf{S}_0, \\beta )=\\int p(t|x, \\mathrm{w},\\beta )p(\\mathrm{w}|\\mathcal{D},  \\mathbf{m}_0, \\mathbf{S}_0, \\beta )\\mathrm{dw}\n$$\nç›´æ¥å°†å¼1ã€3å¸¦å…¥ä¸Šå¼å¯è§£ã€‚\n\nåŒæ—¶è¯¥åéªŒåˆ†å¸ƒä¹Ÿå¯ä»¥é€šè¿‡ã€ŠPattern Recognition and Machine Learningã€‹2.2.3 Bayesâ€™ theorem for Gaussian variables ä¸­ç»“è®º 2.115å¼ç›´æ¥å¾—åˆ°ã€‚\n\nè´å¶æ–¯æ–¹æ³•çš„é€»è¾‘å›å½’çš„é¢„æµ‹åˆ†å¸ƒè¿˜æ²¡çœ‹æ‡‚ï¼Œçœ‹æ‡‚äº†å†è¡¥å……ã€‚\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning\n\n[é¢‘ç‡å­¦æ´¾(Frequentists) è´å¶æ–¯å­¦æ´¾(Bayesians)](https://blog.csdn.net/u012116229/article/details/24636001)\n\n[è´å¶æ–¯æ¨æ–­ä¹‹æœ€å¤§åéªŒæ¦‚ç‡(MAP)](https://www.cnblogs.com/hapjin/p/8834794.html)\n","tags":["Math"],"categories":["Math"]},{"title":"ä¿¡æ¯æ£€ç´¢è¯„ä»·æŒ‡æ ‡(nDCG,MRR,MAP)","url":"%2Fblog%2FMetrics-in-IR.html","content":"\n# nDCG(Normalized Discounted Cumulative Gain)\n\næºè‡ªä¸€ç¯‡å‚è€ƒç»´åŸºç™¾ç§‘çš„æ–‡ç« \n\nNormalized Discounted Cumulative Gainï¼šä¸€ç§å¯¹æœç´¢å¼•æ“æˆ–ç›¸å…³ç¨‹åºæœ‰æ•ˆæ€§çš„åº¦é‡ã€‚\n\n2ä¸ªå‡è®¾ï¼š\n\nâ€‹    1.å¼ºç›¸å…³çš„æ–‡æ¡£åº”è¯¥å‡ºç°åœ¨ç»“æœåˆ—è¡¨å‰é¢ï¼Œä¸”è¶Šé å‰ï¼ˆrankè¶Šé«˜ï¼‰è¶Šæœ‰ç”¨ã€‚\n\nâ€‹    2.å¼ºç›¸å…³æ–‡æ¡£æ¯”å¼±ç›¸å…³æ–‡æ¡£æœ‰ç”¨ï¼Œæ¯”ä¸ç›¸å…³æ–‡æ¡£æœ‰ç”¨ã€‚\n\n$DCG$æ¥æºäºä¸€ä¸ªæ›´æ—©çš„ã€æ›´åŸºç¡€çš„æ–¹æ³•---$CG$ã€‚\n\n## CG\n\n$CG$ä¸è€ƒè™‘ç»“æœé›†ä¸­çš„åºä¿¡æ¯ï¼Œå•çº¯æŠŠåˆ†çº§ç›¸å…³åº¦ç›¸åŠ ã€‚å‰ä¸ª$P$ä¸ªä½ç½®çš„$CG$å€¼æ˜¯ï¼š\n\n$$\nCG_p = \\sum\\limits_{i=1}^{p}{{rel_i}}\n$$\n\n$rel_i$æ˜¯æœç´¢ç»“æœåˆ—è¡¨çš„ä½ç½®$i$å¤„ç»“æœçš„åˆ†çº§ç›¸å…³åº¦ã€‚\n\n## DCG\n\n$DCG$å–ä»£$CG$ä½œä¸ºä¸€ä¸ªæ›´å‡†ç¡®çš„æµ‹é‡æ–¹æ³•ã€‚\n\n å¦‚æœä¸€ä¸ªå¼ºç›¸å…³çš„æ–‡æ¡£æ’åé ååˆ™åº”è¯¥å—åˆ°æƒ©ç½šï¼Œå‰ä¸ª$P$ä¸ªä½ç½®çš„$DCG$å€¼æ˜¯ï¼š\n\n$$\nDCG_p = rel_1 + \\sum_{i=2}^{p}\\frac {rel_i}{log_2 i}\n$$\n\nå¦ä¸€ä¸ª$DCG$è®¡ç®—å…¬å¼æ›´åŠ å¼ºè°ƒç›¸å…³æ€§\n\n$$\nDCG_p = \\sum_{i=1}^{p} \\frac{2^{rel_i}-1}{log_2(1+i)}\n$$\n\nè‹¥åˆ†çº§ç›¸å…³åº¦åªåœ¨0å’Œ1å–äºŒå€¼çš„è¯ï¼ŒäºŒå…¬å¼æ•ˆæœç›¸åŒ\n\n## nDCG\n\næ ¹æ®Queryçš„ä¸åŒï¼Œç»“æœåˆ—è¡¨çš„é•¿åº¦ä¹Ÿä¸åŒï¼Œæ‰€ä»¥è¿™ä¸€åº¦é‡è€ƒè™‘äº†æ­£è§„åŒ–é—®é¢˜\n\n$$\nnDCG_p =\\frac{DCG_p}{IDCG_p}\n$$\n\n$IDCGp$ï¼ˆIdeal DCGï¼‰æ˜¯åœ¨ä¸€ä¸ªå®Œç¾çš„æ’åºä¸‹ï¼Œ$p$æ‰€å…·æœ‰çš„æœ€å¤§$DCG$å€¼\n\nè¿™æ ·ä¸€æ¥æ— è®ºQueryæ˜¯ä»€ä¹ˆï¼Œ$nDCG$éƒ½å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¹³å‡å€¼ï¼Œå› æ­¤ä¸åŒçš„Queryä¹‹é—´çš„æ•ˆèƒ½å°±å¯ä»¥åšæ¯”è¾ƒäº†ã€‚\n\nå®Œç¾çš„æ’åºç®—æ³•ä¼šä½¿$DCG_p$å’Œ$IDCG_p$ç›¸åŒï¼Œä»è€Œä½¿$nDCG_p$ä¸º1ï¼Œ$nDCG$çš„å–å€¼åœ¨0åˆ°1ä¹‹é—´\n\nä¾‹ï¼šæŸ¥è¯¢ç»“æœåˆ—è¡¨ä¸­çš„6ç¯‡æ–‡æ¡£$D1,D2,D3,D4,D5,D6$ï¼Œåˆ¤å®šäº†ä»–ä»¬çš„ç›¸å…³åº¦æ˜¯ $3,2,3,0,1,2$ ï¼Œåˆ™ï¼š\n$$\nCG_6=\\sum_{i=1}^6 rel_i = 3+2+3+0+1+2=11\n$$\n\n| $i$  | $rel_i$ | $\\log _{2}(i+1)$ | $\\frac {rel_{i}}{\\log _{2}(i+1)}$ |\n| ---- | ------- | ---------------- | --------------------------------- |\n| 1    | 3       | 1                | 3                                 |\n| 2    | 2       | 1.585            | 1.262                             |\n| 3    | 3       | 2                | 1.5                               |\n| 4    | 0       | 2.322            | 0                                 |\n| 5    | 1       | 2.585            | 0.387                             |\n| 6    | 2       | 2.807            | 0.712                             |\n\næ‰€ä»¥ $DCG_6$ çš„å€¼ä¸ºï¼š\n$$\n{DCG_{6}} =\\sum _{i=1}^{6}{\\frac {rel_{i}}{\\log _{2}(i+1)}}=3+1.262+1.5+0+0.387+0.712=6.861\n$$\n\n\nä¸€ä¸ªç†æƒ³çš„æ’åºåº”è¯¥æ˜¯ï¼š$3ï¼Œ3ï¼Œ2ï¼Œ2ï¼Œ1ï¼Œ0$ ï¼Œæ‰€ä»¥\n$$\n\\begin {aligned}\n&IDCG_{6} =8.740 \\\\\n\\\\\n& nDCG_{6} ={\\frac {DCG_{6}}{IDCG_{6}}}={\\frac {6.861}{8.740}}=0.785 \\\\\n \\end {aligned}\n$$\n\n$nDCG$çš„ç¼ºç‚¹æ˜¯ï¼šå½“æ’åºçš„æ•°å¾ˆå°‘ï¼ˆæ¯”å¦‚ï¼šåªæœ‰1-3ä¸ªï¼‰ï¼Œé‚£ä¹ˆä»»ä½•æ’åºçš„$nDCGâ€‹$å€¼éƒ½æ¯”è¾ƒæ¥è¿‘ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘ä½¿ç”¨AUCï¼ˆarea under the ROC curveï¼‰ã€‚\n\nAUCå­¦ä¹ å‚è€ƒæ–‡ç« ï¼šhttp://blog.csdn.net/chjjunking/article/details/5933105 \n\n\n\nè‡ªå·±ç”¨pythonå†™çš„ä¸€ä¸ªè®¡ç®—ndcgå‡½æ•°ã€‚\n\n```python\ndef calc_dcg(sorted_vec, at):\n    '''\n    sorted_vec: list[tuple], type of element is tuple,\n    \ttuple(v0, v1), v0: predict score; v1: label score\n    at: int, calculate dcg@at\n    '''\n    import math\n    ranking = [t[1] for t in sorted_vec[0: at]]\n    dcg_ = sum([(2**r - 1) / math.log(i + 2, 2) for i, r in enumerate(ranking)])\n    return dcg_\n\n\ndef calc_ndcg(vec, at):\n    '''\n    vec: list[tuple], type of element is tuple,\n    \ttuple(v0, v1), v0: predict score; v1: label score\n    at: int, calculate ndcg@at\n    '''\n    sorted_vec = sorted(vec, key=lambda t: t[1], reverse=True)\n    ideal_dcg = calc_dcg(sorted_vec, at)\n    sorted_vec = sorted(vec, key=lambda t: t[0], reverse=True)\n    cur_dcg = calc_dcg(sorted_vec, at)\n    if ideal_dcg == 0:\n        return 1\n    else:\n        return cur_dcg / ideal_dcg\n```\n\n\n\n# MRR(Mean Reciprocal Rank)\n\nè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨æ¥è¡¡é‡æœç´¢ç®—æ³•æ•ˆæœçš„æŒ‡æ ‡ï¼Œç›®å‰è¢«å¹¿æ³›ç”¨åœ¨å…è®¸è¿”å›å¤šä¸ªç»“æœçš„é—®é¢˜ï¼Œæˆ–è€…ç›®å‰è¿˜æ¯”è¾ƒéš¾ä»¥è§£å†³çš„é—®é¢˜ä¸­ï¼ˆç”±äºå¦‚æœåªè¿”å›top 1çš„ç»“æœï¼Œå‡†ç¡®ç‡æˆ–å¬å›ç‡ä¼šå¾ˆå·®ï¼Œæ‰€ä»¥åœ¨æŠ€æœ¯ä¸æˆç†Ÿçš„æƒ…å†µä¸‹ï¼Œå…ˆè¿”å›å¤šä¸ªç»“æœï¼‰ã€‚åœ¨è¿™ç±»é—®é¢˜ä¸­ï¼Œç³»ç»Ÿä¼šå¯¹æ¯ä¸€ä¸ªè¿”å›çš„ç»“æœç»™ä¸€ä¸ªç½®ä¿¡åº¦ï¼ˆæ‰“åˆ†ï¼‰ï¼Œç„¶åæ ¹æ®ç½®ä¿¡åº¦æ’åºï¼Œå°†å¾—åˆ†é«˜çš„ç»“æœæ’åœ¨å‰é¢è¿”å›ã€‚\n\nè€ŒMRRå¾—æ ¸å¿ƒæ€æƒ³å¾ˆç®€å•ï¼šè¿”å›çš„ç»“æœé›†çš„ä¼˜åŠ£ï¼Œè·Ÿç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä½ç½®æœ‰å…³ï¼Œç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆè¶Šé å‰ï¼Œç»“æœè¶Šå¥½ã€‚\n\nå…·ä½“æ¥è¯´ï¼š**å¯¹äºä¸€ä¸ªqueryï¼Œè‹¥ç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆæ’åœ¨ç¬¬nä½ï¼Œåˆ™MRRå¾—åˆ†å°±æ˜¯$\\frac1n$ã€‚**ï¼ˆå¦‚æœæ²¡æœ‰æ­£ç¡®ç­”æ¡ˆï¼Œåˆ™å¾—åˆ†ä¸º0ï¼‰\n\nå…¬å¼ä¸ºï¼š \n\n$$\nMRR = \\frac1 {\\left| Q \\right|}\\sum\\limits_{i = 1}^{\\left| Q \\right|} \\frac 1  {rank_i}\n$$\n\nå…¶ä¸­ï¼Œ$Q$ä¸ºæ ·æœ¬queryé›†åˆï¼Œ${\\left| Q \\right|}$è¡¨ç¤º$Q$ä¸­queryä¸ªæ•°ï¼Œ$rank_i$ è¡¨ç¤ºåœ¨ç¬¬$i$ä¸ªqueryä¸­ï¼Œç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„æ’å\n\n## ä¾‹å­\n\næ¯”å¦‚ï¼Œè®¾æµ‹è¯•é›†æœ‰4ä¸ªqueryï¼Œä»–ä»¬çš„ç»“æœä¸­ï¼Œå‰ä¸‰ä¸ªqueryçš„ç¬¬ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆåˆ†åˆ«è¢«æ’åœ¨ç¬¬3ï¼Œ1ï¼Œ5ä½ï¼Œè€Œç¬¬å››ä¸ªqueryæ²¡æœ‰æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆã€‚åˆ™è¯¥ç³»ç»Ÿçš„MRRå¾—åˆ†å°±æ˜¯ï¼š \n\n$$\nMRR=\\left(\\frac13+\\frac11+\\frac15+0 \\right)Ã·4=0.383\n$$\n\n\n\n# MAP(mean average precision)\n\n## Precision\n\nPrecision is the fraction of the documents retrieved that are relevant  to the user's information need.\n\n$$\nprecision = \\frac {\\left| {\\{ relevant\\;documents\\}  \\cap \\{ retrieved\\;documents\\} } \\right|}  {\\left| {\\left\\{ {retieved\\;documents} \\right\\}} \\right|}\n$$\n\nPrecision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called *precision at n* or *P@n*.\n\n## Average precision\nPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision $p(r)$ as a function of recall $r$. Average precision computes the average value of $p(r)$ over the interval from $r=0$ to $r=1$\n\n$$\nAveP=\\int_0^1p\\left(r\\right)dr \n$$\n\nè¿™ä¸ªå€¼å°±æ˜¯AUC(rea under the precision-recall curve)\n\nThat is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\n\n$$\nAveP = \\sum _{k=1}^n{P\\left(k\\right)}\\Delta{r\\left(k\\right)}\n$$\n\n\nwhere $ k$ is the order in the sequence of retrieved documents, $n$ is the number of retrieved documents, $P(k)$ is the precision at cut-off $k$ in the list, and $\\Delta r(k) $is the change in recall from items $k-1$ to $k$.\n\nThis finite sum is equivalent to:\n\n$$\n{AveP} = \\frac{\\sum_{k=1}^n (P(k) \\times {rel}(k))}{number\\; of\\; relevant \\;documents}\n$$\n\nwhere ${rel} (k)$  is an indicator function equaling 1 if the item at k is a relevant document, zero otherwise. Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\n\n## Mean average precision\n\nMean average precision for a set of queries is the mean of the average precision scores for each query.\n\n$$\nMAP=\\frac{\\sum_{q=1}^Q{AveP(q)}}{Q}\n$$\n\nwhere *Q* is the number of queries.\n\n\n\n# å‚è€ƒ\n\nhttp://en.wikipedia.org/wiki/Discounted_cumulative_gain\n\n\n\n\n\n","tags":["IR"],"categories":["IR"]},{"title":"TF-IDF","url":"%2Fblog%2FTF-IDF.html","content":"\nåœ¨ä¿¡æ¯æ£€ç´¢ä¸­æ¯ä¸ªterméƒ½ä¼šèµ‹äºˆä¸€å®šçš„æƒå€¼ï¼ŒTF-IDFæ˜¯å…¶æœ€å¸¸è§çš„æƒé‡ï¼Œæœ¬èŠ‚å™è¿°termçš„TF-IDFçš„è®¡ç®—ã€‚å¦‚å…¶åç§°ï¼ŒTF-IDFåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šè¯é¢‘TFå’Œé€†æ–‡æ¡£é¢‘ç‡IDFã€‚\n\n# è¯é¢‘TFæƒé‡\n\nè¯é¢‘ï¼ˆTFï¼‰ï¼Œå³ä¸€ä¸ªtermåœ¨**ä¸€ä¸ªæ–‡æ¡£**ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œåœ¨æŸä¸ªæ–‡æ¡£ä¸­åå¤å‡ºç°çš„termï¼Œå¾€å¾€èƒ½å¤Ÿè¡¨å¾æ–‡æ¡£çš„ä¸»é¢˜ä¿¡æ¯ï¼Œå³TFå€¼è¶Šå¤§ï¼Œè¶Šèƒ½ä»£è¡¨æ–‡æ¡£æ‰€åæ˜ çš„å†…å®¹ï¼Œé‚£ä¹ˆåº”è¯¥ç»™äºˆè¿™ä¸ªtermæ›´å¤§çš„æƒå€¼ã€‚è¿™æ˜¯ä¸ºä½•å¼•å…¥è¯é¢‘ä½œä¸ºè®¡ç®—æƒå€¼çš„é‡è¦å› å­çš„åŸå› ã€‚\n\nè®¡ç®—è¯é¢‘æƒé‡ä¸»è¦ç›®çš„æ˜¯é€šè¿‡ä¸€ä¸ªtermåœ¨**ä¸€ä¸ªæ–‡æ¡£**ä¸­å‡ºç°çš„æ¬¡æ•°(TF)æ¥è¡¡é‡è¿™ä¸ªtermåœ¨è¯¥æ–‡æ¡£ä¸­çš„é‡è¦æ€§($W_{TF}$)ã€‚\n\nå…·ä½“è®¡ç®—è¯é¢‘å› å­çš„æ—¶å€™ï¼ŒåŸºäºä¸åŒçš„å‡ºå‘ç‚¹ï¼Œå¯ä»¥é‡‡çº³ä¸åŒçš„è®¡ç®—å…¬å¼ã€‚\n\n## ç›´æ¥ç»Ÿè®¡æ³•\n\n$$\nW_{TF} = TF\n$$\n\nå³ä¸€ä¸ªtermåœ¨docmentä¸­å‡ºç°äº†å‡ æ¬¡TFå°±ç­‰äºå‡ ã€‚\n\n## å¯¹æ•°\n\n$$\nW_{TF} = 1+ \\log (TF)\n$$\n\nå…¶ä¸­$$TF$$æ˜¯termåœ¨documentä¸­å®é™…å‡ºç°çš„æ¬¡æ•°ã€‚å¯¹å…¶é€šè¿‡å¯¹æ•°å˜æ¢åçš„$W_{TF}$ä½œä¸ºtermåœ¨è¯¥documentä¸­çš„æƒé‡ã€‚è¯¥å…¬å¼æ˜¯åŸºäºå¦‚ä¸‹è§‚ç‚¹è®¾è®¡çš„ï¼š\n\n- å–å¯¹æ•°ï¼šå³ä½¿ä¸€ä¸ªtermå‡ºç°äº†10æ¬¡ï¼Œä¹Ÿä¸åº”è¯¥åœ¨è®¡ç®—ç‰¹å¾æƒå€¼æ—¶ï¼Œæ¯”å‡ºç°1æ¬¡çš„æƒ…å†µæƒå€¼å¤§10å€ï¼Œæ‰€ä»¥åŠ å…¥$\\logâ€‹$æœºåˆ¶æŠ‘åˆ¶è¿™ç§è¿‡å¤§çš„å·®å¼‚ã€‚\n- æ•°å­—1ï¼šå…¬å¼ä¸­çš„æ•°å­—1æ˜¯ä¸ºäº†å¹³æ»‘è®¡ç®—ç”¨çš„ï¼Œå› ä¸ºå¦‚æœTFå€¼ä¸º1çš„æƒ…å†µä¸‹ï¼Œå–$\\logâ€‹$åå€¼ä¸º0ï¼Œå³æœ¬æ¥å‡ºç°äº†ä¸€æ¬¡çš„termï¼ŒæŒ‰ç…§è¿™ç§æ–¹æ³•è®¡ç®—ä¼šè®¤ä¸ºè¿™ä¸ªtermä»æ¥æ²¡æœ‰åœ¨æ–‡æ¡£ä¸­å‡ºç°è¿‡ï¼Œä¸ºäº†é¿å…è¿™ç§æƒ…å½¢ï¼Œé‡‡ç”¨åŠ 1çš„æ–¹å¼æ¥è¿›è¡Œå¹³æ»‘ã€‚\n\n## å¢å¼ºå‹è§„èŒƒåŒ–TF\n\n$$\nW_{TF} = a+(1-a)\\times \\frac{TF}{\\max(TF)}\n$$\n\nå…¬å¼ä¸­çš„TFä»£è¡¨è¿™ä¸ªtermçš„å®é™…è¯é¢‘æ•°ç›®ï¼Œ$è€Œ\\max(TF)â€‹$ä»£è¡¨äº†æ–‡æ¡£ä¸­æ‰€æœ‰å•termå‡ºç°æ¬¡æ•°æœ€å¤šçš„é‚£ä¸ªå•termåº”çš„è¯é¢‘æ•°ç›®ï¼Œ$aâ€‹$æ˜¯è°ƒèŠ‚å› å­ï¼Œè¿‡å»ç»éªŒå–å€¼0.5ï¼Œæ–°çš„ç ”ç©¶è¡¨æ˜å–å€¼ä¸º0.4æ•ˆæœæ›´å¥½ã€‚\n\nä¹‹æ‰€ä»¥è¦å¦‚æ­¤æ“ä½œï¼Œä¸»è¦å‡ºäºå¯¹é•¿æ–‡æ¡£çš„ä¸€ç§æŠ‘åˆ¶ï¼Œå› ä¸ºå¦‚æœæ–‡æ¡£è¾ƒé•¿ï¼Œä¸çŸ­æ–‡æ¡£ç›¸æ¯”ï¼Œåˆ™é•¿æ–‡æ¡£ä¸­æ‰€æœ‰termçš„TFå€¼ä¼šæ™®éæ¯”çŸ­æ–‡æ¡£çš„å€¼é«˜ï¼Œä½†æ˜¯è¿™å¹¶ä¸æ„å‘³ç€é•¿æ–‡æ¡£ä¸æŸ¥è¯¢æ›´ç›¸å…³ã€‚ç”¨termå®é™…è¯é¢‘é™¤ä»¥æ–‡æ¡£ä¸­æœ€é«˜è¯é¢‘ï¼Œç­‰äºå°†ç»å¯¹çš„æ•°å€¼è¿›è¡Œäº†è§„èŒƒåŒ–è½¬æ¢ï¼Œå…¬å¼çš„å«ä¹‰å°±è½¬æ¢ä¸ºï¼šåŒä¸€ä¸ªæ–‡æ¡£å†…termä¹‹é—´çš„ç›¸å¯¹é‡è¦æ€§ã€‚å³ä½¿ä¸€ä¸ªæ–‡æ¡£å¾ˆé•¿ï¼Œtermè¯é¢‘æ™®éå¾ˆé«˜ï¼Œä½†æ˜¯é™¤ä»¥æ–‡æ¡£æœ€é«˜è¯é¢‘ï¼Œé‚£ä¹ˆé€šè¿‡è¿™ç§è®¡ç®—æ–¹å¼å¾—å‡ºçš„æ•°å€¼æ¯”çŸ­æ–‡æ¡£æ¥è¯´å¹¶ä¸ä¸€å®šå°±å¤§ã€‚è¿™æ ·å°±å‰”é™¤äº†æ–‡æ¡£é•¿åº¦å› ç´ çš„å½±å“ï¼Œé•¿æ–‡æ¡£å’ŒçŸ­æ–‡æ¡£çš„è¯é¢‘å› å­å°±æˆä¸ºå¯æ¯”çš„äº†ã€‚\n\n# é€†æ–‡æ¡£é¢‘ç‡IDF\n\nIDFä»£è¡¨çš„æ˜¯æ–‡æ¡£é›†åˆèŒƒå›´å†…termçš„ç›¸å¯¹é‡è¦æ€§ã€‚\n\né€†æ–‡æ¡£é¢‘ç‡å› å­IDFï¼Œå…¶è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š\n$$\nIDF_k = \\log{\\frac{N}{n_k}}\n$$\nå…¶ä¸­çš„$N$ä»£è¡¨æ–‡æ¡£é›†åˆä¸­æ€»å…±æœ‰å¤šå°‘ä¸ªæ–‡æ¡£ï¼Œè€Œ$n_k$ä»£è¡¨ç‰¹å¾term $k$åœ¨å…¶ä¸­å¤šå°‘ä¸ªæ–‡æ¡£ä¸­å‡ºç°è¿‡ï¼Œå³æ–‡æ¡£é¢‘ç‡ã€‚ç”±å…¬å¼å¯çŸ¥ï¼Œæ–‡æ¡£é¢‘ç‡n_kè¶Šé«˜ï¼Œåˆ™å…¶IDFå€¼è¶Šå°ï¼Œå³è¶Šå¤šçš„æ–‡æ¡£åŒ…å«æŸä¸ªtermï¼Œé‚£ä¹ˆå…¶IDFæƒå€¼è¶Šå°ã€‚IDFå°±æ˜¯è¡¡é‡ä¸åŒtermå¯¹æ–‡æ¡£çš„åŒºåˆ†èƒ½åŠ›çš„ï¼Œå…¶å€¼è¶Šé«˜ï¼Œåˆ™å…¶åŒºåˆ†ä¸åŒæ–‡æ¡£å·®å¼‚çš„èƒ½åŠ›è¶Šå¼ºï¼Œåä¹‹åˆ™åŒºåˆ†èƒ½åŠ›è¶Šå¼±ã€‚æ•´ä½“è€Œè¨€ï¼ŒIDFçš„è®¡ç®—å…¬å¼æ˜¯åŸºäºç»éªŒå’Œç›´è§‰çš„ï¼Œæœ‰ç ”ç©¶è€…è¿›ä¸€æ­¥åˆ†æè®¤ä¸ºï¼šIDFä»£è¡¨äº†termå¸¦æœ‰çš„ä¿¡æ¯é‡çš„å¤šå°‘ï¼Œå…¶å€¼è¶Šé«˜ï¼Œè¯´æ˜å…¶ä¿¡æ¯å«é‡è¶Šå¤šï¼Œå°±è¶Šæœ‰ä»·å€¼ã€‚\n\n# TF-IDF\n\nTF-IDFæ¡†æ¶å°±æ˜¯ç»“åˆäº†ä¸Šè¿°çš„è¯é¢‘å› å­å’Œé€†æ–‡æ¡£é¢‘ç‡å› å­çš„è®¡ç®—æ¡†æ¶ï¼Œä¸€èˆ¬æ˜¯å°†ä¸¤è€…ç›¸ä¹˜ä½œä¸ºç‰¹å¾æƒå€¼ï¼Œç‰¹å¾æƒå€¼è¶Šå¤§ï¼Œåˆ™è¶Šå¯èƒ½æ˜¯å¥½çš„æŒ‡ç¤ºè¯ï¼Œå³ï¼š\n$$\nW_{Term} = TF \\times IDF\n$$\nä»ä¸Šè¿°å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºæŸä¸ªæ–‡æ¡£Dæ¥è¯´ï¼š\nå¦‚æœDä¸­æŸä¸ªtermçš„è¯é¢‘å¾ˆé«˜ï¼Œè€Œä¸”è¿™ä¸ªtermåœ¨æ–‡æ¡£é›†åˆçš„å…¶ä»–æ–‡æ¡£ä¸­å¾ˆå°‘å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªtermçš„æƒå€¼ä¼šå¾ˆé«˜ã€‚\nå¦‚æœDä¸­æŸä¸ªtermçš„è¯é¢‘å¾ˆé«˜ï¼Œä½†æ˜¯è¿™ä¸ªtermåœ¨æ–‡æ¡£é›†åˆçš„å…¶ä»–æ–‡æ¡£ä¸­ä¹Ÿç»å¸¸å‡ºç°ï¼Œæˆ–è€…termè¯é¢‘ä¸é«˜ï¼Œä½†æ˜¯åœ¨æ–‡æ¡£é›†åˆçš„å…¶ä»–æ–‡æ¡£ä¸­å¾ˆå°‘å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªtermçš„æƒå€¼ä¸€èˆ¬ã€‚\nå¦‚æœDä¸­æŸä¸ªtermè¯é¢‘å¾ˆä½ï¼ŒåŒæ—¶è¿™ä¸ªtermåœ¨æ–‡æ¡£é›†åˆçš„å…¶ä»–æ–‡æ¡£ä¸­ç»å¸¸å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªtermæƒå€¼å¾ˆä½ã€‚\n\n# å‚è€ƒèµ„æ–™\n\nè¿™å°±æ˜¯æœç´¢å¼•æ“","tags":["IR"],"categories":["IR"]},{"title":"æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼","url":"%2Fblog%2FLaplace-Approximation.html","content":"\nåœ¨æœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œå¾ˆå¤šæ—¶å€™æ— æ³•ç¡®å®šä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å…·ä½“å¯†åº¦å‡½æ•°ï¼Œå› è€Œåœ¨å¯¹è¿™ç§åˆ†å¸ƒè¿›è¡Œåç»­æ“ä½œï¼ˆä¾‹å¦‚ï¼Œè´å¶æ–¯å­¦æ´¾æ±‚åéªŒæ¦‚ç‡æ—¶ï¼‰æ—¶éš¾åº¦å¾ˆå¤§ï¼Œæ— æ³•è¿›è¡Œã€‚ä¸ºäº†ç®€åŒ–é—®é¢˜ç»å¸¸éœ€è¦å¯¹è¿™ç§å¤æ‚åˆ†å¸ƒè¿›è¡Œè¿‘ä¼¼ï¼Œä»è€Œæ–¹ä¾¿è®¡ç®—æˆ–æ“ä½œã€‚ç›®å‰å¸¸ç”¨çš„è¿‘ä¼¼ç®—æ³•ä¸»è¦æœ‰ä¸‰ç§ï¼šæ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ã€å˜åˆ†è¿‘ä¼¼ã€Gibbsé‡‡æ ·ã€‚æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ä¾¿æ˜¯ä¸€ç§ç®€å•ä¸”å¹¿æ³›åº”ç”¨çš„è¿‘ä¼¼æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¯å¾ˆå¤šé‡‡æ ·æ–¹æ³•çš„åŸºç¡€æ€æƒ³ï¼Œæœ¬æ–‡ä»‹ç»æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ï¼ˆLaplace Approximationï¼‰ã€‚\n\næ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼çš„åŸºæœ¬æ€æƒ³æ˜¯**ä½¿ç”¨ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒæ¥è¿‘ä¼¼å¤æ‚åˆ†å¸ƒ**ï¼Œæ±‚è§£è¿‡ç¨‹å³ä¸ºæ±‚æ­£æ€åˆ†å¸ƒçš„æœŸæœ›$\\mu$å’Œæ–¹å·®$\\sigma^2$(æˆ–ç²¾åº¦$\\frac{1}{\\sigma^2}$)çš„è¿‡ç¨‹ã€‚æ³¨æ„ï¼šè¯¥æ–¹æ³•æ˜¯ç”¨äºå¯¹**è¿ç»­**å˜é‡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°è¿›è¡Œè¿‘ä¼¼çš„ã€‚\n\n# ä¸€ç»´å˜é‡çš„æƒ…å½¢\n\nå‡è®¾æœ‰ä¸€ä¸ªå˜é‡$z$ï¼ˆæ ‡é‡ï¼‰ï¼Œå…¶åˆ†å¸ƒä¸º$p(z)$ï¼Œè¯¥åˆ†å¸ƒçš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š\n\n\n$$\np(z)=\\frac{1}{Z}f(z) \\\\\nZ=\\int f(z) dz\n$$\nå…¶ä¸­$Z$ä¸ºå½’ä¸€åŒ–é¡¹ã€‚\n\nä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼å¯»æ‰¾ä¸€ä¸ªæ­£æ€åˆ†å¸ƒ$q(z)$æ¥è¿‘ä¼¼è¿™ä¸ªåˆ†å¸ƒ$p(z)$ã€‚åªéœ€è¦æ±‚å‡º$q(z)$çš„æœŸæœ›$\\mu$å’Œæ–¹å·®$\\sigma^2$(æˆ–ç²¾åº¦$\\frac{1}{\\sigma^2}$)ã€‚\n\næ€ä¹ˆåœ¨æ•°å­¦ä¸Šå®šä¹‰è¿™ä¸ªè¿‘ä¼¼å‘¢ï¼Ÿ\n\nåœ¨æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ä¸­åˆ†å¸ƒ$q(z)$ä¸åˆ†å¸ƒ$p(z)$è¿‘ä¼¼å®šä¹‰ä¸ºï¼š$q(z)$çš„å³°å€¼ä¸$p(z)$çš„å³°å€¼ç›¸ç­‰ã€‚å³$q(z)$çš„æœŸæœ›ç­‰äº$p(z)$æœ€å¤§å€¼ã€‚\n\nå…ˆè§‚å¯Ÿä¸€ä¸‹å•å˜é‡æ­£æ€åˆ†å¸ƒå…¬å¼çš„å½¢å¼ï¼š\n$$\n\\mathcal{N}(x|\\mu,\\sigma^{2})=\\frac{1}{(2\\pi \\sigma ^{2})^{1/2}}\\ \\exp \\{-\\frac{1}{2\\sigma ^{2}}(x-\\mu)^{2}\\} \\tag1\n$$\næŒ‡æ•°é¡¹ä¸­åŒ…å«$(x-\\mu)^{2}$ï¼Œåœ¨æ­£æ€åˆ†å¸ƒ$q(z)$ä¸­çš„å‡å€¼$\\mu$åº”è¯¥ä¸º$p(z)$çš„æœ€å¤§å€¼ç‚¹ã€‚\n\nå‡è®¾$z_0$æ˜¯$p(z)$çš„æœ€å¤§å€¼ç‚¹ï¼Œåˆ™åœ¨$z_0$å¤„çš„ä¸€é˜¶å¯¼æ•°ä¸º0ï¼š\n$$\n\\frac{dp(z)}{dz} \\bigg|_{z=z_0} = 0 \\Leftrightarrow \\frac{df(z)}{dz} \\bigg|_{z = z_0} =0\n$$\n\nå€Ÿç”¨$\\ln f(z)$åœ¨$z_0$å¤„çš„äºŒé˜¶Taylorå±•å¼€æ¥æ„å»ºæŒ‡æ•°ä¸­çš„å¹³æ–¹é¡¹ï¼Œå¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\n\\ln f(z) &\\simeq \\ \\ln f(z_{0}) -\\frac{1}{2} A (z-z_{0})^{2} \\\\\nwhere: &\\ \\ A=-\\frac{d^{2}}{dz^2} \\ln f(z)\\bigg|_{z=z_0}\n\\end {aligned} \\tag 2\n$$\nnoteï¼šTaylorå±•å¼€çš„ä¸€é˜¶é¡¹ä¸º0ã€‚\n\nå¯¹å¼2ä¸¤è¾¹å»æ‰$\\ln$ï¼Œ åˆ™ï¼š\n$$\nf(z) \\simeq f(z_{0})\\exp \\{-\\frac{A}{2}(z-z_{0})^{2}\\} \\tag 3\n$$\nnoteï¼šä¸Šå¼æ˜¯æœªå½’ä¸€åŒ–çš„ã€‚\n\nå¯¹æ¯”å¼3ä¸­çš„æŒ‡æ•°é¡¹ä¸å¼1ä¸­çš„æŒ‡æ•°é¡¹å¯ä»¥å‘ç°ï¼Œå¦‚æœ$A= \\frac{1}{\\sigma^2}$ ï¼Œåˆ™å¯å¾—å½’ä¸€åŒ–çš„é«˜æ–¯å‡½æ•°$q(z)$ä¸ºä¸‹å¼ï¼š\n$$\nq(z)=(\\frac{A}{2\\pi })^{1/2}\\ \\exp \\{-\\frac{A}{2}(z-z_{0})^{2\\}}\n$$\n **æ€»ç»“ï¼š**\n\n$q(z)$çš„\n\næœŸæœ›$\\mu=z_0$ ï¼Œå³ï¼Œ$f(z)$çš„æœ€å¤§å€¼ç‚¹ï¼›\n\nç²¾åº¦$\\frac{1}{\\sigma^2}=A$ï¼Œå³ï¼Œ$\\ln f(z)$åœ¨$z_0$å¤„çš„äºŒé˜¶å¯¼æ•°ï¼Œç”±äº$z_0$æ˜¯æå¤§å€¼ç‚¹ï¼Œæ‰€ä»¥è¦æ±‚$A>0$ã€‚\n\n# å¤šç»´å˜é‡çš„æƒ…å½¢\n\nå¤šç»´çš„æƒ…å½¢æ¨å¯¼è¿‡ç¨‹å’Œä¸€ç»´çš„æƒ…å½¢ç±»ä¼¼ï¼Œç®€å•æ¨å¯¼å¦‚ä¸‹ã€‚\n\nå¤šç»´å˜é‡çš„æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼çš„ç›®çš„æ˜¯å¯»æ‰¾ä¸€ä¸ªå¤šç»´æ­£æ€åˆ†å¸ƒ$q(\\mathbf z)$æ¥è¿‘ä¼¼è¿™ä¸ªåˆ†å¸ƒ$p(\\mathbf z)$ã€‚\n\né¦–å…ˆçœ‹ä¸€ä¸‹å¤šç»´å˜é‡$\\mathbf x$çš„é«˜æ–¯åˆ†å¸ƒå…¬å¼çš„å½¢å¼:\n$$\n\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu },\\mathbf{\\Sigma })=\\frac{1}{(2\\pi )^{D/2}}\\frac{1}{|\\mathbf{\\Sigma }|^{1/2}} \\exp \\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu })^{\\mathrm{T}}\\mathbf{\\Sigma }^{-1}(\\mathbf{x}-\\mathbf{\\mu })\\} \\tag 4\n$$\nå…¶ä¸­$D$ä¸º$\\mathbf x$çš„ç»´åº¦ã€‚\n\nå‡è®¾æœ‰ä¸€ä¸ªå˜é‡$\\mathbf z$ï¼ˆ$M$ç»´å‘é‡ï¼Œç²—ä½“ï¼‰ï¼Œå…¶åˆ†å¸ƒä¸º$p(\\mathbf z)$ï¼Œè¯¥åˆ†å¸ƒçš„å…·ä½“å½¢å¼æœªçŸ¥ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š\n$$\np(\\mathbf z)=\\frac{1}{\\mathbf Z}f(\\mathbf z) \\\\\n\\mathbf Z=\\int f(\\mathbf z) dz\n$$\nå…¶ä¸­$\\mathbf Z$ä¸ºå½’ä¸€åŒ–é¡¹ã€‚\n\nä»¤$\\mathbf z_0$ä¸º$f(\\mathbf z) $çš„é©»ç‚¹ï¼Œ\n$$\n\\ln f(\\mathbf{z}) \\simeq \\ \\ln f(\\mathbf{z}_{0})-\\frac{1}{2}(\\mathbf{z}- \\mathbf{z}_{0})^{\\mathrm{T}}\\mathbf{A}(\\mathbf{z}-\\mathbf{z}_{0}) \\tag 5\n$$\nå…¶ä¸­$\\mathbf A$ä¸º$M \\times M$çš„çŸ©é˜µï¼Œæ˜¯$\\ln f(\\mathbf z)$åœ¨é©»ç‚¹$\\mathbf z_0$å¤„çš„æµ·æ£®çŸ©é˜µ(Hessian matrix)ã€‚\n$$\n\\mathbf{A}= \\nabla\\nabla \\ln f(\\mathbf{z}) \\bigg|_{\\mathbf{z}= \\mathbf{z_0}}\n$$\nå¯¹äºå…¬å¼5ä¸¤è¾¹å»æ‰$\\ln$ï¼Œåˆ™ï¼š\n$$\nf(\\mathbf{z})\\simeq f(\\mathbf{z}_{0})\\exp\\{-\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_{0})^{\\mathrm T}\\mathbf{A}(\\mathbf{z}-\\mathbf{z}_{0})\\} \\tag6\n$$\nå¯¹æ¯”å¼4ä¸­çš„æŒ‡æ•°é¡¹ä¸å¼6ä¸­çš„æŒ‡æ•°é¡¹å¯ä»¥å‘ç°ï¼Œå¦‚æœ$A= {\\Sigma }^{-1}$ ï¼Œåˆ™å¯å¾—å½’ä¸€åŒ–çš„é«˜æ–¯å‡½æ•°$q(\\mathbf z)$ä¸ºä¸‹å¼ï¼š\n$$\nq(\\mathbf z)=\\frac{A^{1/2}}{(2\\pi )^{M/2}} \\exp \\{-\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_0)^{\\mathrm{T}}\\mathbf{\\Sigma }^{-1}(\\mathbf{z}-\\mathbf{z}_0)\\}\n$$\nå…¶ä¸­$|\\mathbf A|$ä¸ºçŸ©é˜µ$\\mathbf A$çš„è¡Œåˆ—å¼ã€‚\n\n# æ€»ç»“\n\næ±‚ä¸€ä¸ªæœªçŸ¥åˆ†å¸ƒçš„æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼çš„è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š\n\n1. æ±‚å…¶åˆ†å¸ƒå‡½æ•°çš„æœ€å¤§å€¼(mode)ï¼Œå³ï¼Œé©»ç‚¹ã€‚é€šå¸¸æ˜¯é€šè¿‡ä¸€äº›æ•°å€¼ä¼˜åŒ–ç®—æ³•æ±‚çš„ã€‚å®é™…æƒ…å†µä¸‹ä¼šå­˜åœ¨å¤šå³°æƒ…å†µçš„åˆ†å¸ƒï¼Œé‚£ä¹ˆå¯ä»¥å¯¹ä¸åŒçš„æ³¢å³°è¿›è¡Œæ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ã€‚\n2. æ±‚æµ·æ£®çŸ©é˜µã€‚\n\næ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œéšç€æ•°æ®é‡çš„å¢åŠ ï¼Œä¸€ä¸ªæ¨¡å‹çš„åéªŒåˆ†å¸ƒä¼šè¶Šæ¥è¶Šä¸é«˜æ–¯åˆ†å¸ƒç›¸ä¼¼ï¼Œæ‰€ä»¥æ•°æ®é›†è¶Šå¤§ï¼Œæ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼çš„ä½œç”¨çº¦ç†æƒ³ã€‚\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning","tags":["Approximation"],"categories":["Math"]},{"title":"å¯å†³ç³»æ•°(Coefficient of Determination)","url":"%2Fblog%2FCoefficient-of-Determination.html","content":"\nåœ¨scikit-learnä¸­å®šä¹‰çš„å›å½’é—®é¢˜æ¨¡å‹è¯„ä»·æŒ‡æ ‡æœ‰ï¼š\n\n>explained_variance_score(explained_variance)\n>mean_absolute_error(MAE)\n>mean_squared_error(MSE)\n>mean_squared_log_error(MSLE)\n>median_absolute_error(MedAE)\n>r2_score($R^2$)\n\nå¯å†³ç³»æ•°$R^2$æ˜¯è¯„ä»·å›å½’æ¨¡å‹å¥½è¿˜çš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ï¼Œæœ¬æ–‡ä»‹ç»å®ƒçš„æ•°å­¦åŸºç¡€ã€‚\n\n# ç¬¦å·è¯´æ˜\n| ç¬¦å·          | æ¶µä¹‰                                       |\n| ----------- | ---------------------------------------- |\n| $x_i$       | æ ·æœ¬ç‰¹å¾ç‰¹å¾ï¼Œè‡ªå˜é‡                               |\n| $y_i$       | æ ·æœ¬$x_i$ çš„è§‚å¯Ÿå€¼ï¼Œå› å˜é‡                         |\n| $\\hat{y}_i$ | å›å½’æ¨¡å‹å¯¹äºæ ·æœ¬$x_i$ çš„é¢„æµ‹å€¼                       |\n| $\\bar{y}$   | æ‰€æœ‰æ ·æœ¬è§‚å¯Ÿå€¼ï¼ˆå› å˜é‡ï¼‰çš„å¹³å‡å€¼ ${\\bar {y}}={\\frac {1}{n}}\\sum _{i=1}^{n}y_{i}$ |\n\n# å¯å†³ç³»æ•°æ¨å¯¼è¿‡ç¨‹\n\nç¦»å·®å¹³æ–¹å’Œï¼šå› å˜é‡$yâ€‹$ç›¸å¯¹äºå…¶å‡å€¼çš„å·®å¼‚ï¼š$\\sum_i(y_i- \\bar{y})^2â€‹$ ã€‚\n\nå¯¹ç¦»å·®å¹³æ–¹å’Œè¿›è¡Œå¦‚ä¸‹åˆ†è§£ï¼š\n$$\n\\begin {aligned}\n\\sum_i(y_i- \\bar{y})^2 &= \\sum_i\\{(y_i - \\hat{y})+(\\hat{y}_i-\\bar{y})\\}^2 \\\\\n&=\\sum_i(y_i - \\hat{y}_i)^2 +\\sum_i(\\hat{y}_i-\\bar{y})^2 + \\sum_i2(y_i - \\hat{y}_i)(\\hat{y}_i-\\bar{y}) \\\\\n&=\\sum_i(y_i - \\hat{y}_i)^2 + \\sum_i(\\hat{y}_i-\\bar{y})^2\n\\end {aligned}\n$$\n\nä¸Šå¼ä¸­ç­‰å·ä¸¤è¾¹åˆ†åˆ«å®šä¹‰å¦‚ä¸‹ï¼š\n\næ€»ä½“ç¦»å·®å¹³æ–¹å’Œï¼š $SST = \\sum_i(y_i- \\bar{y})^2$ ååº”å› å˜é‡çš„è§‚å¯Ÿå€¼ä¸å…¶å‡å€¼çš„æ€»ä½“ç¦»å·®\n\nå›å½’å¹³æ–¹å’Œï¼š $SSR= \\sum_i(\\hat{y}_i-\\bar{y})^2=\\sum_i(\\beta x_i-\\bar{y})^2$ å› å˜é‡çš„ç†è®ºå€¼ä¸æ ·æœ¬å‡å€¼çš„ç¦»å·®ï¼Œå³æœ‰$x$ ä¸$\\hat y$ä¹‹é—´çš„çº¿æ€§å…³ç³»å¼•èµ·çš„$\\hat y$çš„å–å€¼çš„å˜åŒ–ã€‚ä¹Ÿå°±æ˜¯å›å½’æ¨¡å‹èƒ½å¤Ÿè§£é‡Šçš„ç¦»å·®\n\næ®‹å·®å¹³æ–¹å’Œï¼š$SSE=\\sum_i(y_i - \\hat{y}_i)^2\\sum_i e_i^2$ ååº”é™¤$x$ä¸$\\hat y$çš„çº¿æ€§å…³ç³»ä»¥å¤–çš„å…¶ä»–å› ç´ å¯¹$y$çš„å–å€¼çš„å½±å“ï¼ŒåŠæ®‹å·®\n\nå¯å†³ç³»æ•°ï¼š\n$$\nR^2=\\frac{SSR}{SST} = 1-\\frac{SSE}{SST}\n$$\n\n# å¯å†³ç³»æ•°æ€»ç»“\n\nå¯å†³ç³»æ•°ååº”äº†çº¿æ€§å›å½’æ¨¡å‹èƒ½å¤Ÿè§£é‡Šçš„ç¦»å·®å æ€»ç¦»å·®çš„æ¯”ä¾‹ï¼Œå³æ¨¡å‹çš„å¥½åã€‚\n\n- $R^2 \\in [0,1]$ï¼Œå¯¹äºå¯å†³ç³»æ•°çš„æ”¹è¿›å‹ï¼Œajusted $R^2$å¯èƒ½å‡ºç°å°äº0çš„æƒ…å†µ\n- $R^2$çº¦æ¥è¿‘äº1è¯´æ˜æ¨¡å‹æ•ˆæœè¶Šå¥½\n- $R^2$éœ€è¦åœ¨æµ‹è¯•é›†ä¸Šæ±‚çš„\n- åœ¨ä¸€å…ƒå›å½’ä¸­$R^2 = r^2$ï¼Œå…¶ä¸­$R^2$ä¸ºå¯å†³ç³»æ•°ï¼Œ$r$ä¸ºç›¸å…³ç³»æ•°ã€‚\n- åœ¨å¤šå…ƒå›å½’ä¸­ï¼Œå¯å†³ç³»æ•°ç§°ä¸ºå¤šé‡å¯å†³ç³»æ•°(Maltiple Coefficient of Determination)ã€‚\n\n\n# ä¿®æ­£çš„å¯å†³ç³»æ•°\n\nåœ¨å¤šå…ƒå›å½’ä¸­ï¼Œå½“æ ·æœ¬å®¹é‡ä¸€å®šæ—¶ï¼Œéšç€ç‰¹å¾çš„å¢åŠ å¯å†³ç³»æ•°$R^2$æ€»æ˜¯ä¸Šå‡çš„ï¼Œå³ä½¿æ–°ç‰¹å¾ä¸å› å˜é‡ä¸ç›¸å…³ï¼Œ$R^2$ä¹Ÿæ˜¯ä¸Šå‡ã€‚æ‰€ä»¥ç ”ç©¶äººå‘˜é€šå¸¸ä½¿ç”¨ä¿®æ­£çš„å¯å†³ç³»æ•°(adjusted $R^2$)$R_a^2$ã€‚å…¶å®šä¹‰å¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\nR_a^2 &=1-(1-R^2)\\frac{n-1}{n-p-1} \\\\\n&=1-\\frac{SSE / (n-p-1)}{SST/(n-1)} \\\\\n&=1-\\frac{SSE / df_E}{SST/df_T}\n\\end {aligned}\n$$\nå…¶ä¸­: æ ·æœ¬é‡ä¸º$n$ï¼Œ ç‰¹å¾(è‡ªå˜é‡)æ•°é‡ä¸º$p$ï¼Œ $SST$çš„è‡ªç”±åº¦(degrees of freedom)ä¸º$df_T=n-1$ï¼Œ $SSE$çš„è‡ªç”±åº¦ä¸º$df_E=n-p-1$ã€‚ \n\n# ä¿®æ­£çš„å¯å†³ç³»æ•°æ€»ç»“\n\n- ä¿®æ­£çš„å¯å†³ç³»æ•°$R_a^2 \\le R^2$ã€‚ \n- å¯èƒ½æ˜¯è´Ÿæ•°ï¼Œå½“æ ·æœ¬é‡è¾ƒå°‘ï¼Œç‰¹å¾æ•°é‡è¾ƒå¤šæ—¶å¯èƒ½ä¼šå‡ºç°è´Ÿå€¼ã€‚\n\n\n","tags":["Metrics"],"categories":["Math"]},{"title":"åå·®-æ–¹å·®åˆ†è§£","url":"%2Fblog%2FBias-Variance-Decomposition.html","content":"\nåå·®æ–¹å·®åˆ†è§£(The Bias-Variance Decomposition)æ˜¯ç”¨æ¥è¡¡é‡æ¨¡å‹å¤æ‚åº¦çš„æ•°å­¦å·¥å…·ï¼Œä¹Ÿç§°ä¸º(the bias variance trade-off)ï¼Œæ˜¯è§£é‡Šå­¦ä¹ ç®—æ³•æ³›åŒ–èƒ½åŠ›çš„ä¸€ç§å·¥å…·ã€‚\n\n*note: ä¸ºäº†è¯´æ˜æ–¹ä¾¿ï¼Œæˆ‘ä»¬åªè®¨è®ºå›å½’é—®é¢˜ï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨å¹³æ–¹å’ŒæŸå¤±å‡½æ•°*\n\n# æ³›åŒ–è¯¯å·®\n\n> å‡è®¾æˆ‘ä»¬ç°åœ¨æœ‰æ•°æ®é›†$D=\\{(x_1,t_1),(x_2,t_2),\\dots,(x_N,t_N)\\}$ï¼Œæ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç”¨$x$è¡¨ç¤ºï¼Œlabelç”¨$t$è¡¨ç¤ºã€‚\n>\n> è¯¥æ•°æ®é›†æ˜¯ä»æ€»ä½“$\\Omega$ ä¸­ **ç‹¬ç«‹åŒåˆ†å¸ƒ** çš„æŠ½æ ·å¾—åˆ°çš„ï¼Œå³$D \\in \\Omega$ã€‚ \n>\n> $\\Omega$ä¸­æ ·æœ¬æœä»çš„åˆ†å¸ƒä¸º$p(x,t)$ï¼Œå­˜åœ¨ä¸€ä¸ªæœªçŸ¥å‡½æ•°$h(x)$èƒ½å¤Ÿç²¾ç¡®è®¡ç®—$x$åˆ°$t$çš„æ˜ å°„ã€‚ \n>\n> åœ¨æŠ½æ ·è¿‡ç¨‹ä¸­ï¼Œ(è§‚æµ‹å€¼)tæ˜¯å­˜åœ¨è¯¯å·®çš„ï¼Œæ‰€ä»¥ä¸èƒ½ä¿è¯$t_i=h(x_i)$ã€‚\n>\n> ç”±äºæ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒæŠ½æ ·ï¼Œæ‰€ä»¥$D$ä¹Ÿæœä»åˆ†å¸ƒ$p(x, t)$ ã€‚\n>\n> æˆ‘ä»¬é€šè¿‡æŸç§æ–¹æ³•ä»æ•°æ®é›†$D$ä¸­å­¦ä¹ åˆ°äº†ä¸€ä¸ªæ¨¡å‹$y(x)$ ï¼Œå³ï¼Œä¸åŒçš„æ•°æ®é›†$D$å¯¹åº”ä¸åŒçš„æ¨¡å‹è¶…å‚ã€‚\n\nåœ¨è®­ç»ƒé›†$D$ä¸Šçš„å¹³å‡è¯¯å·®ç§°ä¸º **ç»éªŒè¯¯å·®** ï¼š\n$$\nL=\\int\\int{L(t,y(x;D))p(x,t) dx dt}\n$$\nå¯¹äºå›å½’é—®é¢˜æœ¬æ–‡ä½¿ç”¨å¹³æ–¹å’ŒæŸå¤±å‡½æ•°ï¼ˆsquared lossï¼‰åˆ™$L$ä¸ºï¼š\n$$\nL=\\int\\int{ [y(x;D)-t]^2p(x,t) dx dt}\n$$\n\næ¨¡å‹åœ¨æ€»ä½“$\\Omega$ä¸Šçš„å¹³å‡è¯¯å·®ç§°ä¸º **æ³›åŒ–è¯¯å·®**ã€‚å› ä¸ºæˆ‘ä»¬æ˜¯æ— æ³•è§‚å¯Ÿåˆ°æ€»ä½“$\\Omega$çš„ï¼Œæ‰€ä»¥æ³›åŒ–è¯¯å·®æ˜¯æ— æ³•ç›´æ¥è®¡ç®—çš„ã€‚\n\nä½†æ˜¯å¯ä»¥è§‚å¯Ÿåˆ°$D$ï¼Œå› ä¸º$D$æ˜¯$\\Omega$çš„ç‹¬ç«‹åŒåˆ†å¸ƒé‡‡æ ·è·å¾—çš„ï¼Œæ‰€ä»¥ç»éªŒè¯¯å·®çš„æœŸæœ›ç­‰äºæ³›åŒ–è¯¯å·®ã€‚\n\n$$\n\\begin {aligned}\nE_D[L] =& \\int_D \\int\\int{ [y(x;D)-t]^2p(x,t) dx dt dD} \\\\\n\\end {aligned} \\tag 1\n$$\n\nnote: \n\n1. æ³›åŒ–è¯¯å·®æ˜¯ç»éªŒè¯¯å·®$L$éƒ½$D$ç§¯åˆ†å¾—åˆ°çš„(take the expectation of this expression with respect to $D$)ã€‚\n2. å¯¹äºä¸åŒçš„æ•°æ®é›†$D$ï¼Œæ¨¡å‹çš„è¶…å‚æ˜¯ä¸åŒçš„ã€‚å¯ä»¥é€šè¿‡è´å¶æ–¯ç†è®ºç†è§£ã€‚è´å¶æ–¯æ–¹æ³•è®¤ä¸ºè¶…å‚æ˜¯ä¸€ä¸ªåéªŒåˆ†å¸ƒï¼Œå³è¶…å‚ä¸æ˜¯ä¸€ä¸ªå›ºå®šå€¼ã€‚é¢‘ç‡å­¦æ´¾è®¤ä¸ºè¶…å‚æ˜¯ä¸€ä¸ªç‚¹ä¼°è®¡ï¼Œå¯¹äºä¸åŒçš„æ•°æ®é›†å°†å¾—åˆ°ä¸€ä¸ªå›ºå®šä¼°è®¡å€¼ï¼Œæ‰€ä»¥è¶…å‚æ˜¯ä¸åŒæ•°æ®é›†ç‚¹ä¼°è®¡çš„æœŸæœ›ã€‚\n3. å…¬å¼1 çš„è¡¨è¿°æˆ‘æ²¡æœ‰åœ¨çœ‹åˆ°çš„èµ„æ–™ä¸­æ‰¾åˆ°ï¼Œå¦‚æœä¸ªäººè¡¨è¾¾æœ‰è¯¯ï¼Œä»¥åä¿®æ­£ã€‚\n4. ä¸‹é¢çš„å…¬å¼æ¨å¯¼éƒ½æ˜¯ä»¥å…¬å¼1 ä¸ºåŸºç¡€ã€‚\n\n# å˜é‡å®šä¹‰\n\næœ¬æ–‡ä¸­ä½¿ç”¨çš„å˜é‡å®šä¹‰å¦‚ä¸‹ï¼š\n\n| ç¬¦å·            | æ¶µä¹‰                                 |\n| ------------- | ---------------------------------- |\n| $x$           | æ ·æœ¬ç‰¹å¾å€¼                              |\n| $D$           | æ•°æ®é›†                                |\n| $t$           | $x$ åœ¨æ•°æ®é›†$D$ä¸­çš„æ ‡è®°                    |\n| $h(x)$        | $x$ çš„çœŸå®æ ‡è®°                          |\n| $y(x;D)$      | åœ¨æ•°æ®é›† $D$ å­¦åˆ°çš„æ¨¡å‹å¯¹$D$ä¸­æ ·æœ¬$x$çš„é¢„æµ‹æ ‡è®°      |\n| $L$           | ç»éªŒè¯¯å·®                               |\n| $E_D[L]$      | æ³›åŒ–è¯¯å·®ï¼Œ$L$åœ¨$D$ä¸Šçš„æœŸæœ›                   |\n| $E_D[y(x;D)]$ | å¯ä»¥ç†è§£ä¸ºæœ‰å¤šä¸ªæ•°æ®é›†$D$ï¼Œå°†$x$çš„é¢„æµ‹å€¼åœ¨æ‰€æœ‰çš„$D$ä¸Šæ±‚æœŸæœ› |\n\n# æ¨å¯¼è¿‡ç¨‹\n\nåå·®-æ–¹å·®åˆ†è§£é€šè¿‡å°†æ¨¡å‹çš„**æ³›åŒ–è¯¯å·®**( expected  loss)åˆ†è§£æˆ æ–¹å·®(variance) åå·®(bias) å’Œ å™ªå£°(noise)ä¸‰éƒ¨åˆ†ï¼Œæ¥åˆ†æä¸€ä¸ªæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\nå½“æ ·æœ¬é‡è¶³å¤Ÿå¤§æ—¶ $h(x)=E_t[t|x] = \\int tp(t|x)dt$ã€‚\n\nä½¿ç”¨æœ‰é™æ ·æœ¬é›†$D$ è®­ç»ƒå‡ºæ¥çš„æœ€ä¼˜æ¨¡å‹ $y(x;D) = E_t[t|x]=\\int tp(t|x)$ã€‚(Pattern Recognition and Machine Learningå…¬å¼ 1.87-1.89æ¨å¯¼å¾—åˆ°)\n\nä¸ºäº†æ–¹ä¾¿ä¸‹é¢çš„å…¬å¼æ¨å¯¼ï¼Œå…ˆåˆ—å‡ºå¦‚ä¸‹å‡è®¾ï¼š\n\n- æ ·æœ¬çš„å™ªå£°çš„æœŸæœ›ä¸º0ï¼ˆnote åŒºåˆ«ä¸ä¸‹é¢\"æ ·æœ¬å™ªå£°\"ï¼‰ï¼Œå³åœ¨æ ·æœ¬é›†$D$ä¸Š :\n  $$\n  E [t-h(x)] = \\int\\int [t-h(x)] p(t,x) dxdt = 0\n  $$\n\n\n\n\n\n\n\n\n\n\n\n\nç”±äºæ³›åŒ–è¯¯å·®$E_D[L]$æ˜¯ç»éªŒè¯¯å·®çš„æœŸæœ›$E_D[L]=\\int_D L dD$ï¼Œå…ˆå°†å°†$L$åˆ†è§£å¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\nL &=\\int\\int{ [y(x;D)-t]^2p(x,t) dx dt} \\\\\n     &=\\int\\int{ [y(x;D)-h(x)+h(x)-t]^2p(x,t) dx dt} \\\\\n     &=\\int\\int{ \\{[y(x;D)-h(x)]+[h(x)-t]\\}^2 p(x,t) dx dt}\\\\\n     &=\\int\\int{ \\{[y(x;D)-h(x)]^2 + 2[y(x;D)-h(x)][h(x)-t] + [h(x)-t]^2\\} p(x,t) dx dt} \\\\\n\\end {aligned}\n$$\n\nå‰ä¸¤é¡¹å¯¹tç§¯åˆ†ï¼Œæ ¹æ®å™ªå£°ä¸º0çš„å‡è®¾ï¼Œæ¶ˆæ‰äº¤å‰é¡¹\n\n$$\n\\begin {aligned}\nL &=\\int{ [y(x;D)-h(x)]^2p(x)dx}  +\\int\\int{ [h(x)-t]^2 p(x,t) dx dt}\n\\end {aligned}\n$$\n\nå®šä¹‰ç¬¬äºŒé¡¹ä¸ºæ ·æœ¬å™ªå£°ï¼šååº”æ ·æœ¬æŠ½æ ·å€¼(å³è§‚æµ‹å€¼)ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚\n$$\nnoise = E\\{ [t-h(x)]^2\\} = \\int\\int [t-h(x)]^2p(t,x) dxdt \\tag 2\n$$\n$$\n\\begin {aligned}\nE_D[L] &=\\int_D \\int{ [y(x;D)-h(x)]^2p(x)dxdD}  + \\int_D noise dD \\\\\n&=\\int \\int_D{ [y(x;D)-h(x)]^2p(x)dDdx}  + noise \\\\\n\\end {aligned}\n$$\n\nç»§ç»­åˆ†è§£ç¬¬ä¸€é¡¹ï¼Œåœ¨è¢«ç§¯åˆ†é¡¹ä¸­æ·»åŠ $E_D[y(x;D)]$ï¼š\n$$\n\\begin {aligned}\n\\int \\int_D{ [y(x;D)-h(x)]^2p(x)dDdx} =& \\int\\int_D{ \\{y(x;D)-E_D[y(x;D)]+E_D[y(x;D)]-h(x)\\}^2p(x)dDdx} \\\\\n=& \\int\\int_D\\{y(x;D)-E_D[y(x;D)]\\}^2p(x)dDdx \\\\\n&+ \\int\\int_D2\\{y(x;D)-E_D[y(x;D)]\\}\\{E_D[y(x;D)]-h(x)\\}p(x)dDdx\\\\\n&+ \\int\\int_D\\{E_D[y(x;D)]-h(x)\\}^2 p(x)dDdx\n\\end {aligned}\n$$\n\nä¸‰é¡¹åˆ†åˆ«è¡¨ç¤ºå¦‚ä¸‹:\n\nç¬¬ä¸€é¡¹å®šä¹‰ä¸ºæ–¹å·®\n$$\n\\begin {aligned}\nvariance=& \\int\\int_D\\{y(x;D)-E_D[y(x;D)]\\}^2p(x)dDdx \\\\\n=&\\int E_D\\{y(x;D)-E_D[y(x;D)]\\}^2p(x)dx\n\\end {aligned} \\tag 3\n$$\nç¬¬äºŒé¡¹(äº¤å‰é¡¹)å€¼ä¸º0\n$$\n\\begin {aligned}\n& \\int\\int_D2\\{y(x;D)-E_D[y(x;D)]\\}\\{E_D[y(x;D)]-h(x)\\}p(x)dDdx\\\\\n&= \\int2\\{E_D[y(x;D)]-h(x)\\}p(x) \\int_D\\{y(x;D)-E_D[y(x;D)]\\} dD dx \\\\\n&= \\int2\\{E_D[y(x;D)]-h(x)\\}p(x) \\{\\int_Dy(x;D)dD-E_D[y(x;D)]\\} dx \\\\\n&= \\int2\\{E_D[y(x;D)]-h(x)\\}p(x) \\{E_D[y(x;D)]-E_D[y(x;D)]\\} dx \\\\\n&=0\n\\end {aligned}\n$$\nç¬¬ä¸‰é¡¹å®šä¹‰ä¸ºåå·®\n$$\n\\begin {aligned}\nbias^2 =& \\int\\int_D\\{E_D[y(x;D)]-h(x)\\}^2 p(x)dDdx \\\\\n=& \\int\\{E_D[y(x;D)]-h(x)\\}^2 p(x)dx \\\\\n\\end {aligned} \\tag 4\n$$\nç»¼ä¸Šï¼Œæ³›åŒ–è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºå¦‚ä¸‹å½¢å¼ï¼š\n$$\n\\begin {aligned}\nE_D[L] =& \\int E_D\\{y(x;D)-E_D[y(x;D)]\\}^2p(x)dx \\\\\n&+  \\int\\{E_D[y(x;D)]-h(x)\\}^2 p(x)dx \\\\\n&+ \\int\\int [t-h(x)]^2p(t,x) dxdt \\\\\n=& variance + bias^2 + noise\n\\end {aligned}\n$$\n\n## åˆ†æ\n\n$$\n\\begin {aligned}\nvariance =&\\int E_D\\{y(x;D)-E_D[y(x;D)]\\}^2p(x)dx \\\\\nbias^2=& \\int\\{E_D[y(x;D)]-h(x)\\}^2 p(x)dx \\\\\nnoise =&\\int\\int [t-h(x)]^2p(t,x) dxdt \n\\end {aligned}\n$$\n\nå¯¹ä»¥ä¸Šä¸‰é¡¹åˆ†æå¦‚ä¸‹ï¼š\n\n$bias^2$ï¼šåº¦é‡äº†å­¦ä¹ ç®—æ³•é¢„æµ‹å€¼çš„æœŸæœ›ä¸çœŸå®å€¼ä¹‹é—´çš„åç¦»ç¨‹åº¦ï¼Œå³å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›ï¼›\n\n$variance$ï¼šåº¦é‡äº†è®­ç»ƒæ•°æ®é›†çš„å˜åŒ–å¯¹å¯¹å­¦ä¹ ç®—æ³•æ€§èƒ½çš„å½±å“ã€‚\n\n$noise$ï¼š åªå’ŒæŠ½æ ·å€¼å’Œæ•°æ®çœŸå®å€¼æœ‰å…³ï¼Œæ˜¯æ ·æœ¬æ•°æ®æ‰€å›ºæœ‰çš„å™ªå£°é€ æˆçš„ã€‚æ˜¯ä»»ä½•å­¦ä¹ ç®—æ³•èƒ½å¤Ÿè¾¾åˆ°çš„æ³›åŒ–è¯¯å·®çš„æœ€å°å€¼ã€‚\n\nåå·®å’Œæ–¹å·®æ˜¯ä¸€å¯¹çŸ›ç›¾ä½“ï¼Œå®é™…åº”ç”¨ä¸­æ˜¯éœ€è¦ä»åå·®å’Œæ–¹å·®ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ä½¿å¾—æ¨¡å‹çš„æ€»çš„æ³›åŒ–è¯¯å·®æœ€å°ï¼Œå³ï¼Œtrade-off between bias and varianceã€‚åœ¨å¾ˆå¤šæ—¶å€™è¡¨ç°ä¸ºè¶…å‚æ•°çš„å–å€¼ï¼Œæ¯”å¦‚å›å½’é—®é¢˜ä¸­çš„æ­£åˆ™åŒ–ç³»æ•°ã€‚å½“æ­£åˆ™åŒ–ç³»æ•°è¿‡å°çš„æ—¶å€™æ–¹å·®è¾ƒå¤§(æ ·æœ¬é€‰å–å¯¹æ¨¡å‹å½±å“è¾ƒå¤§)ï¼Œåå·®è¾ƒå°ï¼›å½“æ­£åˆ™åŒ–ç³»æ•°è¿‡å¤§æ—¶ï¼Œæ–¹å·®è¾ƒå°ï¼Œåå·®è¾ƒå¤§ã€‚\n\n# å­˜åœ¨ä¸è¶³\n\nåå·®æ–¹å·®åˆ†è§£æ˜¯ä»é¢‘ç‡å­¦æ´¾çš„è§’åº¦åˆ†ææ¨¡å‹å¤æ‚åº¦çš„å·¥å…·ï¼Œä½†æ˜¯å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼æœ‰é™ã€‚\n\nä½¿ç”¨è¯¥æ–¹æ³•æ—¶é¦–å…ˆéœ€è¦è·å¾—å¤šä¸ªæ•°æ®é›†Dï¼Œå¯¹è¿™äº›æ•°æ®é›†æ±‚ç›¸åº”ç»Ÿè®¡é‡çš„å¹³å‡å€¼ã€‚\n\nå¦‚æœæˆ‘ä»¬çœŸçš„æœ‰å¤šä¸ªæ•°æ®é›†Dï¼ŒæŠŠè¿™äº›æ•°æ®é›†åˆå¹¶æˆä¸ºä¸€ä¸ªå¤§çš„æ•°æ®é›†æ¥ä½œä¸ºè®­ç»ƒé›†ä¼šå¾—åˆ°æ›´å¥½çš„æ¨¡å‹ã€‚\n\n\n\nä¸‹é¢ä»‹ç»ä¸€ç§éªŒè¯æ¨¡å‹åå·®æ–¹å·®çš„å·¥å…·ã€‚\n\n## Learning Curves\n\nå¢åŠ è®­ç»ƒæ•°æ®é‡æ˜¯è§£å†³æ¨¡å‹è¿‡æ‹Ÿåˆçš„ä¸€ä¸ªæœ‰æ•ˆæ‰‹æ®µï¼Œä½†æ˜¯é€šå¸¸æƒ…å†µä¸‹è·å–é¢å¤–çš„è®­ç»ƒæ•°æ®æ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚é€šè¿‡ç»˜åˆ¶**éšç€è®­ç»ƒæ•°æ®é‡çš„å¢é•¿æ¨¡å‹åœ¨åˆ¶è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„accuracyçš„å›¾**å¯ä»¥åˆ¤æ–­ä¸€ä¸ªæ¨¡å‹\n\n\n\n![](Bias-Variance-Decomposition/learning-curves.png)\n\nå¦‚ä¸Šå¦‚æ‰€ç¤ºï¼š\n\n- å³ä¸‹è§’ï¼šåœ¨æ•°æ®é‡å¢é•¿åˆ°ä¸€å®šç¨‹åº¦ä»¥åï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„å‡†ç¡®åº¦éƒ½è¶‹è¿‘ä¸desired accuracyã€‚\n- å·¦ä¸Šè§’ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„accuracyéƒ½è¾ƒä½ï¼Œè¡¨æ˜æ¨¡å‹å…·æœ‰è¾ƒå¤§çš„åå·®ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šæ¬ æ‹Ÿåˆã€‚\n- å³ä¸Šè§’ï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†accuracyæ›²çº¿å…·æœ‰è¾ƒå¤§çš„é—´éš”ï¼Œè¡¨æ˜æ¨¡å‹å…·æœ‰è¾ƒå¤§çš„æ–¹å·®ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡æ‹Ÿåˆã€‚\n\n## è§£å†³è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ\n\næˆ‘è®¤ä¸ºè§£å†³è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—®é¢˜å¯ä»¥ä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘ï¼šæ¨¡å‹ã€æ•°æ®\n\n- è¿‡æ‹Ÿåˆ\n  1. æ¨¡å‹ï¼šå‡å°æ¨¡å‹å¤æ‚åº¦ï¼ˆæ¢ç”¨æ›´ç®€å•çš„æ¨¡å‹ï¼Œå¢åŠ æ­£åˆ™åŒ–ï¼‰\n  2. æ•°æ®ï¼šå¢åŠ æ•°æ®é‡\n- æ¬ æ‹Ÿåˆ\n  1. æ¨¡å‹ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼ˆæ¢ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå‡å°æ­£åˆ™åŒ–ï¼‰\n  2. æ•°æ®ï¼šå¢åŠ æ–°çš„ç‰¹å¾\n\n##  å®ä¾‹\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nnp.random.seed(0)\niris = load_iris()\nX, y = iris.data, iris.target\nindices = np.arange(y.shape[0])\nnp.random.shuffle(indices)\nX, y = X[indices], y[indices]\ny = y > 1\n\npipe_lr = Pipeline([('scl', StandardScaler()),\n                    ('clf', LogisticRegression(penalty='l2', random_state=0, C=0.6))])\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr,\n                                                        X=X,\n                                                        y=y,\n                                                        train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=10,\n                                                        n_jobs=1)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean,\n         color='blue', marker='o',\n         markersize=5, label='training accuracy')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color='blue')\n\nplt.plot(train_sizes, test_mean,\n         color='green', linestyle='--',\n         marker='s', markersize=5,\n         label='validation accuracy')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig('./figures/learning_curve.png', dpi=300)\nplt.show()\n```\n\n![](Bias-Variance-Decomposition/learning-curves-pra.png)\n\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\nPattern Recognition and Machine Learning\n\næœºå™¨å­¦ä¹ _å‘¨å¿—å\n\n\n\n\n\n\n\n","tags":["Math"],"categories":["Math"]},{"title":"æ‹‰æ ¼æœ—æ—¥å¯¹å¶é—®é¢˜","url":"%2Fblog%2FLagrange-Duality.html","content":"\næ‹‰æ ¼æœ—æ—¥å¯¹å¶(Lagrange Dualtiy)é—®é¢˜åœ¨æœºå™¨å­¦ä¹ çš„å¾ˆå¤šæ¨¡å‹æ¨å¯¼ä¸­éƒ½ä¼šç”¨åˆ°ï¼Œå¦‚SVMã€‚åœ¨æ—¥å¸¸çš„æ–‡ç« é˜…è¯»è¿‡ç¨‹ä¸­ä¹Ÿå¤šæ¬¡é‡åˆ°ã€‚\n\nä½œç”¨ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ˜¯ç”¨åœ¨ æŸæœ€ä¼˜åŒ–é—®é¢˜çš„æ±‚è§£è¿‡ç¨‹ä¸­ã€‚\n\nè¦ç†è§£æ‹‰æ ¼æœ—æ—¥å¯¹å¶éœ€è¦å¼„æ¸…æ¥šä¸‰ä¸ªé—®é¢˜ï¼š\n\n1. åŸå§‹é—®é¢˜\n2. å¯¹å¶é—®é¢˜\n3. åŸé—®é¢˜ä¸å¯¹å¶é—®é¢˜çš„å…³ç³»\n\nä¸‹é¢é€ä¸ªè®²è§£ç€ä¸‰ä¸ªé—®é¢˜ã€‚\n\n# åŸå§‹é—®é¢˜\n\nå¯¹å¶é—®é¢˜æ˜¯ç›¸å¯¹ä¸åŸå§‹é—®é¢˜è€Œè¨€çš„ã€‚ä¸‹é¢è¿™ä¸ªå«æœ‰çº¦æŸçš„æœ€ä¼˜åŒ–é—®é¢˜å°±ç§°ä¸º **åŸå§‹æœ€ä¼˜åŒ–é—®é¢˜ æˆ– åŸå§‹é—®é¢˜** ã€‚\n$$\n\\left \\{\n\\begin {aligned}\n      & \\min f(x) \\\\\ns.t. & h_i(x) = 0 (i=1,2,\\dots ,m) \\\\\n      & g_j(x) \\le 0 (j=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right . \\tag 1\n$$\nå¯¹äºè¿™ä¸ªåŸå§‹é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ±‚è§£æ€è·¯å°±æ˜¯æ„å»ºä¸€ä¸ªå¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š\n$$\nL(x, \\lambda, \\upsilon) = f(x) + \\sum_{i=1}^m \\lambda_i h_i(x) +\\sum_{j=1}^l  \\upsilon_j g_j(x)\n$$\nå…¶ä¸­ï¼š$\\upsilon_j \\ge 0$ã€‚\n\né€šè¿‡æ±‚è§£$L(x, \\lambda, \\upsilon)$å‡½æ•°çš„æœ€å°å€¼æ¥å¾—åˆ°$f(x)$çš„æœ€å°å€¼ã€‚\n\nå°†$x$çœ‹ä½œå¸¸æ•°$\\lambda_i, \\upsilon_j$çœ‹ä½œå˜é‡ï¼Œæ±‚$L(x, \\lambda, \\upsilon)$çš„æœ€å¤§å€¼ï¼š\n$$\n\\begin {aligned}\n\\theta_P(x) & = \\max_{\\lambda_i,\\upsilon_j \\ge 0} L(x, \\lambda, \\upsilon) \\\\\n                     &= \\max_{\\lambda_i,\\upsilon_j \\ge 0} \\{ f(x) + \\sum_{i=1}^m \\lambda_i h_i(x) +\\sum_{j=1}^l  \\upsilon_j g_j(x) \\}\n\\end {aligned}\n$$\nå½“$x$ æ»¡è¶³çº¦æŸæ¡ä»¶æ—¶ï¼Œ$\\sum_{i=1}^m \\lambda_i h_i(x) = 0, \\sum_{j=1}^l  \\upsilon_j g_j(x) \\le 0$ ,æ‰€ä»¥ $\\theta_(xP) = f(x)$ã€‚\n\nå½“$x$ ä¸æ»¡è¶³çº¦æŸæ¡ä»¶æ—¶ï¼Œ $\\theta_P(x) = + \\infty$ã€‚\n\nå³ï¼š\n$$\n\\theta_P(x) =\n\\left \\{\n\\begin {aligned}\n&f(x),        &xæ»¡è¶³çº¦æŸæ¡ä»¶ \\\\\n&+\\infty,  &xä¸æ»¡è¶³çº¦æŸæ¡ä»¶\n\\end {aligned}\n\\right .\n$$\næ‰€ä»¥ï¼š\n$$\n \\min \\theta_P(x) = \\min_x \\max_{\\lambda_i,\\upsilon_j \\ge 0} L(x, \\lambda, \\upsilon) \\tag 2\n$$\nç§°ä¸º**å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„æå°æå¤§é—®é¢˜**ã€‚\n\nå½“$x$ æ»¡è¶³çº¦æŸæ¡ä»¶æ—¶ï¼š\n$$\n\\min f(x) = \\min \\theta_P(x)\n$$\nç”±äºä»¥ä¸Šç­‰ä»·å…³ç³»ï¼Œå¼(2)ä¸å¼åŸå§‹ä¼˜åŒ–é—®é¢˜(1)æ˜¯ç­‰ä»·çš„ï¼Œæ‰€ä»¥åœ¨æœ‰äº›æ–‡ç« ä¸­è®¨è®ºå¯¹å¶é—®é¢˜æ—¶å¸¸ç”¨$\\min \\theta_P(x)$è¡¨ç¤º**åŸå§‹æœ€ä¼˜åŒ–é—®é¢˜**ã€‚\n\nåŸå§‹é—®é¢˜çš„å€¼ç”¨pè¡¨ç¤ºï¼ˆé€šå¸¸æ—¶ pæ˜Ÿ, hexoéœ€è¦è½¬ä¹‰ï¼Œæˆ‘å°±ç”¨päº†ï¼‰ã€‚\n$$\np = \\min \\theta_P(x)\n$$\n\n# å¯¹å¶é—®é¢˜\n\nåœ¨ã€Šå‡¸ä¼˜åŒ–ã€‹è¿™æœ¬ä¹¦ä¸­ï¼Œå°†**å¯¹å¶å‡½æ•°**å®šä¹‰ä¸ºå¦‚ä¸‹å½¢å¼(ä¸ºäº†ç¬¦å·ç»Ÿä¸€ï¼Œæˆ‘ä¿®æ”¹äº†éƒ¨åˆ†ç¬¦å·)ï¼š\n$$\n\\theta_D(\\lambda, \\upsilon) = \\min_x L(x, \\lambda, \\upsilon)\n$$\nå³ï¼Œå¯¹å¶é—®é¢˜é¦–å…ˆå°†$\\lambda, \\upsilon$çœ‹ä½œå¸¸æ•°ï¼Œå°†$x$ çœ‹ä½œå˜é‡ï¼Œæ±‚$L(x, \\lambda, \\upsilon)$å…³äº$x$çš„æå°å€¼ã€‚\n\nç„¶åæ±‚$\\theta_D(\\lambda, \\upsilon)$çš„æå¤§å€¼ï¼š\n$$\n\\max_{\\lambda_i,\\upsilon_j \\ge 0} \\theta_D(\\lambda, \\upsilon) = \\max_{\\lambda_i,\\upsilon_j \\ge 0} \\min_x L(x, \\lambda, \\upsilon) \\tag 3\n$$\nè¿™å°±æ˜¯**å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„æå¤§æå°é—®é¢˜**ã€‚ä¸ä¹‹ç­‰ä»·çš„çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜å¦‚ä¸‹ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n      & \\max_{\\lambda_i,\\upsilon_j} \\theta_D(\\lambda, \\upsilon) = \\max_{\\lambda_i,\\upsilon_j \\ge 0} \\min_x L(x, \\lambda, \\upsilon) \\\\\ns.t. & \\upsilon_j \\ge 0,  (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right . \\tag 4\n$$\nç§°ä¸ºåŸå§‹é—®é¢˜çš„**å¯¹å¶é—®é¢˜**ã€‚\n\nå¯¹å¶é—®é¢˜çš„å€¼ä¸º:\n$$\nd = \\max_{\\lambda_i,\\upsilon_j \\ge 0} \\theta_D(\\lambda, \\upsilon)\n$$\n\nnote: å¯¹äºåŸå§‹é—®é¢˜(1)æ¥è¯´å˜é‡æ˜¯$x$ , å¯¹äºå¯¹å¶é—®é¢˜(4)æ¥è¯´å˜é‡æ˜¯$\\lambda, \\upsilon$ã€‚æ‰€ä»¥åŸå§‹é—®é¢˜çš„æœ€ä¼˜è§£è®°ä¸º$\\hat x$ï¼Œå¯¹å¶é—®é¢˜çš„æœ€ä¼˜è§£è®°ä¸º$\\hat \\lambda, \\hat \\upsilon$ã€‚\n\n# åŸé—®é¢˜ä¸å¯¹å¶é—®é¢˜çš„å…³ç³»\n\nåœ¨å®é™…åº”ç”¨ä¸­å¯¹å¶é—®é¢˜æ˜¯æ€ä¹ˆä½¿ç”¨çš„å‘¢ï¼Ÿ\n\nå½“åŸå§‹é—®é¢˜å’Œå¯¹å¶é—®é¢˜éƒ½æœ‰æœ€ä¼˜å€¼æ—¶ï¼š\n$$\n\\begin {aligned}\n& \\left \\{\n\\begin {aligned}\n\\theta_D(\\lambda, \\upsilon) = \\min_x L(x, \\lambda, \\upsilon)\\\\ \n\\\\\n\\theta_P(x) = \\max_{\\lambda_i,\\upsilon_j \\ge 0} L(x, \\lambda, \\upsilon) \\\\\n\\end {aligned}\n\\right . \\\\\n\\\\\n& \\Rightarrow  \n\\theta_D(\\lambda, \\upsilon) = \\min_x L(x, \\lambda, \\upsilon) \n\\le L(x, \\lambda, \\upsilon) \\le \n\\max_{\\lambda_i,\\upsilon_j \\ge 0} L(x, \\lambda, \\upsilon) = \\theta_P(x)\\\\\n\\\\\n& \\Rightarrow\nd = \\max_{\\lambda_i,\\upsilon_j \\ge 0} \\theta_D(\\lambda, \\upsilon)  \\le  \\min_x \\theta_P(x) = p\n\\end {aligned}\n$$\n\nå¯ä»¥çœ‹å‡ºï¼š**åœ¨æŸç§æƒ…å†µä¸‹åŸå§‹é—®é¢˜çš„æœ€ä¼˜å€¼$p$ ä¸å¯¹å¶é—®é¢˜çš„æœ€ä¼˜å€¼$d$ç›¸ç­‰ã€‚**é‚£ä¹ˆä»€ä¹ˆæƒ…å†µä¸‹åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜çš„æœ€ä¼˜å€¼ç›¸ç­‰å‘¢ï¼Ÿ\n\nä¸‹é¢ç›´æ¥ç»™å‡ºã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ä¸­çš„å‡ ä¸ªå®šç†å’Œæ¨è®ºã€‚\n\n>è€ƒè™‘åŸå§‹é—®é¢˜(1)å’Œå¯¹å¶é—®é¢˜(4)ã€‚å‡è®¾$f(x)$å’Œ$g_j(x)$æ˜¯å‡¸å‡½æ•°ï¼Œ$h_i(x)$æ˜¯ä»¿å°„å‡½æ•°ï¼›å¹¶ä¸”å‡è®¾ä¸ç­‰å¼çº¦æŸ$g_j(x)$æ—¶ä¸¥æ ¼å¯è¡Œçš„ï¼Œå³å­˜åœ¨$x$ï¼Œå¯¹äºæ‰€æœ‰çš„$i$æœ‰$g_j(x) < 0$ï¼Œåˆ™å­˜åœ¨$\\hat x, \\hat \\lambda, \\hat \\upsilon$ï¼Œä½¿$\\hat x$æ˜¯åŸé—®é¢˜çš„è§£ï¼Œ$\\hat x, \\hat \\lambda, \\hat \\upsilon$æ˜¯å¯¹å¶é—®é¢˜çš„è§£ï¼Œå¹¶ä¸”$p = d = L(\\hat x, \\hat \\lambda, \\hat \\upsilon)$\n\n\n\n> å¯¹äºåŸå§‹é—®é¢˜(1)å’Œå¯¹å¶é—®é¢˜(4)ã€‚å‡è®¾$f(x)$å’Œ$g_j(x)$æ˜¯å‡¸å‡½æ•°ï¼Œ$h_i(x)$æ˜¯ä»¿å°„å‡½æ•°ï¼›å¹¶ä¸”å‡è®¾ä¸ç­‰å¼çº¦æŸ$g_j(x)$æ—¶ä¸¥æ ¼å¯è¡Œçš„ã€‚åˆ™$\\hat x, \\hat \\lambda, \\hat \\upsilon$åˆ†åˆ«æ˜¯åŸå§‹é—®é¢˜å’Œå¯¹å¶é—®é¢˜çš„è§£çš„å……åˆ†å¿…è¦æ¡ä»¶æ˜¯$\\hat x, \\hat \\lambda, \\hat \\upsilon$æ»¡è¶³ä¸‹é¢çš„Karush -Kuhn -Tucker(KKT)æ¡ä»¶ï¼š\n> $$\n> \\left \\{\n> \\begin {aligned}\n>  \\nabla_x f(\\hat x) + \\sum_i^m \\lambda_i \\nabla_x h_i(\\hat x) + \\sum_j^l \\upsilon_j  \\nabla_x g_j(\\hat x) &= 0 \\\\\n>                                                                                                                                                                   h_i(\\hat x) &= 0 (i=1,2,\\dots ,m) \\\\\n>                                                                                                                                                                   g_j(\\hat x) &\\le 0, (j = 1,2 \\dots l) \\\\\n>                                                                                                                                                \\upsilon_j g_j(\\hat x) &= 0, (j = 1,2 \\dots l) \\\\\n>                                                                                                                                                                  \\upsilon_j & \\ge 0, (j = 1,2 \\dots l) \\\\\n>  \\end {aligned}\n> \\right .\n> $$\n>\n\nä¹Ÿå°±æ˜¯è¯´åœ¨æ»¡è¶³ä¸Šé¢çš„(KKT)æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œå¯¹å¶é—®é¢˜çš„æœ€ä¼˜å€¼å’ŒåŸå§‹é—®é¢˜çš„æœ€ä¼˜å€¼ç›¸ç­‰ã€‚\n\n# æ€»ç»“\n\nåœ¨æ±‚è§£çº¦æŸçš„æœ€ä¼˜åŒ–é—®é¢˜æ—¶ï¼Œå¦‚æœåŸå§‹é—®é¢˜æ±‚è§£å›°éš¾ï¼Œè€Œå¯¹å¶é—®é¢˜æ±‚è§£ç›¸å¯¹å®¹æ˜“ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨æ»¡è¶³KKTæ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ±‚è§£å¯¹å¶é—®é¢˜æ¥å¾—åˆ°åŸå§‹é—®é¢˜çš„æœ€ä¼˜å€¼å’Œæœ€ä¼˜è§£ã€‚\n\n\n# å‚è€ƒèµ„æ–™\n\nå‡¸ä¼˜åŒ–(Boyd)\n\nç»Ÿè®¡å­¦ä¹ æ–¹æ³•\n\n","tags":["Math"],"categories":["Math"]},{"title":"Matrix and Vector Products in Numpy","url":"%2Fblog%2FMatrix-and-Vector-Products-in-Numpy.html","content":"\næœ€è¿‘å¤ä¹ äº†numpyä¸­çš„æ•°ç»„å’Œå¹¿æ’­æœºåˆ¶å¹¶æ€»ç»“æˆä¸¤ç¯‡æ–‡ç« åˆ†åˆ«ä¸ºã€broadcasting in numpyã€‘å’Œã€ndarray in Numpyã€‘ã€‚æœ¬æ–‡æ€»ç»“Numpyæä¾›çš„å‡ ä¸ªç”¨äºçŸ©é˜µå’Œå‘é‡ä¹˜æ³•çš„çº¿æ€§ä»£æ•°å‡½æ•°ã€‚\n\n[`inner(a, b)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.inner.html#numpy.inner)\n\n**a å’Œ bçš„æœ€åä¸€ä¸ªç»´åº¦çš„å†…ç§¯ã€‚ç»“æœçš„ç»´åº¦ä¸º`a.shape[:-1] + b.shape[:-1]`**\n\nä¸€ç»´æ•°ç»„çš„å†…ç§¯\n\n```python\n>>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n```\n\nå¤šç»´æ•°ç»„ä¸ä¸€ç»´æ•°ç»„çš„å†…ç§¯\n\n```python\n>>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> np.inner(a, b)\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n```\n\næ•°ç»„ä¸æ ‡é‡\n\n```python\n>>> np.inner(np.eye(2), 7)\narray([[ 7.,  0.],\n       [ 0.,  7.]])\n```\n\n[`matmul(a, b[, out])`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul)\n\nMatrix product of two arrays.\n\nThe behavior depends on the arguments in the following way.\n\n- å¦‚æœéƒ½æ˜¯äºŒç»´æ•°ç»„æ—¶ they are multiplied like conventional matrices.\n- å¦‚æœaæ˜¯ä¸€ç»´æ•°ç»„, it is promoted to a matrix by prepending a 1 to its dimensions. After matrix multiplication the prepended 1 is removed.\n- å¦‚æœbæ˜¯ä¸€ç»´æ•°ç»„, it is promoted to a matrix by appending a 1 to its dimensions. After matrix multiplication the appended 1 is removed.\n- ~~If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.~~\n- a å’Œ béƒ½ä¸èƒ½æ˜¯æ ‡é‡(scalar)\n\nä¸¤ä¸ªäºŒç»´æ•°ç»„\n\n```python\n>>> a = [[1, 0], [0, 1]]\n>>> b = [[4, 1], [2, 2]]\n>>> np.matmul(a, b)\narray([[4, 1],\n       [2, 2]])\n\n\n```\n\näºŒç»´æ•°ç»„ä¸ä¸€ç»´æ•°ç»„\n\n```python\n>>> a = [[1, 0], [0, 1]]\n>>> b = [1, 2]\n>>> np.matmul(a, b)\narray([1, 2])\n>>> np.matmul(b, a)\narray([1, 2])\n```\n\n[`dot(a, b[, out])`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot)\n\nè¯¥å‡½æ•°å¯ä»¥æ˜¯innerï¼Œ matmulï¼Œ multiplyä¸‰ä¸ªå‡½æ•°çš„ç»“åˆä½“ã€‚\n\n- å¦‚æœaå’Œbæœ‰ä¸€ä¸ªæ˜¯ä¸€ç»´æ•°ç»„æ—¶ï¼Œç­‰ä»·äº`inner(a, b)`ã€‚\n\n- å¦‚æœaå’Œbéƒ½æ˜¯äºŒç»´æ•°ç»„æ—¶ï¼Œç­‰ä»·äº`matmul(a, b)`ã€‚\n- å¦‚æœaå’Œbæœ‰ä¸€ä¸ªæ˜¯æ ‡é‡æ—¶ï¼Œç­‰ä»·äº`multiply(a, b)`ã€‚\n- ~~If *a* is an N-D array and *b* is an M-D array (where `M>=2`), it is a sum product over the last axis of *a* and the second-to-last axis of *b*~~\n\n```python\n>>> x = np.arange(12).reshape((3,4))\n>>> a = np.arange(4)\n>>> b = np.arange(12, 24).reshape((3,4))\n>>> c = 10\n>>> np.dot(x, a)\narray([14, 38, 62])\n>>> np.inner(x, a)\narray([14, 38, 62])\n>>> \n>>> np.dot(x, b.T)\narray([[ 86, 110, 134],\n       [302, 390, 478],\n       [518, 670, 822]])\n>>> np.matmul(x, b.T)\narray([[ 86, 110, 134],\n       [302, 390, 478],\n       [518, 670, 822]])\n>>> \n>>> np.dot(x, c)\narray([[  0,  10,  20,  30],\n       [ 40,  50,  60,  70],\n       [ 80,  90, 100, 110]])\n>>> np.multiply(x, c)\narray([[  0,  10,  20,  30],\n       [ 40,  50,  60,  70],\n       [ 80,  90, 100, 110]])\n```\n\n[`vdot(a, b)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vdot.html#numpy.vdot)\n\nè®¡ç®—ä¸¤ä¸ªä¸€ç»´å‘é‡çš„ å†…ç§¯ ã€‚ å¦‚æœå‚æ•°ä¼ çš„æ˜¯å¤šç»´æ•°ç»„ï¼Œåˆ™å°†å…¶ `flatten` åè®¡ç®—å†…ç§¯\n\n```python\n>>> x = np.arange(1, 7).reshape(2, 3)\n>>> y = np.arange(2, 8).reshape(2, 3)\n>>> np.vdot(x, y) == np.vdot(x.flatten(), y.flatten())\nTrue\n```\n\n[`outer(a, b[, out])`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html#numpy.outer)\n\nä¸€ç»´æ•°ç»„ä¸ä¸€ç»´æ•°ç»„çš„å¤–ç§¯\n\n```python\n>>> m = np.arange(1, 4)\n>>> n = np.arange(3, 6)\n>>> np.outer(m, n)\narray([[ 3,  4,  5],\n       [ 6,  8, 10],\n       [ 9, 12, 15]])\n```\n\n\n\nå‚è€ƒèµ„æ–™\n\nhttps://docs.scipy.org/doc/numpy/reference/routines.linalg.html","tags":["Python"],"categories":["Python"]},{"title":"broadcasting in numpy","url":"%2Fblog%2Fbroadcasting-in-numpy.html","content":"\nå¹¿æ’­ï¼ˆbroadcastingï¼‰æ˜¯numpyæ•°ç»„çš„é‡è¦æ¦‚å¿µï¼Œå¯¹äºå®ƒçš„ç†è§£æœ‰åˆ©äºæ›´æ·±å…¥çš„ç†è§£ndarrayæ•°ç»„çš„è®¡ç®—é€»è¾‘ã€‚Numpyä¸­æœ‰ä¸€ç§é‡è¦çš„å‡½æ•°å«åš `universal function (ufunc)`  ï¼Œä¸€ä¸ª `ufunc` ä»¥ä¸¤ä¸ª `ndarray` ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ª`ndarray` ï¼Œ`ufunc` æŒ‰ç…§element-by-elementæ–¹å¼æ“ä½œå‚æ•°ä¸­çš„ä¸¤ä¸ª `ndarray` ï¼Œå¹¶è¿”å›ç»“æœã€‚å¯¹äºä¸€ä¸ª`ufun` `np.add` æœ‰å¦‚ä¸‹ä¾‹å­:\n\n```python\n>>> import numpy as np\n>>> arr1 = np.arange(24).reshape(2,3,4)\n>>> arr2 = np.arange(24).reshape(2,3,4)\n>>> np.add(arr1, arr2)\narray([[[ 0,  2,  4,  6],\n        [ 8, 10, 12, 14],\n        [16, 18, 20, 22]],\n\n       [[24, 26, 28, 30],\n        [32, 34, 36, 38],\n        [40, 42, 44, 46]]])\n```\n\n`np.add` å°† arr1 å’Œ arr2ä¸­ç›¸åŒä½ç½®çš„å…ƒç´ ç›¸åŠ å¾—åˆ°ä¸€ä¸ª`ndarray` ç±»å‹çš„è€Œç»“æœã€‚\n\né‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œå¦‚æœåšä¸ºè¾“å…¥å‚æ•°çš„ä¸¤ä¸ª ndarray çš„ å…·æœ‰ä¸åŒçš„shapeæ€ä¹ˆåŠå‘¢ï¼Ÿå¹¿æ’­å°±æ˜¯ç”¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜çš„ã€‚\n\n# é—®é¢˜åˆ†æ\n\né¦–å…ˆæˆ‘ä»¬é€šè¿‡ä¸‹é¢çš„ä¾‹å­æ¥åˆ†æä¸€ä¸‹ä¸¤ä¸ª ndarray æœ‰å“ªäº›ä¸­ä¸åŒshapeçš„å‚æ•°ï¼Ÿ\n\nå‡è®¾ä¸€ä¸ªå‚æ•°å›ºå®šä¸º `arr.shape`ï¼š(5, 6)\n\nå¦å¤–ä¸€ä¸ªå‚æ•°ä¸º\n\n- `a.shape` is (5,1)  \n- `b.shape` is (1,6) \n- `c.shape` is (6,) \n- `d.shape` is ()  ï¼šä¸€ä¸ªæ ‡é‡(scalar)\n- `e.shape` is (5,) \n- f å¦å¤–è¿˜æœ‰ä¸€ç§æƒ…å†µï¼šä¸¤ä¸ªæ•°ç»„åˆ†åˆ«ä¸º m.shape(5, 1), n.shape(1,6)ï¼Œ æ±‚m å’Œn çš„ufuncç»“æœã€‚\n- `g.shape` is (4, 3)\n- `h.shape` is (4, 6)\n\nç†è®ºä¸Šï¼Œå¹¿æ’­å¯ä»¥è§£å†³ä»¥ä¸Šæ¯ç§æƒ…å†µï¼Œä½†æ˜¯ï¼Œæˆ‘ä»¬å¸¸è§çš„æ˜¯ aã€bã€cã€d å››ç§ï¼Œeã€f å¶å°”ä¼šé‡åˆ°ï¼Œgã€håŸºæœ¬é‡ä¸åˆ°ã€‚\n\n# å¹¿æ’­çš„æœºåˆ¶\n\nä¸‹é¢ä¸º[Broadcasting](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#broadcasting)å¯¹å¹¿æ’­æœºåˆ¶çš„æè¿°ï¼š\n\n> 1. All input arrays with `ndim`smaller than the input array of largest `ndim`, have 1â€™s **prepended** to their shapes. ï¼ˆå¦‚æœä¸äººä¸ºæŒ‡å®šï¼Œé»˜è®¤æ˜¯åœ¨shapeçš„æœ€å‰é¢å¢åŠ è‹¥å¹²ä¸ª 1ï¼Œä½¿å¾—ä¸¤ä¸ªarrayçš„ ndimå±æ€§ç›¸ç­‰ï¼‰\n> 2. The size in each dimension of the output shape is the maximum of all the input sizes in that dimension.\n> 3. An input can be used in the calculation if its size in a particular dimension either matches the output size in that dimension, or has value exactly 1.\n> 4. If an input has a dimension size of 1 in its shape, the first data entry in that dimension will be used for all calculations along that dimension. In other words, the stepping machinery of the `ufunc` will simply not step along that dimension (the stride will be 0 for that dimension).\n\næŒ‰ç…§ä¸Šè¿°æœºåˆ¶ï¼Œå¯ä»¥å°†`a-e`5ç§æƒ…å†µå¯¹åº”çš„å¹¿æ’­æœºåˆ¶å¤„ç†æµç¨‹æ•´ç†æˆå¦‚ä¸‹å›¾æ‰€ç¤ºçš„è¿‡ç¨‹ï¼šå…¶ä¸­`shape_arr` æ˜¯ä¸€ä¸ªå’Œarrçš„shapeç›¸åŒçš„æ•°ç»„ã€‚è¯¥æµç¨‹ä¸­å˜åŒ–çš„æ˜¯ä»–ä»¬çš„shape\n\n![](broadcasting-in-numpy/a2e.png)\n\né—®é¢˜ f ä¸­ m å’Œ n çš„shapeå˜åŒ–æµç¨‹æ˜¯è¿™æ ·çš„:\n\n![](broadcasting-in-numpy/mn.png)\n\n**éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼šå¹¿æ’­æœºåˆ¶æ˜¯æˆ‘ä»¬ç†è§£Numpyçš„ä¸€ç§é€»è¾‘æ¨¡å‹ï¼Œåœ¨å®é™…çš„æ•°æ®è®¡ç®—é˜¶æ®µæ˜¯ä¸ä¼šæ²¿ç€è½´é•¿åº¦ä¸º1çš„æ–¹å‘å°†æ•°ç»„å¤åˆ¶è‹¥å¹²ä»½çš„ã€‚**\n\n# ä¸¾ä¾‹\n\nä¸‹é¢ä»¥ `ufunc` å‡½æ•° `np.add` ä¸ºä¾‹ä»‹ç»ä»¥ä¸Šæ¯ç§æƒ…å†µæ•°æ®å¤„ç†æµç¨‹ã€‚\n\næ•°æ®å‡†å¤‡ï¼š\n\n```python\n>>> arr\narray([[ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.]])\n```\n\n**æƒ…å†µa & æƒ…å†µb**\n\n```python\n>>> a = np.arange(5).reshape(5, 1)\n>>> a\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n>>> np.add(arr, a)\narray([[ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 2.,  2.,  2.,  2.,  2.,  2.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.],\n       [ 4.,  4.,  4.,  4.,  4.,  4.],\n       [ 5.,  5.,  5.,  5.,  5.,  5.]])\n\n>>> b = np.arange(6).reshape(1, 6)\n>>> b\narray([[0, 1, 2, 3, 4, 5]])\n>>> np.add(arr, b)\narray([[ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.]])\n```\n\n`np.add(arr, a)` çš„è®¡ç®—è¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºå¦‚ä¸‹è¿‡ç¨‹ï¼š\n\n1. å°†açš„ç¬¬ä¸€åˆ—æ•°æ®æ²¿ç€è½´1å¤åˆ¶ï¼Œå¾—åˆ°å¦‚ä¸‹æ•°ç»„:ï¼ˆå…¶å®åªæœ‰è¿™ä¸€æ­¥èƒ½ç®—çš„æ˜¯å¹¿æ’­ï¼‰\n\n   ```\n   array([[0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1],\n          [2, 2, 2, 2, 2, 2],\n          [3, 3, 3, 3, 3, 3],\n          [4, 4, 4, 4, 4, 4]])       \n   ```\n\n2. å°†è¯¥æ•°ç»„ä¸arrç›¸åŠ \n\n`np.add(arr, b)` çš„è®¡ç®—è¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºå¦‚ä¸‹è¿‡ç¨‹ï¼š\n\n1. å°†açš„ç¬¬ä¸€åˆ—æ•°æ®æ²¿ç€è½´1å¤åˆ¶ï¼Œå¾—åˆ°å¦‚ä¸‹æ•°ç»„:ï¼ˆå…¶å®åªæœ‰è¿™ä¸€æ­¥èƒ½ç®—çš„æ˜¯å¹¿æ’­ï¼‰\n\n   ```python\n   array([[0, 1, 2, 3, 4, 5],\n          [0, 1, 2, 3, 4, 5],\n          [0, 1, 2, 3, 4, 5],\n          [0, 1, 2, 3, 4, 5],\n          [0, 1, 2, 3, 4, 5],\n          [0, 1, 2, 3, 4, 5]])  \n   ```\n\n2. å°†è¯¥æ•°ç»„ä¸arrç›¸åŠ \n\n**æƒ…å†µc**\n\n```python\n>>> c = np.arange(6).reshape(6)\n>>> c\narray([0, 1, 2, 3, 4, 5])\n>>> np.add(arr, c)\narray([[ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.],\n       [ 1.,  2.,  3.,  4.,  5.,  6.]])\n```\n\nè¯¥è¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºå¦‚ä¸‹è¿‡ç¨‹ï¼š\n\n1. å…ˆå°†cè½¬æ¢æˆæƒ…å†µbï¼Œc.shape is (6,) ï¼Œåœ¨shapeçš„æœ€å‰é¢æ·»åŠ ä¸ª '1' ä½¿å¾— c.shape is (1, 6)ã€‚å³ä½¿ ` c.ndim == arr.ndim`\n2. æŒ‰ç…§æƒ…å†µbè¿›è¡Œè®¡ç®—\n\n**æƒ…å†µd**\n\n```python\n>>> d = 5\n>>> d\n5\n>>> np.add(arr, d)\narray([[ 6.,  6.,  6.,  6.,  6.,  6.],\n       [ 6.,  6.,  6.,  6.,  6.,  6.],\n       [ 6.,  6.,  6.,  6.,  6.,  6.],\n       [ 6.,  6.,  6.,  6.,  6.,  6.],\n       [ 6.,  6.,  6.,  6.,  6.,  6.]])\n```\n\nè¯¥è¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºå¦‚ä¸‹è¿‡ç¨‹ï¼š\n\n1. d.shape is (,) ï¼Œåœ¨shapeçš„æœ€å‰é¢æ·»åŠ ä¸¤ä¸ª '1' ä½¿å¾— d.shape is (1, 1)ã€‚å³ä½¿ ` c.ndim == arr.ndim`\n\n2. åœ¨è½´1ï¼Œå’Œè½´0ä¸Šåˆ†åˆ«å¤åˆ¶ï¼Œå¾—åˆ°ï¼š\n\n   ```python\n   array([[5, 5, 5, 5, 5, 5],\n          [5, 5, 5, 5, 5, 5],\n          [5, 5, 5, 5, 5, 5],\n          [5, 5, 5, 5, 5, 5],\n          [5, 5, 5, 5, 5, 5]])\n   ```\n\n3. å°†è¯¥æ•°ç»„ä¸arrç›¸åŠ \n\n**æƒ…å†µe**\n\n```python\n>>> e = np.arange(5).reshape(5)\n>>> e\narray([0, 1, 2, 3, 4])\n>>> np.add(arr, e)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: operands could not be broadcast together with shapes (5,6) (5,)\n```\n\nå¦‚ä¸Šæ‰€ç¤ºï¼Œç›´æ¥æ“ä½œarrå’Œeæ˜¯é”™è¯¯çš„ã€‚ç”±äºå¹¿æ’­æœºåˆ¶é»˜è®¤æ˜¯åœ¨e.shapeçš„æœ€å‰é¢åŠ ä¸€ä¸ªâ€˜1â€™ï¼Œè¿™æ · arr.shape is (5, 6)ï¼Œe.shape is (1, 5)ï¼Œä½†æ˜¯ æœ€åä¸€ä¸ªè½´çš„å€¼ä¸æƒ³ç­‰ï¼Œæ‰€ä»¥ä¸èƒ½è®¡ç®—ã€‚\n\nå¦‚æœå®é™…æƒ…å†µå…è®¸çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥eå½“åšä¸€ä¸ªåˆ—å‘é‡ï¼Œå³ï¼Œshape(5, 1)ã€‚å¯ä»¥é€šè¿‡å¦‚ä¸‹ä¸¤ç§æ–¹æ³•å®ç°ï¼šé€šè¿‡ reshapeå‡½æ•° æˆ–è€… np.newaxis \n\n*æ¨èä½¿ç”¨np.newaxis*\n\n```python\n>>> e_reshape = e.reshape(5, 1)\n>>> e_reshape\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n>>> np.add(arr, e_reshape)\narray([[ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 2.,  2.,  2.,  2.,  2.,  2.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.],\n       [ 4.,  4.,  4.,  4.,  4.,  4.],\n       [ 5.,  5.,  5.,  5.,  5.,  5.]])       \n       \n>>> e_newaxis = e[:, np.newaxis]\n>>> e_newaxis\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n>>> np.add(arr, e_newaxis)\narray([[ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 2.,  2.,  2.,  2.,  2.,  2.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.],\n       [ 4.,  4.,  4.,  4.,  4.,  4.],\n       [ 5.,  5.,  5.,  5.,  5.,  5.]])\n```\n\n**æƒ…å†µf**\n\n```python\n>>> m = np.arange(5).reshape(5, 1)\n>>> m\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n>>> n = np.arange(6).reshape(1, 6)\n>>> n\narray([[0, 1, 2, 3, 4, 5]])\n>>> np.add(m, n)\narray([[0, 1, 2, 3, 4, 5],\n       [1, 2, 3, 4, 5, 6],\n       [2, 3, 4, 5, 6, 7],\n       [3, 4, 5, 6, 7, 8],\n       [4, 5, 6, 7, 8, 9]])\n```\n\nmå’Œnçš„ndimæ˜¯ç›¸ç­‰çš„ï¼Œä½†æ˜¯m.shape is (5, 1)ï¼Œ n.shape is (1, 6)ã€‚\n\n1. å¯¹äºè½´1 æ¥è¯´ï¼Œéœ€è¦æ²¿ç€è½´1 å°†må¤åˆ¶6æ¬¡å¾—åˆ°ï¼š\n\n```python\narray([[0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4, 4]])\n```\n\n2. å¯¹äºè½´0æ¥è¯´ï¼Œéœ€è¦å°†næ²¿ç€è½´0å¤åˆ¶5æ¬¡å¾—åˆ°ï¼š\n\n```python\narray([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]])\n```\n\n3. ç„¶åå°†ä»¥ä¸Šä¸¤ä¸ªæ•°ç»„ç›¸åŠ ã€‚\n\n# æ€»ç»“\n\nå¹¿æ’­å°±æ˜¯ç”¨äºæ“ä½œ ndimç›¸åŒçš„ä¸¤ä¸ªæ•°ç»„çš„ã€‚\n\nè¿™ä¸¤ä¸ªæ•°ç»„çš„shapeå€¼åªèƒ½æ˜¯ä¸¤ç§æƒ…å†µ(å³ï¼Œå¯¹åº”çš„è½´é•¿åº¦)ï¼š\n\n1. å¯¹åº”ä½ç½®çš„å€¼ç›¸ç­‰ï¼Œ\n2. å¯¹åº”ä½ç½®çš„å€¼ä¸ç›¸ç­‰ï¼Œä½†å…¶ä¸­ä¸€ä¸ªå¿…é¡»æ˜¯ 1ã€‚\n\nå¯¹äºndimä¸åŒçš„ä¸¤ä¸ªæ•°ç»„éœ€è¦åœ¨ æŸä¸ªæ•°ç»„çš„shapeä¸­åˆé€‚çš„ä½ç½®è¡¥ 1ï¼Œä½¿å¾— ndimæƒ³ç­‰:\n\n1. é»˜è®¤æ˜¯åœ¨shapeçš„æœ€å‰é¢è¡¥å……è‹¥å¹²ä¸ª 1ï¼Œ\n2. ä¹Ÿå¯ä»¥é€šè¿‡reshapeå‡½æ•°æˆ–è€…np.newaxis äººä¸ºæŒ‡å®šéœ€è¦è¡¥ 1 çš„è½´ã€‚\n\n\nã€Šåˆ©ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æã€‹ä¸­ç”¨ä¸€å¥è¯å®šä¹‰äº†å¹¿æ’­çš„åŸåˆ™\n\n>å¦‚æœä¸¤ä¸ªæ•°æ®çš„åç¼˜ç»´åº¦(trailing dimension, å³ä»æœ«å°¾å¼€å§‹ç®—èµ·çš„ç»´åº¦)çš„è½´é•¿åº¦ç›¸ç¬¦æˆ–è€…ä¸€æ–¹çš„é•¿åº¦æ˜¯1ï¼Œåˆ™è®¤ä¸ºä»–ä»¬æ˜¯å¹¿æ’­å…¼å®¹çš„ã€‚å¹¿æ’­ä¼šåœ¨ç¡®å®å’Œ(æˆ–)é•¿åº¦ä¸º1çš„ç»´åº¦ä¸Šè¿›è¡Œã€‚\n\n**å†æ¬¡å¼ºè°ƒä¸€ä¸‹ï¼šå¹¿æ’­æœºåˆ¶æ˜¯æˆ‘ä»¬ç†è§£Numpyçš„ä¸€ç§é€»è¾‘æ¨¡å‹ï¼Œåœ¨å®é™…çš„æ•°æ®è®¡ç®—é˜¶æ®µæ˜¯ä¸ä¼šæ²¿ç€è½´é•¿åº¦ä¸º1çš„æ–¹å‘å°†æ•°ç»„å¤åˆ¶è‹¥å¹²ä»½çš„ã€‚**\n\n# å‚è€ƒèµ„æ–™\n\n[Broadcasting](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#broadcasting)\n\n[ç”¨Pythonåšç§‘å­¦è®¡ç®—](http://old.sebug.net/paper/books/scipydoc/)\n\nåˆ©ç”¨pythonè¿›è¡Œæ•°æ®åˆ†æ","tags":["Python"],"categories":["Python"]},{"title":"ndarray in Numpy","url":"%2Fblog%2Fndarray-in-numpy.html","content":"\nndarray æ•°ç»„æ˜¯æˆ‘ä»¬ç”¨pythonè¿›è¡Œç§‘å­¦è®¡ç®—æ—¶å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼Œå¯¹å®ƒçš„æ·±å…¥äº†è§£æ˜¯éå¸¸å¿…è¦çš„ã€‚æœ¬æ–‡å›é¡¾ä¹‹å‰å­¦ä¹ çš„æœ‰å…³adarrayçš„çŸ¥è¯†ã€‚\n\n# å†…å­˜ç»“æ„\n\nndarray çš„å†…å­˜ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](ndarray-in-numpy/numpy_memory_struct.png)\n\n- dataï¼šæŒ‡å‘æ•°ç»„ä¸­å…ƒç´ çš„äºŒè¿›åˆ¶æ•°æ®å—ã€‚\n- dtype:å®šä¹‰äº†æ•°ç»„ä¸­å­˜æ”¾çš„å¯¹è±¡çš„æ•°æ®ç±»å‹ï¼Œé€šè¿‡å®ƒå¯ä»¥çŸ¥é“å¦‚ä½•å°†å…ƒç´ çš„äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºå¯ç”¨çš„å€¼ã€‚å¦‚ä¸Šå›¾æ¯32ä½è¡¨ç¤ºä¸€ä¸ªæœ‰ç”¨æ•°æ®\n- dim count: è¡¨ç¤ºæ•°ç»„ç»´æ•°ï¼Œä¸Šå›¾ä¸º2ç»´æ•°ç»„ (å¯¹åº”ndarrayçš„å±æ€§`ndim`)\n- dimmension: æ•°ç»„çš„å½¢çŠ¶ï¼Œ3Ã—3ç»™å‡ºæ•°ç»„çš„(å¯¹åº”ndarrayçš„å±æ€§`shape`)\n- strides: ä¿å­˜çš„æ˜¯å½“æ¯ä¸ªè½´çš„ä¸‹æ ‡å¢åŠ 1æ—¶ï¼Œæ•°æ®å­˜å‚¨åŒºä¸­çš„æŒ‡é’ˆæ‰€å¢åŠ çš„å­—èŠ‚æ•°ã€‚ä¾‹å¦‚å›¾ä¸­çš„stridesä¸º12,4ï¼Œå³ç¬¬0è½´çš„ä¸‹æ ‡å¢åŠ 1æ—¶ï¼Œæ•°æ®çš„åœ°å€å¢åŠ 12ä¸ªå­—èŠ‚ï¼šå³a[1,0]çš„åœ°å€æ¯”a[0,0]çš„åœ°å€è¦é«˜12ä¸ªå­—èŠ‚ï¼Œæ­£å¥½æ˜¯3ä¸ªå•ç²¾åº¦æµ®ç‚¹æ•°çš„æ€»å­—èŠ‚æ•°ï¼›ç¬¬1è½´ä¸‹æ ‡å¢åŠ 1æ—¶ï¼Œæ•°æ®çš„åœ°å€å¢åŠ 4ä¸ªå­—èŠ‚ï¼Œæ­£å¥½æ˜¯å•ç²¾åº¦æµ®ç‚¹æ•°çš„å­—èŠ‚æ•°ã€‚\n\n## dtype\n\ndtypeï¼ˆæ•°æ®ç±»å‹ï¼‰æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å¯¹è±¡ï¼Œå®ƒå«æœ‰ndarrayå°†ä¸€å—å†…å­˜è§£é‡Šä¸ºç‰¹å®šæ•°æ®ç±»å‹æ‰€éœ€çš„ä¿¡æ¯ã€‚\n\nå¯ä»¥é€šè¿‡ndarrayçš„astypeæ–¹æ³•æ˜¾å¼åœ°è½¬æ¢å…¶dtypeã€‚\n\n```python\n>>> import numpy as np\n>>> arr = np.array([1, 2, 3], dtype=np.int32)\n>>> arr\narray([1, 2, 3], dtype=int32)\n>>> arr.dtype\ndtype('int32')\n>>> float_arr = arr.astype(np.float128)\n>>> float_arr.dtype\ndtype('float128')\n```\n\nnumpy æœ‰å¤šç§é¢„å®šä¹‰çš„dtypeï¼ˆç”¨çš„æ¯”è¾ƒå¤šçš„è¿˜æ˜¯`np.float64`ï¼‰ï¼Œéœ€è¦çš„æ—¶å€™å¯ä»¥è‡ªè¡ŒæŸ¥æ‰¾\n\nè¿˜å¯ä»¥è‡ªå®šä¹‰æ•°ç»„çš„dtypeã€‚\n\n## strideså†è¯´æ˜\n\næœ‰æ—¶ndarrayä¸­çš„æ•°æ®å¹¶ä¸ä¸€å®šéƒ½æ˜¯è¿ç»­å‚¨å­˜çš„ï¼Œé€šè¿‡ä¸‹æ ‡èŒƒå›´å¾—åˆ°æ–°çš„æ•°ç»„æ˜¯åŸå§‹æ•°ç»„çš„è§†å›¾(ç±»ä¼¼äºå¼•ç”¨)ï¼Œå³å®ƒå’ŒåŸå§‹è§†å›¾å…±äº«æ•°æ®å­˜å‚¨åŒºåŸŸï¼Œ å¯¹äºæ–°æ•°ç»„æ¥è¯´ï¼Œå…¶ä¸­çš„æ•°æ®ä¸ä¸€å®šæ˜¯è¿ç»­å­˜å‚¨çš„ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\n>>> import numpy as np\n>>> a = np.array([[0,1,2],[3,4,5],[6,7,8]], dtype=np.float32)\n>>> a\narray([[ 0.,  1.,  2.],\n       [ 3.,  4.,  5.],\n       [ 6.,  7.,  8.]], dtype=float32)\n>>> a.strides\n(12, 4)\n>>> b = a[::2,::2]\n>>> b\narray([[ 0.,  2.],\n       [ 6.,  8.]], dtype=float32)\n>>> b.strides\n(24, 8)\n```\n\nç”±äºæ•°ç»„bå’Œæ•°ç»„aå…±äº«æ•°æ®å­˜å‚¨åŒºï¼Œè€Œbä¸­çš„ç¬¬0è½´å’Œç¬¬1è½´éƒ½æ˜¯æ•°ç»„aä¸­éš”ä¸€ä¸ªå…ƒç´ å–ä¸€ä¸ªï¼Œå› æ­¤æ•°ç»„bçš„strideså˜æˆäº†24,8ï¼Œæ­£å¥½éƒ½æ˜¯æ•°ç»„açš„ä¸¤å€ã€‚\n\nå…ƒç´ åœ¨æ•°æ®å­˜å‚¨åŒºä¸­çš„æ’åˆ—æ ¼å¼æœ‰ä¸¤ç§ï¼šCè¯­è¨€æ ¼å¼å’ŒFortanè¯­è¨€æ ¼å¼ã€‚åœ¨Cè¯­è¨€ä¸­ï¼Œå¤šç»´æ•°ç»„çš„ç¬¬0è½´æ˜¯æœ€ä¸Šä½çš„ï¼Œå³ç¬¬0è½´çš„ä¸‹æ ‡å¢åŠ 1æ—¶ï¼Œå…ƒç´ çš„åœ°å€å¢åŠ çš„å­—èŠ‚æ•°æœ€å¤šï¼›è€ŒFortanè¯­è¨€çš„å¤šç»´æ•°ç»„çš„ç¬¬0è½´æ˜¯æœ€ä¸‹ä½çš„ï¼Œå³ç¬¬0è½´çš„ä¸‹æ ‡å¢åŠ 1æ—¶ï¼Œåœ°å€åªå¢åŠ ä¸€ä¸ªå…ƒç´ çš„å­—èŠ‚æ•°ã€‚åœ¨NumPyä¸­ï¼Œå…ƒç´ åœ¨å†…å­˜ä¸­çš„æ’åˆ—ç¼ºçœæ˜¯ä»¥Cè¯­è¨€æ ¼å¼å­˜å‚¨çš„ï¼Œå¦‚æœä½ å¸Œæœ›æ”¹ä¸ºFortanæ ¼å¼çš„è¯ï¼Œåªéœ€è¦ç»™æ•°ç»„ä¼ é€’order=\"F\"å‚æ•°ï¼š\n\n```python\n>>> c = np.array([[0,1,2],[3,4,5],[6,7,8]], dtype=np.float32, order=\"C\")\n>>> c.strides\n(12, 4)\n>>> f = np.array([[0,1,2],[3,4,5],[6,7,8]], dtype=np.float32, order=\"F\")\n>>> f.strides\n(4, 12)\n```\n\n# ndarrayçš„åˆ›å»º\n\nåˆ›å»ºndarrayå¯¹è±¡çš„å¸¸ç”¨å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\n np.array\n np.asarray\n np.arange  # é€šè¿‡å¼€å§‹å€¼ã€ç»ˆå€¼å’Œæ­¥é•¿ åˆ›å»ºä¸€ç»´æ•°ç»„\n np.linspace  # é€šè¿‡å¼€å§‹å€¼ã€ç»ˆå€¼å’Œå…ƒç´ ä¸ªæ•°åˆ›å»ºä¸€ç»´æ•°ç»„\n np.logspace  # äº§ç”Ÿç­‰æ¯”æ•°åˆ—\n np.ones\n np.ones_like\n np.zeros\n np.zeros_like\n np.empty\n np.empty_like\n np.eye\n np.identity\n np.fromfunction  # æ ¹æ®ä¸‹æ ‡ç”Ÿæˆæ•°ç»„\n```\n\nç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š\n\n```python\n>>> np.arange(0,1,0.1)\narray([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])\n>>> np.linspace(0, 1, 12)\narray([ 0.        ,  0.09090909,  0.18181818,  0.27272727,  0.36363636,\n        0.45454545,  0.54545455,  0.63636364,  0.72727273,  0.81818182,\n        0.90909091,  1.        ])\n>>> np.logspace(0, 2, 20)\narray([   1.        ,    1.27427499,    1.62377674,    2.06913808,\n          2.6366509 ,    3.35981829,    4.2813324 ,    5.45559478,\n          6.95192796,    8.8586679 ,   11.28837892,   14.38449888,\n         18.32980711,   23.35721469,   29.76351442,   37.92690191,\n         48.32930239,   61.58482111,   78.47599704,  100.        ])\n\n>>> def func(i):\n...     return i%4+1\n... \n>>> np.fromfunction(func, (10,))\narray([ 1.,  2.,  3.,  4.,  1.,  2.,  3.,  4.,  1.,  2.])\n\n>>> def func2(i, j):\n...     return (i+1) * (j+1)\n... \n>>> np.fromfunction(func2, (4, 4))\narray([[  1.,   2.,   3.,   4.],\n       [  2.,   4.,   6.,   8.],\n       [  3.,   6.,   9.,  12.],\n       [  4.,   8.,  12.,  16.]])\n```\n\n\n\n# ndarrayå…ƒç´ çš„ç´¢å¼•\n\nndarrayæä¾›äº†ä¸‰ç§ç´¢å¼•æ–¹æ³•ã€‚\n\n## åˆ‡ç‰‡ç´¢å¼•\n\né€šè¿‡ä¸‹æ ‡èŒƒå›´è·å–çš„æ–°çš„æ•°ç»„æ˜¯åŸå§‹æ•°ç»„çš„ä¸€ä¸ªè§†å›¾ã€‚**å®ƒä¸åŸå§‹æ•°ç»„å…±äº«åŒä¸€å—æ•°æ®ç©ºé—´**ï¼Œæ‰€ä»¥å¯¹ ç´¢å¼•æ•°ç»„çš„ä¿®æ”¹ å°±æ˜¯å¯¹åŸå§‹æ•°ç»„çš„ä¿®æ”¹ã€‚\n\nå¦‚æœä½ æƒ³è¦å¾—åˆ°çš„æ˜¯ndarrayåˆ‡ç‰‡çš„ä¸€ä»½å‰¯æœ¬è€Œéè§†å›¾ï¼Œå°±éœ€è¦æ˜¾å¼åœ°è¿›è¡Œå¤åˆ¶æ“ä½œï¼Œä¾‹å¦‚arr[5:8].copy()ã€‚\n\n```python\n>>> a = np.arange(10)\n>>> a[5]    # ç”¨æ•´æ•°ä½œä¸ºä¸‹æ ‡å¯ä»¥è·å–æ•°ç»„ä¸­çš„æŸä¸ªå…ƒç´ \n5\n>>> a[3:5]  # ç”¨èŒƒå›´ä½œä¸ºä¸‹æ ‡è·å–æ•°ç»„çš„ä¸€ä¸ªåˆ‡ç‰‡ï¼ŒåŒ…æ‹¬a[3]ä¸åŒ…æ‹¬a[5]\narray([3, 4])\n>>> a[:5]   # çœç•¥å¼€å§‹ä¸‹æ ‡ï¼Œè¡¨ç¤ºä»a[0]å¼€å§‹\narray([0, 1, 2, 3, 4])\n>>> a[:-1]  # ä¸‹æ ‡å¯ä»¥ä½¿ç”¨è´Ÿæ•°ï¼Œè¡¨ç¤ºä»æ•°ç»„åå¾€å‰æ•°\narray([0, 1, 2, 3, 4, 5, 6, 7, 8])\n>>> a[2:4] = 100,101    # ä¸‹æ ‡è¿˜å¯ä»¥ç”¨æ¥ä¿®æ”¹å…ƒç´ çš„å€¼\n>>> a\narray([  0,   1, 100, 101,   4,   5,   6,   7,   8,   9])\n>>> a[1:-1:2]   # èŒƒå›´ä¸­çš„ç¬¬ä¸‰ä¸ªå‚æ•°è¡¨ç¤ºæ­¥é•¿ï¼Œ2è¡¨ç¤ºéš”ä¸€ä¸ªå…ƒç´ å–ä¸€ä¸ªå…ƒç´ \narray([  1, 101,   5,   7])\n>>> a[::-1] # çœç•¥èŒƒå›´çš„å¼€å§‹ä¸‹æ ‡å’Œç»“æŸä¸‹æ ‡ï¼Œæ­¥é•¿ä¸º-1ï¼Œæ•´ä¸ªæ•°ç»„å¤´å°¾é¢ å€’\narray([  9,   8,   7,   6,   5,   4, 101, 100,   1,   0])\n>>> a[5:1:-2] # æ­¥é•¿ä¸ºè´Ÿæ•°æ—¶ï¼Œå¼€å§‹ä¸‹æ ‡å¿…é¡»å¤§äºç»“æŸä¸‹æ ‡\narray([  5, 101])\n```\n\n## èŠ±å¼ç´¢å¼•(æ•´æ•°åºåˆ—)\n\nå½“ä½¿ç”¨æ•´æ•°åºåˆ—å¯¹æ•°ç»„å…ƒç´ è¿›è¡Œå­˜å–æ—¶ï¼Œå°†ä½¿ç”¨æ•´æ•°åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ä½œä¸ºä¸‹æ ‡ï¼Œæ•´æ•°åºåˆ—å¯ä»¥æ˜¯åˆ—è¡¨æˆ–è€…æ•°ç»„ã€‚\n\n**ä½¿ç”¨æ•´æ•°åºåˆ—ä½œä¸ºä¸‹æ ‡è·å¾—çš„æ•°ç»„ä¸å’ŒåŸå§‹æ•°ç»„å…±äº«æ•°æ®ç©ºé—´ã€‚**\n\n```python\n>>> x = np.arange(10,1,-1)\n>>> x\narray([10,  9,  8,  7,  6,  5,  4,  3,  2])\n>>> x[[3, 3, 1, 8]] # è·å–xä¸­çš„ä¸‹æ ‡ä¸º3, 3, 1, 8çš„4ä¸ªå…ƒç´ ï¼Œç»„æˆä¸€ä¸ªæ–°çš„æ•°ç»„\narray([7, 7, 9, 2])\n>>> b = x[np.array([3,3,-3,8])]  #ä¸‹æ ‡å¯ä»¥æ˜¯è´Ÿæ•°\n>>> b[2] = 100\n>>> b\narray([  7,   7, 100,   2])\n>>> x   # ç”±äºbå’Œxä¸å…±äº«æ•°æ®ç©ºé—´ï¼Œå› æ­¤xä¸­çš„å€¼å¹¶æ²¡æœ‰æ”¹å˜\narray([10,  9,  8,  7,  6,  5,  4,  3,  2])\n>>> x[[3,5,1]] = -1, -2, -3 # æ•´æ•°åºåˆ—ä¸‹æ ‡ä¹Ÿå¯ä»¥ç”¨æ¥ä¿®æ”¹å…ƒç´ çš„å€¼\n>>> x\narray([10, -3,  8, -1,  6, -2,  4,  3,  2])\n```\n\n## å¸ƒå°”ç´¢å¼•(å¸ƒå°”æ•°ç»„ & å¸ƒå°”åˆ—è¡¨)\n\nå½“ä½¿ç”¨å¸ƒå°”æ•°ç»„bä½œä¸ºä¸‹æ ‡å­˜å–æ•°ç»„xä¸­çš„å…ƒç´ æ—¶ï¼Œå°†æ”¶é›†æ•°ç»„xä¸­æ‰€æœ‰åœ¨æ•°ç»„bä¸­å¯¹åº”ä¸‹æ ‡ä¸ºTrueçš„å…ƒç´ ã€‚\n\n**ä½¿ç”¨å¸ƒå°”æ•°ç»„ä½œä¸ºä¸‹æ ‡è·å¾—çš„æ•°ç»„å’ŒåŸå§‹æ•°ç»„å…±äº«æ•°æ®ç©ºé—´ï¼Œæ³¨æ„è¿™ç§æ–¹å¼åªå¯¹åº”äºå¸ƒå°”æ•°ç»„**\n\n**ä¸èƒ½ä½¿ç”¨å¸ƒå°”åˆ—è¡¨ï¼Œ å¦åˆ™ä¼šæŠŠTrueå½“ä½œ1, Falseå½“ä½œ0ï¼ŒæŒ‰ç…§æ•´æ•°åºåˆ—æ–¹å¼è·å–åŸå§‹ä¸­çš„å…ƒç´ ï¼Œä¸å…±äº«å†…å­˜**\n\n```python\n>>> x = np.arange(5,0,-1)\n>>> x\narray([5, 4, 3, 2, 1])\n>>> x[np.array([True, False, True, False, False])] # å¸ƒå°”æ•°ç»„ä¸­ä¸‹æ ‡ä¸º0ï¼Œ2çš„å…ƒç´ ä¸ºTrueï¼Œå› æ­¤è·å–xä¸­ä¸‹æ ‡ä¸º0,2çš„å…ƒç´ \narray([5, 3])\n>>> x[[True, False, True, False, False]] # å¦‚æœæ˜¯å¸ƒå°”åˆ—è¡¨ï¼Œåˆ™æŠŠTrueå½“ä½œ1, Falseå½“ä½œ0ï¼ŒæŒ‰ç…§æ•´æ•°åºåˆ—æ–¹å¼è·å–xä¸­çš„å…ƒç´ \narray([5, 3])\n>>> x[np.array([True, False, True, True])] # å¸ƒå°”æ•°ç»„çš„é•¿åº¦ä¸å¤Ÿæ—¶ï¼Œä¸å¤Ÿçš„éƒ¨åˆ†éƒ½å½“ä½œFalse\n__main__:1: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 5 but corresponding boolean dimension is 4\narray([5, 3, 2])\n>>> x[np.array([True, False, True, True])] = -1, -2, -3  # å¸ƒå°”æ•°ç»„ä¸‹æ ‡ä¹Ÿå¯ä»¥ç”¨æ¥ä¿®æ”¹å…ƒç´ \n>>> x\narray([-1,  4, -2, -3,  1])\n```\n\nå¸ƒå°”æ•°ç»„ ç´¢å¼•å¯ä»¥ä½¿ç”¨é€»è¾‘è¿ç®—ï¼Œ åˆ†åˆ«ä¸º `|, &, ~ ` (æˆ– ä¸ é)ã€‚*ä¸èƒ½ä½¿ç”¨ and or not*\n\n```python\n>>> names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe']) # ä¸ƒä¸ªäºº ï¼Œæœ‰é‡å¤\n>>> data = np.random.randn(7, 4) # 7ä¸ªäººçš„æˆç»©ï¼Œæ¯è¡Œä¸€ä¸ªäºº\n>>> data[~(names == 'Bob')]   #  æŸ¥çœ‹é™¤äº† Bob ä¹‹å¤– å…¶ä»–å…­ä¸ªäººçš„æˆç»© ï¼Œ ç­‰ä»·äº data[(names != 'Bob')]\narray([[ 0.07820045, -1.71734855, -0.34533472,  0.13199966],\n       [ 1.44077541,  0.599357  ,  0.02741339, -0.27952545],\n       [ 0.40324184, -1.14696069, -2.63616404, -0.89460574],\n       [-1.07763886,  1.38956886, -0.51076578,  0.22787857],\n       [-0.56223972, -0.4285839 ,  1.00189764, -1.27962868]])\n>>> data[names == 'Bob']  # æŸ¥çœ‹ Bob çš„æˆç»©\narray([[-0.20510993,  1.16346024,  1.0559742 , -0.17387184],\n       [ 0.08864056,  0.49012893, -0.27613655, -0.89549472]])\n>>> mask = (names == 'Bob') | (names == 'Will')\n>>> data[mask]   # æŸ¥çœ‹ Bob å’Œ Will çš„æˆç»©\narray([[-0.20510993,  1.16346024,  1.0559742 , -0.17387184],\n       [ 1.44077541,  0.599357  ,  0.02741339, -0.27952545],\n       [ 0.08864056,  0.49012893, -0.27613655, -0.89549472],\n       [ 0.40324184, -1.14696069, -2.63616404, -0.89460574]])\n```\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[ç”¨Pythonåšç§‘å­¦è®¡ç®—](http://old.sebug.net/paper/books/scipydoc/)\n\nåˆ©ç”¨pythonè¿›è¡Œæ•°æ®åˆ†æ","tags":["Python"],"categories":["Python"]},{"title":"å‡è®¾æ£€éªŒ","url":"%2Fblog%2Fhypothesis-testing.html","content":"\nçœ‹æ–‡ç« æ˜¯æ¯æ¬¡é‡åˆ°å‡è®¾æ£€éªŒæ–¹é¢çš„å†…å®¹è€æ˜¯çŠ¯è¿·ç³Šï¼Œå¯¼è‡´æ¯æ¬¡éƒ½éœ€è¦é‡æ–°ç¿»ä¹¦æœ¬ã€‚æ‰€æœ‰æˆ‘å†³å®šå¹²è„†æŠŠå‡è®¾æ£€éªŒæƒ³ç®¡çš„å†…å®¹åšä¸€æ¬¡ç³»ç»Ÿæ€§çš„æ•´ç†ã€‚\n\n# å¼•ä¾‹\n\næŸè½¦é—´ç”¨ä¸€å°åŒ…è£…æœºåŒ…è£…è‘¡è„ç³–.è¢‹è£…ç³–çš„å‡€é‡æ˜¯ä¸€ä¸ªéšæœºå˜é‡,å®ƒæœä»æ­£æ€åˆ†å¸ƒå½“æœºå™¨æ­£å¸¸æ—¶,å…¶å‡å€¼ä¸º0.5kg,æ ‡å‡†å·®ä¸º0.015kg,æŸæ—¥å¼€å·¥åä¸ºæ£€éªŒåŒ…è£…æœºæ˜¯å¦æ­£å¸¸,éšæœºåœ°æŠ½å–å®ƒæ‰€åŒ…è£…çš„ç³–9è¢‹ç§°å¾—å‡€é‡ä¸º(kg)\n$$\n0.497, 0.506, 0.518, 0.524, 0.498, 0.511, 0.520, 0.515, 0.512\n$$\né—®æœºå™¨æ˜¯å¦æ­£å¸¸?\n\nä»¥$\\mu$, $\\sigma$åˆ†åˆ«è¡¨ç¤ºè¿™ä¸€å¤©è¢‹è£…ç³–çš„å‡€é‡æ€»ä½“$X$çš„å‡å€¼å’Œæ ‡å‡†å·®. $\\sigma=0.015$. äºæ˜¯$X \\sim N(\\mu,0.0152)$,è¿™é‡ŒæœªçŸ¥é—®é¢˜æ˜¯æ ¹æ®æ ·æœ¬å€¼æ¥åˆ¤æ–­$\\mu = 0.5$è¿˜æ˜¯$\\mu \\neq 0.5$.ä¸ºæ­¤æˆ‘ä»¬æå‡ºä¸¤ä¸ªç›¸äº’å¯¹ç«‹çš„å‡è®¾.\n$$\nH_0: \\mu = 0.5 \\\\\nH_1: \\mu \\neq 0.5\n$$\nç”±äºè¦æ£€éªŒçš„å‡è®¾æ¶‰åŠæœªçŸ¥çš„æ€»ä½“$X$çš„å‡å€¼$\\mu$, ä½¿ç”¨$\\mu$çš„æ— åç»Ÿè®¡é‡ $\\bar x$ æ›¿ä»£ã€‚å¦‚æœå‡è®¾$H_0$ä¸ºçœŸ, åˆ™è§‚å¯Ÿå€¼ä¸$\\mu_0=0.5$çš„åå·®$|\\bar x - \\mu_0|$ ä¸€èˆ¬ä¸åº”å¤ªå¤§. è‹¥ $|\\bar x - \\mu_0|$ è¿‡å¤§,æˆ‘ä»¬å°±æ€€ç–‘å‡è®¾$H_0$çš„æ­£ç¡®æ€§è€Œæ‹’ç»$H_0$ .\n\nå½“$H_0$ä¸ºçœŸæ—¶$\\frac{\\bar X - \\mu_0}{\\sigma / \\sqrt n} \\sim N(0, 1)$ .è€Œè¡¡é‡ $|\\bar x - \\mu_0|$ çš„å¤§å°å¯å½’ç»“ä¸ºè¡¡é‡ $\\frac{ | \\bar x - \\mu_0 | }{\\sigma / \\sqrt n}$ çš„å¤§å°. åŸºäºä¸Šé¢çš„æƒ³æ³•,æˆ‘ä»¬å¯é€‚å½“é€‰å®šä¸€æ­£æ•° $k$ . ä½¿å½“è§‚å¯Ÿå€¼æ»¡è¶³ $\\frac{ | \\bar x - \\mu_0 | }{\\sigma / \\sqrt n} \\ge k$  æ—¶å°±æ‹’ç»å‡è®¾$H_0$, åä¹‹,å°±æ¥å—å‡è®¾$H_0$ ã€‚$\\frac{ | \\bar x - \\mu_0 | }{\\sigma / \\sqrt n}$ å³ä¸º **æ£€éªŒç»Ÿè®¡é‡**ã€‚\n\nå½“æ£€éªŒç»Ÿè®¡é‡å–æŸä¸ªåŒºåŸŸ$C$ä¸­çš„å€¼æ—¶, æˆ‘ä»¬æ‹’ç»åŸå‡è®¾ $H_0$, åˆ™ç§°åŒºåŸŸ$C$ä¸º**æ‹’ç»åŸŸ**ï¼Œ åä¹‹åˆ™ä¸º**æ¥å—åŸŸ**ã€‚ æ‹’ç»åŸŸçš„è¾¹ç•Œç‚¹ç§°ä¸º**ä¸´ç•Œç‚¹**ã€‚æ­¤å¤„çš„$k$ å³æ˜¯ä¸´ç•Œç‚¹ã€‚\n\n# å‡è®¾æ£€éªŒ\n\n##  ä¸¤ç±»é”™è¯¯\n\nç”±äºæ£€éªŒæ³•åˆ™æ˜¯æ ¹æ®æ ·æœ¬ä½œå‡ºçš„, æ€»æœ‰å¯èƒ½ä½œå‡ºé”™è¯¯çš„å†³ç­–ã€‚ åœ¨å‡è®¾ $H_0$ å®é™…ä¸Šä¸ºçœŸæ—¶,æˆ‘ä»¬å¯èƒ½çŠ¯æ‹’ç» $H_0$ çš„é”™è¯¯,ç§°è¿™ç±»â€œå¼ƒçœŸâ€çš„é”™è¯¯ä¸ºç¬¬Iç±»é”™è¯¯. åˆå½“ $H_0$ å®é™…ä¸Šä¸çœŸæ—¶, æˆ‘ä»¬ä¹Ÿæœ‰å¯èƒ½æ¥å— $H_0$ ç§°è¿™ç±»â€œå–ä¼ªâ€çš„é”™è¯¯ä¸ºç¬¬â…¡ç±»é”™ã€‚\n\nä¸€èˆ¬æ¥è¯´,å½“æ ·æœ¬å®¹é‡å›ºå®šæ—¶,è‹¥å‡å°‘çŠ¯ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡, åˆ™çŠ¯å¦ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡å¾€å¾€å¢å¤§. è‹¥è¦ä½¿çŠ¯ä¸¤ç±»é”™è¯¯çš„æ¦‚ç‡éƒ½å‡å°,é™¤éå¢åŠ æ ·æœ¬å®¹é‡. \n\nåœ¨ç»™å®šæ ·æœ¬å®¹é‡çš„æƒ…å†µä¸‹, æˆ‘ä»¬æ€»æ˜¯æ§åˆ¶çŠ¯ç¬¬Iç±»é”™è¯¯çš„æ¦‚ç‡, ä½¿å®ƒä¸å¤§äº $\\alphaâ€‹$,  $\\alphaâ€‹$çš„å¤§å°è§†å…·ä½“æƒ…å†µè€Œå®š,é€šå¸¸å–0.1, 0.05, 0.01, 0.005ç­‰å€¼.  $\\alphaâ€‹$ å³ä¸º **æ˜¾è‘—æ€§æ°´å¹³**ã€‚\n\nè¿™ç§åªå¯¹çŠ¯ç¬¬Iç±»é”™è¯¯çš„æ¦‚ç‡åŠ ä»¥æ§åˆ¶,è€Œä¸è€ƒè™‘çŠ¯ç¬¬â…¡ç±»é”™è¯¯çš„æ¦‚ç‡çš„æ£€éªŒ, ç§°**æ˜¾è‘—æ€§æ£€éªŒ** ã€‚\n\n## å¼•ä¾‹æ±‚è§£\n\næ­¤ä¾‹ä¸­ çŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡ä¸º $P(\\text{å½“}H_0 \\text{ä¸ºçœŸæ—¶æ‹’ç»}H_0)$  , ä½¿å¾—çŠ¯è¿™ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡ä¸è¶…è¿‡ æ˜¾è‘—æ€§æ°´å¹³ $\\alpha$ ã€‚\n$$\nP(\\text{å½“}H_0 \\text{ä¸ºçœŸæ—¶æ‹’ç»}H_0) = P_{\\mu \\in H_0}(\\frac{ | \\bar x - \\mu_0 | }{\\sigma / \\sqrt n} \\ge k ) \\le \\alpha\n$$\nå…¶ä¸­:  $P_{\\mu \\in H_0}(.)$ è¡¨ç¤º $\\mu$ å– $H_0$ è§„å®šçš„å€¼æ—¶çš„äº‹ä»¶ï¼Œ å³$H_0$ ä¸ºçœŸæ—¶ã€‚æ­¤æ—¶, $z = \\frac{\\bar X - \\mu_0}{\\sigma / \\sqrt n} \\sim N(0, 1)$ . ç”±æ ‡å‡†æ­£å¤ªåˆ†å¸ƒçš„åˆ†ä½ç‚¹å¾— \n$$\nk = z_{\\alpha/2}\n$$\nåˆ†ä½ç‚¹ä¸æ˜¾è‘—è¡Œæ°´å¹³çš„å…³ç³»å¦‚ä¸‹ï¼š\n\n![](hypothesis-testing/normal-dist.png)\n\nå› è€Œ,è‹¥$Z$çš„è§‚å¯Ÿå€¼æ»¡è¶³ $|z| = \\frac{|\\bar X - \\mu_0|}{\\sigma / \\sqrt n} \\ge k = z_{\\alpha/2}$ åˆ™æ‹’ç»$H_0$ã€‚è€Œè‹¥ $|z| = \\frac{|\\bar X - \\mu_0|}{\\sigma / \\sqrt n} \\lt k = z_{\\alpha/2}$ , åˆ™æ¥å—$H_0$ .\næœ¬ä¾‹ä¸­å–$\\alpha=0.05$,åˆ™æœ‰$k=z_{0.05/2} =1.96$, åˆçŸ¥$n=9$, $\\sigma = 0.015$, ç”±æ ·æœ¬ç®—å¾—$\\bar x=0.511$,å³æœ‰\n$$\n\\frac{|\\bar X - \\mu_0|}{\\sigma / \\sqrt n} = 2.2 \\gt 1.96\n$$\n\näºæ˜¯æ‹’ç»$H_0$, è®¤ä¸ºè¿™å¤©åŒ…è£…æœºå·¥ä½œä¸æ­£å¸¸.\n\nnoteï¼šé€šå¸¸è½¯ä»¶(å¦‚spss)ä¸­ä¼šç›´æ¥ç»™å‡º p å€¼ï¼Œå³ çŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡ï¼Œ æˆ‘ä»¬å¯ä»¥ç›´æ¥å’Œè®¾å®šçš„æ˜¾è‘—æ€§æ°´å¹³è¿›è¡Œæ¯”è¾ƒã€‚\n\n# å•è¾¹æ£€éªŒ\n\nåƒå¼•ä¾‹ä¸­çš„å¤‡æ‹©å‡è®¾$H_1$,è¡¨ç¤º$\\mu$å¯èƒ½å¤§äº$\\mu_0$,ä¹Ÿå¯èƒ½å°äº$\\mu_0$,ç§°ä¸º**åŒè¾¹å¤‡æ‹©å‡è®¾**, è€Œç§°å½¢å¦‚å¼•ä¾‹é‡çš„å‡è®¾æ£€éªŒä¸º**åŒè¾¹å‡è®¾æ£€éªŒ**ã€‚\næœ‰æ—¶,æˆ‘ä»¬åªå…³å¿ƒæ€»ä½“å‡å€¼æ˜¯å¢å¤§ï¼Œè¿˜æ˜¯å‡å°ã€‚\n\nå½¢å¦‚ï¼š\n\n$$\nH_0: \\mu \\le \\mu_0 \\\\\nH_1: \\mu \\gt \\mu_0\n$$\n\nçš„å‡è®¾æ£€éªŒæˆä¸ºå³è¾¹æ£€éªŒï¼ˆå¤‡é€‰å‡è®¾è½äºå³è¾¹ï¼‰ã€‚\n\nå½¢å¦‚ï¼š\n\n$$\nH_0: \\mu \\ge \\mu_0 \\\\\nH_1: \\mu \\lt \\mu_0\n$$\n\nçš„å‡è®¾æ£€éªŒæˆä¸ºå·¦è¾¹æ£€éªŒï¼ˆå¤‡é€‰å‡è®¾è½äºå·¦è¾¹ï¼‰ã€‚\n\nå³è¾¹æ£€éªŒå’Œå·¦è¾¹æ£€éªŒæˆä¸ºåŒè¾¹æ£€éªŒã€‚\n\n### å•è¾¹æ£€éªŒçš„æ±‚è§£\n\nè®¾æ€»ä½“$X \\sim N(\\mu, \\sigma^2)$, $\\mu$æœªçŸ¥ã€$\\sigma$å·²çŸ¥,$X_1,X_2, \\dots ,X_n$,æ˜¯æ¥è‡ª$X$çš„æ ·æœ¬. ç»™å®šæ˜¾è‘—æ€§æ°´å¹³$a$. æ±‚æ£€éªŒé—®é¢˜\n$$\nH_0: \\mu \\le \\mu_0 \\\\\nH_1: \\mu \\gt \\mu_0\n$$\nçš„æ‹’ç»åŸŸã€‚\n\nå› $H_0$ä¸­çš„å…¨éƒ¨$\\mu$éƒ½æ¯”H1ä¸­çš„Î¼è¦å°,å½“$H_1$ä¸ºçœŸæ—¶,è§‚å¯Ÿå€¼$\\bar x$å¾€å¾€åå¤§. å› æ­¤, æ‹’ç»åŸŸçš„å½¢å¼ä¸º\n$$\nxâ‰¥k(kæ˜¯æŸä¸€æ­£å¸¸æ•°)\n$$\nä¸‹é¢æ¥ç¡®å®šå¸¸æ•°k:\n\n$$\n\\begin {align}\nP(\\text{å½“}H_0 \\text{ä¸ºçœŸæ—¶æ‹’ç»}H_0) &= P_{\\mu \\in H_0}(\\bar X \\ge k) \\\\\n&= P_{\\mu \\le \\mu_0}(\\frac{ \\bar X - \\mu_0}{\\sigma / \\sqrt n} \\ge \\frac{k - \\mu_0}{\\sigma / \\sqrt n} ) \\\\\n&\\le  P_{\\mu \\le \\mu_0}(\\frac{ \\bar X - \\mu}{\\sigma / \\sqrt n} \\ge \\frac{k - \\mu_0}{\\sigma / \\sqrt n} ) \\\\\n\\end {align}\n$$\n\nä¸Šå¼ä¸ç­‰å·æˆç«‹æ˜¯ç”±äº$\\mu \\le \\mu_0, \\frac{ \\bar X - \\mu}{\\sigma / \\sqrt n} \\ge \\frac{ \\bar X - \\mu_0}{\\sigma / \\sqrt n}$\n\nè¦æ§åˆ¶$P(\\text{å½“}H_0 \\text{ä¸ºçœŸæ—¶æ‹’ç»}H_0)$,åªéœ€ä»¤\n$$\nP_{\\mu \\le \\mu_0}(\\frac{ \\bar X - \\mu}{\\sigma / \\sqrt n} \\ge \\frac{k - \\mu_0}{\\sigma / \\sqrt n} ) =a\n$$\n\n\n\n\nç”±äº $\\frac{\\bar X - \\mu}{\\sigma / \\sqrt n} \\sim N(0, 1)$, ç”±ä¸Šå¼å¾— $\\frac{k - \\mu_0}{\\sigma / \\sqrt n} = z_a$ , å¦‚ä¸‹å›¾æ‰€ç¤º.\n\n![](hypothesis-testing/right-test.jpg)\n\næ‰€ä»¥ $k = \\mu_0 + \\frac \\sigma {\\sqrt n} z_a$, å³ï¼Œå³è¾¹æ£€éªŒé—®é¢˜çš„æ‹’ç»åŸŸä¸ºï¼š\n$$\n\\bar x \\ge \\mu_0 + \\frac \\sigma {\\sqrt n} z_a \\\\\nz = \\frac{\\bar x - \\mu}{\\sigma / \\sqrt n} \\ge z_a\n$$\n\nç±»ä¼¼çš„ï¼Œ å·¦è¾¹æ£€éªŒ\n$$\nH_0: \\mu \\ge \\mu_0 \\\\\nH_1: \\mu \\lt \\mu_0\n$$\n\nçš„æ‹’ç»åŸŸ\n\n$$\nz = \\frac{\\bar x - \\mu}{\\sigma / \\sqrt n} \\le -z_a\n$$\n\n\n\n# å¸¸ç”¨çš„å‡è®¾æ£€éªŒ\n\n\n\n## å•ä¸ªæ€»ä½“$N(\\mu,\\sigma^2)$å‡å€¼$\\mu$çš„æ£€éªŒ\n\n### $\\sigma$ å·²çŸ¥ (zæ£€éªŒ)\n\nå‡è®¾æ£€éªŒé—®é¢˜ï¼š$X \\sim N(\\mu, \\sigma^2)$, $\\mu$æœªçŸ¥, $\\sigma$ å·²çŸ¥ ,æ±‚æ£€éªŒé—®é¢˜$H_0: \\mu = \\mu_0; H_1 \\neq \\mu_0$ çš„æ‹’ç»åŸŸã€‚\n\nç»Ÿè®¡é‡ï¼š $Z=\\frac{\\bar X - \\mu_0}{\\sigma / \\sqrt n} \\sim N(0, 1)$\n\næ‹’ç»åŸŸï¼š  $\\|z\\| = \\frac{\\|\\bar X - \\mu_0\\|}{\\sigma / \\sqrt n} \\ge z_{\\alpha/2}$\n\n### $\\sigma$ æœªçŸ¥ (tæ£€éªŒ)\n\nå‡è®¾æ£€éªŒé—®é¢˜ï¼š $X \\sim N(\\mu, \\sigma^2)$, $\\mu$,$\\sigma$æœªçŸ¥  ,æ±‚æ£€éªŒé—®é¢˜$H_0: \\mu = \\mu_0; H_1 \\neq \\mu_0$ çš„æ‹’ç»åŸŸã€‚\n\nç»Ÿè®¡é‡ï¼š   $t=\\frac{\\bar X - \\mu_0}{S / \\sqrt n} \\sim t(n-1)$\n\næ‹’ç»åŸŸï¼š$\\|t\\|=\\|\\frac{\\bar X - \\mu_0}{S / \\sqrt n}\\| \\ge t_{a/2}(n-1)$\n\nnote: \n\n1. $n-1$ ä¸ºè‡ªç”±åº¦ï¼›\n2. tæ£€éªŒä¸­å®é™…ä¸Šæ˜¯ç”¨æ ·æœ¬æ–¹å·® $S^2$ æ›¿ä»£æœªçŸ¥çš„ æ€»ä½“æ–¹å·® $\\sigma^2$. æ›¿ä»£ä»¥åçš„ç»Ÿè®¡é‡æ°å¥½æ˜¯ tç»Ÿè®¡é‡\n\n## ä¸¤ä¸ªæ­£å¤ªæ€»ä½“å‡å€¼å·®çš„æ£€éªŒ (t æ£€éªŒ)\n\nå‡è®¾æ£€éªŒé—®é¢˜ï¼š $X \\sim N(\\mu_1, \\sigma^2); Y \\sim N(\\mu_2, \\sigma^2)$ $X$ å’Œ $Y$ çš„æ ·æœ¬å‡å€¼åˆ†åˆ«ä¸º$\\bar X$ï¼Œ $\\bar Y$, æ–¹å·®ä¸º $S_1^2$, $\\S_2^2$. æ ·æœ¬é‡ä¸ºï¼š $n_1$, $n_2$.æ±‚æ£€éªŒé—®é¢˜ $H_0: \\mu_1 - \\mu_2 = \\delta, H_1: \\mu_1 - \\mu_2 \\neq \\delta$\n\nç»Ÿè®¡é‡ï¼š $t=\\frac{(\\bar X - \\bar Y) - \\delta}{S_w / \\sqrt (\\frac 1n_1+ \\frac 1n_2)} \\sim t(n_1 +n_2 -2)$  å…¶ä¸­ $S_w^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 +n_2 -2}$\n\næ‹’ç»åŸŸï¼š$|t|=\\frac{|(\\bar X - \\bar Y) - \\delta|}{S_w / \\sqrt (\\frac 1n_1+ \\frac 1n_2)} \\ge t_{a/2}(n_1 +n_2 -2)$\n\n## å…¶ä»–æ£€éªŒ\n\næˆå¯¹æ ·æœ¬çš„æ£€éªŒ (tæ£€éªŒ)\n\næ­£æ€æ€»ä½“æ–¹å·®çš„æ£€éªŒï¼š å•ä¸ªæ€»ä½“ï¼ˆ$\\chi^2$ æ£€éªŒï¼‰ï¼›ä¸¤ä¸ªæ€»ä½“ï¼ˆFæ£€éªŒï¼‰\n\n# æ€»ç»“\n\n- ä¸¤ç±»é”™è¯¯ï¼šä¸€èˆ¬æ¥è¯´æˆ‘ä»¬æ€»æ˜¯æ§åˆ¶çŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡ã€‚\n- æ˜¾è‘—æ€§æ°´å¹³ $\\alpha$ ï¼šæœ¬è´¨æ˜¯æˆ‘ä»¬äººä¸ºå¯ä»¥æ¥å—çš„çŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡ã€‚\n- ç½®ä¿¡åº¦ ï¼š $(1-\\alpha)$\n- æ˜¾è‘—æ€§æ£€éªŒï¼šåªå¯¹çŠ¯ç¬¬ä¸€ç±»é”™è¯¯çš„æ¦‚ç‡åŠ ä»¥æ§åˆ¶ï¼Œè€Œä¸è€ƒè™‘çŠ¯ç¬¬äºŒç±»é”™è¯¯çš„æ¦‚ç‡çš„æ£€éªŒã€‚\n- åŒè¾¹æ£€éªŒã€å•è¾¹æ£€éªŒï¼ˆå·¦è¾¹æ£€éªŒï¼Œå³è¾¹æ£€éªŒï¼‰åŠå…¶æ±‚è§£ã€‚\n- ç»Ÿè®¡é‡ï¼š z ç»Ÿè®¡é‡ï¼Œt ç»Ÿè®¡é‡ï¼Œ$\\chi^2$ç»Ÿè®¡é‡ï¼Œ Fç»Ÿè®¡é‡\n\n\n# å‚è€ƒèµ„æ–™\n\nã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹-ç¬¬å››ç‰ˆ-æµ™å¤§ç‰ˆ ç¬¬å…«ç« ","tags":["Basic"],"categories":["Math"]},{"title":"Hexoé…ç½®","url":"%2Fblog%2Fhexo-config.html","content":"\nä½œä¸ºä¸€ä¸ªæŠ€æœ¯äººå‘˜ï¼Œå¹³æ—¶è‚¯å®šéœ€è¦å­¦ä¹ ä¸€äº›ä¸œè¥¿ï¼Œå¤šå¹´ä»¥æ¥æˆ‘ä¹ æƒ¯å°†å­¦çš„çš„ä¸œè¥¿è®°å½•åœ¨æ–‡ä»¶ä¸­ä¿å­˜ã€‚æ—¶é—´é•¿äº†ä»¥åå°±å‡ºé—®é¢˜äº†ï¼š\n\n1. æ–‡ä»¶æŸ¥æ‰¾å›°éš¾ã€‚å½“æˆ‘æƒ³å›å¤´çœ‹ä¹‹å‰è®°å½•çš„ä¸œè¥¿æ—¶ï¼Œç”±äºæ–‡ä»¶è¾ƒå¤šï¼ŒæŸ¥æ‰¾å›°éš¾ï¼Œæœ€ç»ˆè¿˜æ˜¯å»ç½‘ä¸Šæ‰¾ã€‚\n2. æ— æ³•æ·»åŠ tag\n\nåæ¥å°è¯•ä½¿ç”¨åšå®¢å›­ï¼Œå‘ç°åšå®¢çš„å‘å¸ƒå’Œç®¡ç†ç›¸å½“ä¸æ–¹ä¾¿ã€‚\n\næœ€åå†³å®šä½¿ç”¨Hexoåœ¨githubä¸Šå†™ã€‚ç”±äºæˆ‘éœ€è¦ç¼–å†™Latexå…¬å¼ï¼ŒHexoçš„é…ç½®è¿˜æ˜¯æ¶ˆè€—äº†ä¸€äº›æ—¶é—´çš„ï¼Œä¸ºäº†é˜²æ­¢ä»¥åæ­å»ºåšå®¢æ—¶çš„é‡å¤åŠ³åŠ¨ï¼Œé‚æˆæ­¤æ–‡ã€‚\n\næœ¬æ–‡è®°å½•ä¸ªäººçš„Hexoçš„é…ç½®è¿‡ç¨‹(åœ¨windowsä¸Š)ï¼Œå¸Œæœ›æœ€ç»ˆå½¢æˆä¸€ä¸ªè„šæœ¬ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å®Œæˆä¸ªäººåšå®¢çš„é…ç½®å·¥ä½œã€‚\n\n# å‰æœŸå‡†å¤‡\n\n1. å®‰è£…[Git Bash](https://git-scm.com/downloads)\n   é…ç½®gitï¼šåœ¨æ–°æœºå™¨ä¸Šç”Ÿæˆä¸€ä¸ªsshç§˜é’¥å’Œå…¬é’¥\n\n   ```shell\n   ssh-keygen -t rsa -C \"weirping@work-VM\"  ## -t åŠ å¯†ç®—æ³•ï¼Œ -C comment ç”¨äºè¿æ¥github\n   git config --global user.name 'weirping'  ## \n   git config --global user.email \"zhangweiping1988@gmail.com\"  ## \n   ```\n\n2. å®‰è£…[NodeJs](https://nodejs.org/en/) ï¼Œå®‰è£…æ˜¯å‹¾é€‰ `Add to PATH`é€‰é¡¹\n\n3. ä½¿ç”¨æ·˜å® NPM é•œåƒ\n\n   ```\n   npm --registry https://registry.npm.taobao.org info underscore \n   ```\n# blogåˆå§‹åŒ–ä¸é…ç½®\n\n```shell\nnpm install hexo -g  # å®‰è£…hexoæ¨¡å—\n\nhexo init blog  # åˆå§‹åŒ–ä¸€ä¸ªåšå®¢ç›®å½•\n\ncd blog\n\nnpm install  #è¿™æ ·ä¼šåœ¨blogæ–‡ä»¶å¤¹ä¸­ç”Ÿæˆæ•´ä¸ªåšå®¢ç¨‹åº\n```\n\nä¿®æ”¹ç«™ç‚¹ä¿¡æ¯\n```yaml\n# Site\ntitle: Weiping's notes\nsubtitle:\ndescription:\nkeywords: NLP, ML\nauthor: Weiping\nlanguage: zh-Hans\ntimezone:\n```\n\nä¿®æ”¹url ï¼šå»æ‰urlä¸­å¹´æœˆæ—¥çš„æ˜¾ç¤º\n\n```yaml\npermalink: blog/:title.html  # :year/:month/:day/:title/\n```\n\n`_post` ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œ`_post` ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹æ˜¯ä¸ä¼šå‘å¸ƒçš„ã€‚å½“æˆ‘ç¼–å†™ä¸€ä¸ªmarkdownæ–‡ä»¶çš„æ—¶å€™ï¼Œå¦‚ a.md ã€‚æˆ‘ä¼šåœ¨åŒçº§ç›®å½•ä¸‹å»ºä¸€ä¸ª **æ–‡ä»¶å¤¹a**ï¼Œå°†markdownæ–‡ä»¶æ‰€éœ€è¦çš„å›¾ç‰‡æ”¾åˆ°**æ–‡ä»¶å¤¹a**ä¸­ï¼Œåœ¨mdæ–‡ä»¶ä¸­åªéœ€è¦ä½¿ç”¨  `![](a/p.png)`å°±èƒ½å¼•ç”¨è¿™ä¸ªå›¾ç‰‡ã€‚\n\nå½“æˆ‘ç¼–å†™å¥½ä¸€ä¸ªmdæ–‡ä»¶åï¼Œåªéœ€è¦å°†**a.md**å’Œ**æ–‡ä»¶å¤¹a** åŒæ—¶æ‹·è´åˆ°`_post` ä¸­ï¼Œå‘å¸ƒå³å¯ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œéœ€è¦ï¼š\n\n```yaml\npost_asset_folder: true  # false\n```\n\n# NexTä¸»é¢˜å®‰è£…ä¸é…ç½®\n\n## å®‰è£…\n\n```shell\ngit clone https://github.com/theme-next/hexo-theme-next themes/next  # install\n```\n\nå¯ç”¨NexTä¸»é¢˜ï¼šæ‰“å¼€ ç«™ç‚¹çš„é…ç½®æ–‡ä»¶`_config.yml`ï¼Œ æ‰¾åˆ° `theme` å­—æ®µï¼Œå¹¶å°†å…¶å€¼æ›´æ”¹ä¸º `next`\n\n```yaml\ntheme: next\n```\n\n## NexTé…ç½®\n\n### é€‰æ‹© Scheme\n\n```yaml\n# ---------------------------------------------------------------\n# Scheme Settings\n# ---------------------------------------------------------------\n# Schemes\n#scheme: Muse\nscheme: Mist\n#scheme: Pisces\n#scheme: Gemini\n```\n\n### è®¾ç½®å¤´åƒ\n\næ–°å»ºæ–‡ä»¶å¤¹`source/avatar` åœ¨å…¶ä¸­æ”¾å…¥å¤´åƒæ–‡ä»¶ `avatar.jpg`\n\nä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶ `_config.yml`\n\n```\navatar: /avatar/avatar.jpg\n```\n\n### è®¾ç½®sidebar\n\n```yaml\nsidebar:\n  display: post\n```\n\nè®¾ç½®æˆ‘çš„github å’Œemail\n\n```yaml\nsocial:\n  GitHub: https://github.com/weirping || github\n  E-Mail: mailto:zhangweiping1988@gmail.com || envelope\n```\n\n### é¡µè„š (footer)\n\n```yaml\nfooter:\n  since: 2017\n  icon:\n    name: user\n    animated: false\n    color: \"#808080\"\n  copyright:\n  powered:\n    enable: false\n    version: false\n  theme:\n    enable: false\n    version: false\n```\n\n### æ˜¾ç¤ºblogä¿®æ”¹æ—¶é—´\n\n```yaml\npost_meta:\n  updated_at: true\n```\n\n### æ‘˜è¦\n\nhomeé¡µé¢è‡ªåŠ¨æå–å„blogçš„æ‘˜è¦\n\n```yaml\nauto_excerpt:\n  enable: true\n  length: 150\n```\n\n### è®¾ç½® èœå•\n\n```shell\nhexo new page tags  # æ–°å»ºtagsé¡µé¢\nhexo new page categories  # æ–°å»ºcategoriesé¡µé¢\n```\n\nåœ¨`source\\tags\\index.md` ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹\n\n```\n---\ntitle: æ ‡ç­¾\ndate: 2014-12-22 12:39:04\ntype: \"tags\"\n---\n```\n\nåœ¨`source\\categories\\index.md` ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹\n\n```\n---\ntitle: åˆ†ç±»\ndate: 2014-12-22 12:39:04\ntype: \"categories\"\n---\n```\n\nä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶ `_config.yml`\n\n```yaml\nmenu:\n  home: / || home\n  #about: /about/ || user\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive\n  #schedule: /schedule/ || calendar\n  #sitemap: /sitemap.xml || sitemap\n  #commonweal: /404/ || heartbeat\n```\n\n### Latexå…¬å¼æ”¯æŒ\n\nä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶ `_config.yml`\n\n```yaml\nmath:\n  enable: true  \n  per_page: false\n```\n\nMathjaxå’ŒMarkdownä¸€èµ·å·¥ä½œæœ‰ä¸ªå‘ã€‚\nä¸‹åˆ’çº¿`_` åœ¨Markdowné‡Œé¢æ˜¯æ ‡è®°æ–œä½“å­—ï¼Œè€Œä¸‹åˆ’çº¿`_`åœ¨Latexè¯­æ³•ä¸­æ˜¯è¡¨ç¤ºä¸‹æ ‡ã€‚\n`\\\\` åœ¨ Markdownä¸­ä¼šè¢«è½¬ä¹‰ä¸º`\\` ï¼Œè€Œåœ¨Latexä¸­`\\\\` è¡¨ç¤ºå¤šè¡Œå…¬å¼çš„æ¢è¡Œç¬¦ã€‚\n\nMarkdownä¼˜å…ˆäºMathjaxï¼Œæ‰€ä»¥ä¸‹åˆ’çº¿ä¼šæ¸²æŸ“æˆæ–œä½“å­—ï¼Œå¯¼è‡´ä¸€äº›MathJaxå…¬å¼ä¸èƒ½æ­£å¸¸æ˜¾ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦ä¿®æ”¹Hexoçš„æ¸²æŸ“å¼•æ“ `marked` çš„è½¬ä¹‰é…ç½®ã€‚ä½äº: `node_modules\\marked\\lib\\marked.js` ä¸­æ›¿æ¢`escape` å’Œ`em` \n\n```javascript\nvar inline = {\n  // escape: /^\\\\([\\\\`*{}\\[\\]()#+\\-.!_>])/,\n  escape: /^\\\\([`*\\[\\]()# +\\-.!_>])/,  \n  // em: /^_([^\\s_](?:[^_]|__)+?[^\\s_])_\\b|^\\*((?:\\*\\*|[^*])+?)\\*(?!\\*)/,\n  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n```\n\n# å‘å¸ƒåˆ°github\n\nåœ¨ github ä¸Šåˆ›å»º blog repo\n\n![](hexo-config/create-blog-repo.png)\n\né…ç½®blog\n\n```yaml\ndeploy:\n  type: git\n  repository: git@github.com:Weirping/weirping.github.io.git\n  branch: master\n```\n\n# éƒ¨ç½²è„šæœ¬\n\næ ¹æ®ä»¥ä¸Šå†…å®¹æˆ‘ç¼–å†™äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–éƒ¨ç½²Hexo Blog è„šæœ¬ï¼Œåœ°å€ä¸ºï¼š\n\n```\nhttps://github.com/Weirping/hexo-install\n```\n\n\n\n","tags":["Hexo"],"categories":["Hexo"]},{"title":"PRankç®—æ³•","url":"%2Fblog%2FPRank.html","content":"\nLTRæ¨¡å‹å¯ä»¥åˆ†ä¸º point-wiseã€pair-wiseã€list-wiseä¸‰ç±»ï¼ŒPrankç®—æ³•å±äºpoint-wiseæ¨¡å‹ã€‚\n\nPointwiseå¯ä»¥ç”¨äºæ¨èç³»ç»Ÿä¸­ä¹Ÿå¯ä»¥ç”¨äºIRä¸­ã€‚\n\nä»¥Pointwiseåœ¨IRä¸­çš„åº”ç”¨ä¸ºä¾‹ï¼Œå®ƒå¤„ç†å¯¹è±¡æ˜¯doc-query pairï¼Œå°†doc-queryçš„matchä¿¡æ¯è½¬åŒ–ä¸ºç‰¹å¾å‘é‡åï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿæ ¹æ®è®­ç»ƒå¾—å‡ºçš„æ¨¡å‹å¯¹doc-query pairè¿›è¡Œæ‰“åˆ†ï¼Œæ‰“åˆ†çš„é¡ºåºå³ä¸ºæœç´¢æ’åºçš„ç»“æœã€‚è®¡ç®—æ‰“åˆ†çš„å…¬å¼å¦‚ä¸‹ï¼š\n\n$$\n\\mathrm{score} =  \\mathbf w \\cdot \\mathbf x\n$$\n\nå…¶ä¸­$\\mathbf w$ä¸ºç‰¹å¾å„ç»´åº¦çš„æƒé‡ï¼Œ$\\mathbf x$ä¸ºdoc-query matchä¿¡æ¯è½¬åŒ–ä¸ºçš„ç‰¹å¾å‘é‡ã€‚\n\nåœ¨Rankingé—®é¢˜ä¸­æˆ‘ä»¬ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬çš„labelå€¼ä¸æ˜¯åˆ†æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¼±åˆ†çº§çš„labelï¼Œå¦‚`Perfect > Excellent > Good > Fair > Bad` è¿™æ ·å…·æœ‰å¼ºå¼±å…³ç³»çš„äº”çº§labelã€‚ä¸ºäº†å°†scoreæ˜ å°„åˆ°åˆ†çº§labelï¼Œéœ€è¦è®¾ç½®5ä¸ªé˜ˆå€¼æ¥åŒºåˆ†scoreçš„labelã€‚å¦‚æœscoreè½åœ¨æŸä¸¤ä¸ªç›¸é‚»é˜ˆå€¼ä¹‹é—´ï¼Œåˆ™åˆ’åˆ†ä¸ºç›¸åº”çš„labelï¼Œå¸¸è§ç®—æ³•åŒ…æ‹¬ï¼šPRankã€McRankã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºä¸ºPRankç®—æ³•ç¤ºæ„å›¾ï¼š\n\n![](PRank/score-rank.png)\n\n# ç®—æ³•æ€æƒ³\n\næ•°æ®ï¼š$X_{T \\times n}$ ï¼Œ $yï¼Œ y \\in  {1, 2, 3, \\dots, k}$ï¼Œ å³è®­ç»ƒæ ·æœ¬é‡ä¸º$T$ï¼Œ ç‰¹å¾ç»´åº¦ä¸ºnï¼Œ$y$å…±æœ‰kä¸ªç­‰çº§çš„å–å€¼ã€‚\n\nPRankçš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªranking rule H : $R^n \\rightarrow yâ€‹$ ï¼Œå³ï¼Œå°†æ ·æœ¬ç‰¹å¾æŠ•å½±åˆ°$kâ€‹$ä¸ªç­‰çº§ä¸Šã€‚$Hâ€‹$ å®šä¹‰å¦‚ä¸‹ï¼š\n$$\nH(\\mathbf x)=\\min_{r \\in {1, 2, 3, \\dots, k}}\\{r: \\mathbf w \\cdot \\mathbf x - b_r < 0 \\}\n$$\nå…¶ä¸­ï¼š$\\mathbf w$ å’Œ $b_r, r \\in {1, 2, 3, \\dots, k}$ æ˜¯æ¨¡å‹å‚æ•°ã€‚ $ \\mathbf w \\cdot \\mathbf x$ å°±æ˜¯ä¸Šæ–‡ä¸­æ‰€è¯´çš„scoreï¼Œ$k$ ä¸ª $b_r$ å°±æ˜¯ä¸Šæ–‡æ‰€è¯´çš„é˜ˆå€¼ï¼Œ$b_1 \\le \\dots \\le  b_{k-1} \\le  b_k = \\infty$ ã€‚å¦‚ä¸Šæ–‡æ‰€è¿°å¦‚æœä¸€ä¸ªæ ·æœ¬æ»¡è¶³$b_{r-1} \\lt \\mathbf w \\cdot \\mathbf x \\le b_r$ ï¼Œåˆ™è¯¥æ ·æœ¬çš„é¢„æµ‹rankå€¼ä¸º$r$ï¼Œå³ï¼Œ$\\hat y = H(\\mathbf x) = r$ã€‚\n\n# æ¨¡å‹å‚æ•°ä¼°è®¡\n\nPRankæ¨¡å‹å­˜åœ¨ä¸¤ç»„å‚æ•°$\\mathbf w$ å’Œ $b_r, r \\in {1, 2, 3, \\dots, k}$ ï¼Œåªè¦èƒ½å°†è¿™ä¸¤ç»„å‚æ•°ç¡®å®šä¸‹æ¥ï¼Œå°±å¤§åŠŸå‘Šæˆã€‚ä¸‹é¢è®²è§£å‚æ•°çš„ç¡®å®šè¿‡ç¨‹ã€‚PRankæ›´æ–°å‚æ•°çš„æ–¹æ³•ç±»ä¼¼äºéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå³ï¼Œæ¯å½“é¢„æµ‹é”™è¯¯ä¸€ä¸ªæ ·æœ¬åï¼Œå°±æ›´æ–°å‚æ•°ã€‚\n\nå¯¹äºä¸€ä¸ªæ ·æœ¬$(\\mathbf x, y)$ å’Œæ¨¡å‹å‚æ•°$\\mathbf w$ å’Œ $b_r, r \\in \\{1, 2, 3, \\dots, k\\}$ ã€‚å¦‚æœæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®é¢„æµ‹ï¼Œåˆ™ :\n$$\n\\begin {align}\n& \\mathbf w \\cdot \\mathbf x \\gt b_r ,  r \\le y-1 \\\\\n& \\mathbf w \\cdot \\mathbf x \\lt b_r ,  r \\ge y\n\\end {align}\n$$\n\nå¦‚ä¸Šæ‰€è¿°ï¼Œ$y$çš„å–å€¼æ˜¯ä»é›†åˆ$\\{1, 2, 3, \\dots, k\\}$ ä¸­é€‰å–ã€‚æ¨¡å‹å¼•å…¥è¾…åŠ©å˜é‡$s$(åŸæ–‡ä¸­ä¾ç„¶ä½¿ç”¨$y$ï¼Œæˆ‘è®¤ä¸ºå¯èƒ½äº§ç”Ÿç¬¦å·æ··æ·†ï¼Œæ‰€ä»¥æ”¹ç”¨$s$) ï¼Œ \n\n$s$æ˜¯ä¸€ä¸ªå‘é‡ï¼Œé•¿åº¦ä¸º$k-1$ ï¼Œ $s$ ä¸ $y$ æ˜¯ä¸€ä¸€å¯¹åº”çš„(ç±»ä¼¼äºå¤šåˆ†ç±»é—®é¢˜ä¸­ï¼Œä½¿ç”¨onehotç¼–ç å°†labelå‘é‡åŒ–)ã€‚å¯¹äºæ ·æœ¬ $(\\mathbf x, y)$æ¥è¯´ï¼Œsçš„å„ç»´åº¦å€¼å®šä¹‰å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\n& s_r = +1 , r \\le y-1 \\\\\n& s_r = -1,  r \\ge y , r \\lt k\n\\end {align}\n$$\nä¾‹å¦‚å½“ $k=5$æ—¶ï¼Œ$y$çš„å–å€¼ä¸$s$çš„å…³ç³»å¦‚ä¸‹ï¼š\n\n$$\n\\begin {align}\n& y=1 \\Leftrightarrow s=[-1,  -1, -1, -1] \\\\\n& y=2 \\Leftrightarrow s=[ +1, -1, -1, -1] \\\\\n& y=3 \\Leftrightarrow s=[ +1,+1, -1, -1] \\\\\n& y=4 \\Leftrightarrow s=[ +1,+1,+1, -1] \\\\\n& y=5 \\Leftrightarrow s=[ +1,+1,+1,+1]\n\\end {align}\n$$\n\nå¯¹äºæ ·æœ¬$(\\mathbf x, y)$æ¥è¯´ï¼Œ å¯¹äºæ‰€æœ‰çš„$r \\in \\{1, \\dots , k-1\\}$ ï¼š\n\n1. å¦‚æœæ¨¡å‹é¢„æµ‹ç»“æœæ˜¯æ­£ç¡®çš„ï¼Œåˆ™$s_r\\cdot (\\mathbf w \\cdot \\mathbf x - b_r ) \\gt 0$ï¼Œæ­¤æ—¶å‚æ•°ä¸éœ€è¦æ›´æ–°\n2. å¦‚æœæ¨¡å‹é¢„æµ‹ç»“æœæ˜¯é”™è¯¯çš„ï¼Œåˆ™**å­˜åœ¨**$s_r\\cdot (\\mathbf w \\cdot \\mathbf x - b_r ) \\le 0$ï¼Œæ­¤æ—¶éœ€è¦æ›´æ–°å‚æ•°\n\nå¯¹äºç‰¹å®šçš„$r$ï¼Œå¦‚æœ$s_r\\cdot (\\mathbf w \\cdot \\mathbf x - b_r ) \\le 0$ï¼Œ åˆ™è¯´æ˜scoreå€¼$\\mathbf w \\cdot \\mathbf x$ è½åœ¨äº†é˜ˆå€¼$b_r$çš„**é”™è¯¯çš„ä¸€ä¾§**ï¼Œä¸ºäº†æ”¹æ­£è¿™ä¸ªé”™è¯¯çš„é¢„æµ‹ï¼Œåªéœ€è¦å°†$\\mathbf w \\cdot \\mathbf x$å’Œ$b_r$**ç›¸å¯¹**ç§»åŠ¨å°±èƒ½å®Œæˆå¯¹äºå‚æ•°$\\mathbf w$ å’Œ $b_r$ çš„æ›´æ–°ã€‚æ›´æ–°å…¬å¼å¦‚ä¸‹ï¼š\n\nå¯¹äº$s_r\\cdot (\\mathbf w \\cdot \\mathbf x - b_r ) \\le 0â€‹$ï¼š\n$$\n\\begin {align}\n& b_r = b_r - s_r \\\\\n& \\mathbf w = \\mathbf w + s_r \\cdot \\mathbf x\n\\end {align}\n$$\nå½“$y=4$ ï¼Œé¢„æµ‹å€¼è½åœ¨äº†Rank 3 èŒƒå›´å†…æ—¶ï¼š$s_ 3= +1$ ï¼Œæ­¤æ—¶éœ€è¦å°†$b_3$å‘å·¦ç§»åŠ¨ï¼Œ $\\mathbf w \\cdot \\mathbf x$å‘å³ç§»åŠ¨ã€‚\n$$\n\\begin {align}\n& b_3 = b3 - s_3  = b3 - (+1) \\\\\n& \\mathbf w = \\mathbf w + (+1) \\cdot \\mathbf x  \\Leftrightarrow \\mathbf w \\cdot \\mathbf x = \\mathbf w \\cdot \\mathbf x + (+1) \\mathbf x^2\n\\end {align}\n$$\n\n\n![](PRank/wrong-left.png)\n\nå½“$y=2$ ï¼Œé¢„æµ‹å€¼è½åœ¨äº†Rank 3 èŒƒå›´å†…æ—¶ï¼š$s_ 2= -1$ ï¼Œæ­¤æ—¶éœ€è¦å°†$b_2$å‘å³ç§»åŠ¨ï¼Œ $\\mathbf w \\cdot \\mathbf x$å‘å·¦ç§»åŠ¨ã€‚\n$$\n\\begin {align}\n& b_2 = b2 - s_2  = b3 - (-1) \\\\\n& \\mathbf w = \\mathbf w + (-1) \\cdot \\mathbf x  \\Leftrightarrow \\mathbf w \\cdot \\mathbf x = \\mathbf w \\cdot \\mathbf x + (-1) \\mathbf x^2\n\\end {align}\n$$\n![](PRank/wrong-right.png)\n\nå¦‚æœæœ‰å­˜åœ¨å¤šä¸ª$r$ ä½¿å¾—$s_r\\cdot (\\mathbf w \\cdot \\mathbf x - b_r ) \\le 0$æ—¶ï¼Œæ›´æ–°è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](PRank/update-rule.png)\n\n# å®ç°æµç¨‹\n\nPRankç®—æ³•æ¯”è¾ƒç®€å•ï¼ŒåŸºæœ¬æµç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n![](PRank/prank-algorithm.png)\n\n# å®ç°ä»£ç \n\nä»¥ä¸‹ä¸ºPRankç®—æ³•çš„Pythonå®ç°ä»£ç \n\nnoteï¼šç”±äºå®é™…ç¯å¢ƒä¸­æˆ‘ä»¬ä½¿ç”¨çš„rateæ˜¯ä»0å¼€å§‹çš„ï¼Œå¦‚5çº§ä½¿ç”¨çš„æ˜¯$\\{0, 1, 2, 3 ,4\\}$ æ¥åšæ ‡è®°ï¼Œæ‰€ä»¥ä¸‹é¢çš„ä»£ç å’Œä¸Šæ–‡ä¸­çš„è®ºè¿°æœ‰ä¸€ç‚¹å‡ºå…¥ã€‚\n\n```python\nimport numpy as np\n\ndef prank(X, y, rate_levels, epochs=10, delta=0.01):\n    '''\n    epochs: int, æ”¶æ•›ä¹‹å‰ éå†æ ·æœ¬æ¬¡æ•°\n    rate_lavels: int, yä¸­rateçš„çº§åˆ«æ•°é‡ï¼Œå¦‚yç”¨ 0,1,2,3 æ ‡è®°ï¼Œåˆ™rate_lavels = 4\n    X: è®­ç»ƒæ ·æœ¬featureé›†åˆ\n    y: è®­ç»ƒæ ·æœ¬çš„rateé›†åˆï¼Œéœ€è¦ä» 0å¼€å§‹ã€‚å¦‚æœæœª5çº§æ ‡æ³¨ï¼Œéœ€è¦ä¸º 0, 1, 2, 3, 4\n    delta: float, ç»ˆæ­¢è¿­ä»£æ¡ä»¶\n    '''\n    N = len(X)  # æ ·æœ¬é‡\n    D = len(X[0])  # æ ·æœ¬ç»´åº¦\n    theta = np.zeros(D)\n    b = np.zeros(rate_levels - 1)\n    s = np.zeros((N, rate_levels - 1))  # è¾…åŠ©å˜é‡ï¼Œç±»ä¼¼äºyçš„one-hotè¡¨ç¤º\n    for i in range(0, rate_levels - 1):  # åˆå§‹åŒ–b\n        b[i] = i\n    for n in range(0, N):  # åˆå§‹åŒ– s\n        for l in range(0, rate_levels - 1):\n            if (y[n] <= l):\n                s[n][l] = -1\n            else:\n                s[n][l] = 1\n    last_err_ratio = 1.0 + delta\n    for it in range(0, epochs):\n        err_cnt = 0\n        for n in range(0, N):\n            E = np.zeros(rate_levels - 1) - 1  # ç”¨äºè®°å½•è¯¥æ ·æœ¬\n            for l in range(0, rate_levels - 1):\n                if (s[n][l] * (np.dot(theta, X[n]) - b[l])) <= 0:\n                    E[l] = l\n            if(len(E[E >= 0]) > 0):\n                err_cnt += 1\n                sumval = 0  # é¢„æµ‹rateå’ŒçœŸæ­£rate çš„é—´è·ï¼ŒçœŸæ­£labelæ˜¯3ï¼Œé¢„æµ‹labelæ˜¯1ï¼Œåˆ™sumval=3-1=2\n                for l in range(0, rate_levels - 1):  # ç»Ÿè®¡ sumval\n                    if(E[l] >= 0):\n                        sumval += s[n][int(E[l])]\n                for d in range(0, D):  # æ›´æ–°theta\n                    theta[d] += sumval * X[n][d]\n                for l in range(0, rate_levels - 1):  # æ›´æ–°b\n                    if(E[l] >= 0):\n                        b[int(E[l])] -= s[n][int(E[l])]\n        cur_err_ratio = err_cnt * 1.0 / N\n        print(err_cnt, N, cur_err_ratio)\n        if abs(last_err_ratio - cur_err_ratio) < delta:\n            break\n        last_err_ratio = cur_err_ratio\n    return theta, b\n```\n\nsklearn estimatorç‰ˆ\n\n```python\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin\nfrom sklearn.base import BaseEstimator\n\nclass PRank(BaseEstimator, TransformerMixin):\n\n    def __init__(self, rate_levels, epochs, delta):\n        self._rate_levels =  rate_levels\n        self._epochs = epochs\n        self._delta = delta\n\n    def _prank(self, X, y, rate_levels, epochs=10, delta=0.01):\n        '''\n        epochs: int, æ”¶æ•›ä¹‹å‰ éå†æ ·æœ¬æ¬¡æ•°\n        rate_lavels: int, yä¸­rateçº§åˆ«çš„æ•°é‡ï¼Œå¦‚yç”¨ 0,1,2,3 æ ‡è®°ï¼Œåˆ™rate_lavels = 4\n        X: array è®­ç»ƒæ ·æœ¬featureé›†åˆ\n        y: list è®­ç»ƒæ ·æœ¬çš„rateé›†åˆï¼Œéœ€è¦ä» 0å¼€å§‹ã€‚å¦‚ä¸º5çº§æ ‡æ³¨ï¼Œéœ€è¦ä¸º 0, 1, 2, 3, 4\n        delta: float, ç»ˆæ­¢è¿­ä»£æ¡ä»¶\n        '''\n        N = len(X)  # æ ·æœ¬é‡\n        D = len(X[0])  # æ ·æœ¬ç»´åº¦\n        theta = np.zeros(D)\n        b = np.zeros(rate_levels - 1)\n        s = np.zeros((N, rate_levels - 1))  # è¾…åŠ©å˜é‡ï¼Œç±»ä¼¼äºyçš„one-hotè¡¨ç¤º\n        for i in range(0, rate_levels - 1):  # åˆå§‹åŒ–b\n            b[i] = i\n        for n in range(0, N):  # åˆå§‹åŒ– s\n            for l in range(0, rate_levels - 1):\n                if (y[n] <= l):\n                    s[n][l] = -1\n                else:\n                    s[n][l] = 1\n        last_err_ratio = 1.0 + delta\n        for it in range(0, epochs):\n            err_cnt = 0\n            for n in range(0, N):\n                E = np.zeros(rate_levels - 1) - 1  # ç”¨äºè®°å½•è¯¥æ ·æœ¬\n                for l in range(0, rate_levels - 1):\n                    if (s[n][l] * (np.dot(theta, X[n]) - b[l])) <= 0:\n                        E[l] = l\n                if(len(E[E >= 0]) > 0):\n                    err_cnt += 1\n                    sumval = 0  # é¢„æµ‹rateå’ŒçœŸæ­£rate çš„é—´è·ï¼ŒçœŸæ­£labelæ˜¯3ï¼Œé¢„æµ‹labelæ˜¯1ï¼Œåˆ™sumval=3-2=1\n                    for l in range(0, rate_levels - 1):  # ç»Ÿè®¡ sumval\n                        if(E[l] >= 0):\n                            sumval += s[n][int(E[l])]\n                    for d in range(0, D):  # æ›´æ–°theta\n                        theta[d] += sumval * X[n][d]\n                    for l in range(0, rate_levels - 1):  # æ›´æ–°b\n                        if(E[l] >= 0):\n                            b[int(E[l])] -= s[n][int(E[l])]\n            cur_err_ratio = err_cnt * 1.0 / N\n            # print(err_cnt, N, cur_err_ratio)\n            if abs(last_err_ratio - cur_err_ratio) < delta:\n                break\n            last_err_ratio = cur_err_ratio\n        return theta, b\n\n    def fit(self, X, y):\n        self.theta_, self.b_ = self._prank(X, y, self._rate_levels, self._epochs, self._delta)\n        return self\n\n    def predict(self, X):\n        def get_rank(row):\n            rank = len(row)\n            for ind in range(len(row)):\n                if row[ind] < 0:\n                    rank = ind\n                    break\n            return rank\n        scores = np.dot(X_test_scale, pranker.theta_)[:, np.newaxis] - pranker.b_ \n        pred = np.apply_along_axis(get_rank, 1, scores)\n        return pred\n    \n    def score():\n        pass\n```\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[Pranking with Ranking](http://papers.nips.cc/paper/2023-pranking-with-ranking.pdf)","tags":["LTR"],"categories":["LTR"]},{"title":"multiprocessing - Pythonä¸­çš„å¹¶å‘å¤„ç†","url":"%2Fblog%2Fmultiprocessing-in-Python.html","content":"\nä¹‹å‰å†™è¿‡javaï¼ŒC++ ç­‰ç¨‹åºï¼Œæ¥è§¦åˆ°pythonåå¦‚æœæƒ³å†™ä¸€äº›éœ€è¦å¤§é‡è®¡ç®—çš„ç¨‹åºï¼Œé¦–å…ˆæƒ³åˆ°çš„å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªå¤šçº¿ç¨‹ç¨‹åºã€‚ä½†æ˜¯â€¦â€¦\n\n# threadingæ¨¡å—å’Œmultiprocessingæ¨¡å—\n\nåœ¨pythonä¸­threadingæ¨¡å—ç”¨äºå¤„ç†å¤šçº¿ç¨‹é—®é¢˜ï¼Œä½†æ˜¯ç”±äºPythonçš„GILï¼ˆå…¨å±€è§£é‡Šé”ï¼‰ï¼Œå¯¼è‡´pythonä¸­çš„å¤šçº¿ç¨‹ä¸èƒ½åˆ©ç”¨å¤šæ ¸CPUã€‚é€šè¿‡å®é™…codingå¯ä»¥å‘ç°ï¼Œpythonä¸­ä½¿ç”¨threadingå®ç°çš„å¤šçº¿ç¨‹è®¡ç®—ç¨‹åºå®é™…ä¸Š**æœ€å¤šåªèƒ½ä½¿ç”¨ä¸€ä¸ªCPUæ ¸å¿ƒ** ï¼Œæ‰€ä»¥å¯¹äºéœ€è¦å¤§é‡è®¡ç®—çš„åº”ç”¨æ¥è¯´ï¼Œthreadingæ¨¡å—å®é™…èµ·ä¸åˆ°ä»€ä¹ˆä½œç”¨ã€‚\n\nè®¡ç®—æœºç¨‹åºå¯ä»¥åˆ†ä¸º **è®¡ç®—å¯†é›†å‹** å’Œ **IOå¯†é›†å‹** ä¸¤ç§ï¼š\né€šè¿‡ä»¥ä¸Šåˆ†æå¯ä»¥å‘ç°ï¼Œthreadingæ¨¡å—ç”±äºä¸èƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼Œæ‰€ä»¥å¯¹äºè®¡ç®—å¯†é›†å‹çš„ç¨‹åºæ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚\nä½†æ˜¯å¯¹äºIOå¯†é›†å‹ç¨‹åºï¼Œthreadingæ¨¡å—å´èƒ½å¤Ÿåˆ©ç”¨CPUçš„æ€§èƒ½ã€‚å¦‚æœç¨‹åºä¸­éœ€è¦è¿›è¡Œå¤§é‡çš„ç½‘ç»œä¼ è¾“æˆ–è€…æ–‡ä»¶è¯»å†™ç­‰IOæ“ä½œæ—¶ï¼Œç”±äºè®¡ç®—æœºIOçš„é€Ÿåº¦è¿œæ²¡æœ‰CPUå¤„ç†æ•°æ®çš„é€Ÿåº¦å¿«ï¼Œæ‰€ä»¥å¿…ç„¶ä¼šå‡ºç°CPUç­‰å¾…IOå®Œæˆçš„ç°è±¡ï¼Œå¦‚æœåœ¨ä¸€æ®µä»£ç å¤„åœ¨ç­‰å¾…IOæ—¶æ‰§è¡Œå…¶ä»–å¾…æ‰§è¡Œçš„ä»£ç ï¼Œå¿…ç„¶èƒ½å¤ŸåŠ å¿«ç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦å¹¶å……åˆ†åˆ©ç”¨CPUã€‚\n\næ‰€æœ‰threadingæ¨¡å—å¯¹äºIOå¯†é›†å‹ç¨‹åºæœ‰ä¼˜åŒ–ä½œç”¨ï¼Œå¯¹äºè®¡ç®—å¯†é›†å‹çš„ç¨‹åºåŸºæœ¬æ²¡æœ‰ä»€ä¹ˆä½œç”¨ã€‚\n\nå¯¹äº **è®¡ç®—å¯†é›†å‹**çš„ç¨‹åºï¼Œpythonä½¿ç”¨çš„æ˜¯multiprocessingï¼Œå³å¤šè¿›ç¨‹ã€‚ä¸‹æ–‡æ•´ç†pythonçš„å¤šè¿›ç¨‹ä½¿ç”¨æ–¹æ³•ã€‚é¦–å…ˆè®¤è¯†è¿›ç¨‹å¯¹è±¡Processã€‚ç„¶åè¿›ç¨‹å®‰å…¨æœºåˆ¶ã€‚è¿›ç¨‹æ± poolçš„ä½¿ç”¨ã€‚\n\n# Process\n\n## åˆå§‹åŒ–\n\n```python\nclass multiprocessing.Process(group=None  # ä¸ºäº†å’ŒThreadingä¿æŒç»Ÿä¸€ï¼Œæ­¤å¤„æ— ç”¨\n                              ,target=None  # ç›®æ ‡å‡½æ•°\n                              ,name=None  # è¯¥è¿›ç¨‹çš„åå­—\n                              ,args=()  # ç›®æ ‡å‡½æ•°çš„ä½ç½®å‚æ•°&å…ƒç»„å‚æ•°\n                              ,kwargs={})  # ç›®æ ‡å‡½æ•°çš„å­—å…¸å‚æ•°\n```\n\n## å…³é”®å‡½æ•°ï¼š\n\n\n> start()  # å¼€å§‹æ‰§è¡Œ\n> is_alive()  #\n> join([timeout])   # \n> terminate()  # ç»ˆæ­¢\n> exitcode  # å±æ€§ï¼Œè¡¨æ˜è¯¥è¿›ç¨‹é€€å‡ºçš„çŠ¶æ€, çŠ¶æ€å€¼çš„å«ä¹‰ https://docs.python.org/2/library/signal.html\n\n## å®ä¾‹ï¼š\n\n```python\nfrom multiprocessing import Process\nimport time\nimport os\n\ndef info(title):\n    process_info = u'title:' + title\n    process_info += u'|module name:' + __name__\n    if hasattr(os, u'getppid'):  # only available on Unix\n        process_info += u'|parent process:' + unicode(os.getppid())\n    process_info += u'|process id:' + unicode(os.getpid())\n    print process_info\n\ndef f(name):\n    info(u'function f')\n    time.sleep(0.5)\n    print u'hello', name\n\ninfo(u'main line')\np1 = Process(target=f, args=(u'bob',))\np1.start()\nprint u'p1 is_alive:', p1.is_alive()\np1.join()\nprint u'p1 is_alive:', p1.is_alive()\n\np2 = Process(target=f, args=(u'tom',))\np2.start()\nprint u'p2 started, p2 is_alive:', p2.is_alive()\np2.terminate()\ntime.sleep(0.1)  # éœ€è¦ä¸€æ®µæ—¶é—´æ‰èƒ½åœæ­¢p2\nprint u'p2 terminated, p2 is_alive:', p2.is_alive()\nprint p2.exitcode \n================================outpupt===================================================\ntitle:main line|module name:__main__|parent process:2794|process id:2824\ntitle:function f|module name:__main__|parent process:2824|process id:3761\np1 is_alive: True\nhello bob\np1 is_alive: False\np2 started, p2 is_alive: True\np2 terminated, p2 is_alive: False\n-15\n```\n\n\n\n# è¿›ç¨‹ä¹‹é—´ä¼ è¾“æ•°æ®\n\nï¼ˆExchanging objects between processesï¼‰\n\næœ‰ä¸¤ç§è¿›ç¨‹ä¹‹é—´é€šä¿¡çš„æ–¹å¼Pipeså’ŒQueuesã€‚\n\n- `Pipe()` ä¸¤ä¸ªè¿›ç¨‹ä¹‹é—´çš„é€šä¿¡\n- `queue`å¤šç”Ÿäº§è€…å’Œå¤šæ¶ˆè´¹è€…ä¹‹é—´é€šä¿¡ã€‚åŒ…æ‹¬`Queue`, `multiprocessing.queues.SimpleQueue` å’Œ`JoinableQueue` ä¸‰ç§\n\n*æ³¨æ„ï¼šPipe()ï¼Œ å’Œ queue åªèƒ½ç”¨äºProcessä¹‹é—´çš„é€šä¿¡ã€‚ä¸ç”¨ç”¨äºPoolç®¡ç†çš„è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡*\n\n## Pipe\n\n![](multiprocessing-in-Python/pipe.png)\n\nå®šä¹‰ï¼š\n\n```python\nPipe(duplex=True)  # é€šè¿‡æŸ¥çœ‹æºç å¯ä»¥å‘ç°ï¼ŒPipeå®é™…ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°\n```\n\nä¼ è¾“çš„æ•°æ®å¯¹è±¡å¯ä»¥ä½¿ç”¨`pickle`è¿›è¡Œåºåˆ—åŒ–æ—¶\n\n- `send(obj)` å°†objä½¿ç”¨pickleåºåˆ—åŒ–åé€å…¥pipeé€šé“\n- `recv()`   æ¥å—`send(obj)`å‘é€çš„pickleåºåˆ—åŒ–æ•°æ®ï¼Œå¹¶è§£ææˆåŸå§‹çš„objå¯¹è±¡ã€‚**å¦‚æœpipeä¸­æ²¡æœ‰æ•°æ®æ—¶ï¼Œè¯¥å‡½æ•°ä¼šé˜»å¡å…¶æ‰€åœ¨è¿›ç¨‹ï¼Œç›´åˆ°æ¥æ”¶åˆ°æ–°çš„æ•°æ®.**\n\nå¦‚æœä¼ è¾“çš„å¯¹è±¡ä¸èƒ½ä½¿ç”¨`pickle`åºåˆ—åŒ–ï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ä»¥byteè¿›è¡Œä¼ è¾“ï¼š\n\n- `send_bytes(buffer[, offset[, size]])`\n\n- `recv_bytes([maxlength])`\n\n- `recv_bytes_into(buffer[, offset])`\n\n\n```python\nfrom multiprocessing import Process, Pipe\nimport time\nimport os\n\ndef client(conn):\n    i = 0\n    while i < 20:\n        print 'client', 'send',i\n        conn.send(i)\n        time.sleep(0.1)\n        i += 1\n        r = conn.recv()\n        print 'client', 'recv',r\n    conn.send(None)\n    conn.close()\n    \ndef calc(conn):\n    i = 0\n    while True:\n        r = conn.recv()\n        if r == None:\n            break        \n        print 'calc', 'recv', r\n        time.sleep(0.5)\n        conn.send(str(r) +  '*' + str(r) + '=' + str(r*r))\n    conn.close()\n\n# duplex=True åŒå‘é€šä¿¡ ,\n# duplex=False å•å‘ä¼ è¾“ï¼Œconn1 æ¥æ”¶ç«¯ï¼Œconn2 å‘é€ç«¯\nconn1, conn2 = Pipe(duplex=True)  \nsend_pro = Process(target=client, args=(conn2,))\nrecv_pro = Process(target=calc, args=(conn1,))\nsend_pro.start()\nrecv_pro.start()\nsend_pro.join()\nrecv_pro.join()\n```\n\n##  Queues\n\nç”¨äºè¿›ç¨‹é—´é€šä¿¡çš„è¿˜æœ‰é˜Ÿåˆ—ï¼Œå¤šè¿›ç¨‹çš„é˜Ÿåˆ—å¯¹è±¡å®é™…ä¸Šæ˜¯å¯¹`Pipe`çš„å°è£…ã€‚å­˜åœ¨å¦‚ä¸‹ä¸‰ä¸ªé˜Ÿåˆ—ç±»ã€‚\n\n```\nmultiprocessing.Queue  # å¯¹Pipeçš„å°è£…\nmultiprocessing.JoinableQueue  # ç»§æ‰¿è‡ªQueueï¼Œå¢åŠ äº†join() å’Œ task_done() æ–¹æ³•\nmultiprocessing.SimpleQueue  # å¯¹Pipeçš„å°è£…, ç›¸å¯¹äºQueueæ¥è¯´ æ›´ç®€å•\n```\n\n### Queue\n\nä½¿ç”¨`Queue`å°†æ•°æ®ä»ç”Ÿäº§è€…ä¼ è¾“åˆ°æ¶ˆè´¹ä¹‹çš„æµç¨‹å¦‚ä¸‹ã€‚\n\n![](multiprocessing-in-Python/queue.png)\n\nå¦‚ä¸Šæ‰€è¿°ï¼Œ`Queue` å®é™…ä¸Šæ˜¯å¯¹`Pipe`çš„å°è£…ï¼Œä½†æ˜¯å½“ç”Ÿäº§è€…å°†æ•°æ®æ”¾å…¥`Queue`æ—¶ï¼Œä¸æ˜¯ç›´æ¥æ”¾å…¥`Pipe`ä¸­ï¼Œ`Queue`ä½¿ç”¨äº†ä¸€ä¸ªç¼“å†²åŒº`Buffer` ã€‚`put`å‡½æ•°å…ˆå°†æ•°æ®æ”¾å…¥`Buffer`ä¸­ï¼Œå†ç”±ä¸€ä¸ªä¸“é—¨çš„`feed`çº¿ç¨‹å°†å…¶æ”¾å…¥`Pipe`å½“ä¸­ã€‚æ¶ˆè´¹ä¹‹åˆ™æ˜¯ç›´æ¥ä»`Pipe`ä¸­`get`å¯¹è±¡ã€‚\n\nå®šä¹‰ï¼š\n\n```\nQueue([maxsize])  # é˜Ÿåˆ—åŒæ—¶èƒ½å®¹çº³çš„å¯¹è±¡çš„æ•°é‡\n```\n\nå…³é”®å‡½æ•°ï¼š\n\n- ä½¿ç”¨æ ‡å‡†åº“ä¸­çš„ `Queue.Empty` å’Œ `Queue.Full`exceptions æ¥åˆ¤æ–­é˜Ÿåˆ—æ˜¯å¦å·²ç»æ»¡äº†ï¼Œç”¨åœ¨putå‡½æ•°ä¸­ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚ä¸‹ä¸¤ä¸ªå‡½æ•°ï¼Œä½†æ˜¯ä¸å¯é ã€‚`empty()` ã€`full()`\n\n\n\n- `put(obj[, block[, timeout]])`\n  å°†`obj`æ·»åŠ åˆ°`Queue`ä¸­ã€‚å¦‚æœé˜Ÿåˆ—å·²æ»¡ï¼Œåˆ™é˜»å¡è¯¥è¿›ç¨‹`timeout`é•¿æ—¶é—´ï¼Œå¦‚æœ`timeout`æ—¶é—´ä»¥åé˜Ÿåˆ—è¿˜æ˜¯æ»¡çš„ï¼Œåˆ™äº§ç”Ÿå¼‚å¸¸`Queue.Full` ã€‚`block` é»˜è®¤ `True` ï¼Œ`timeout` é»˜è®¤æ— ç©·å¤§(`None`)ã€‚`block` ä¸º `False`æ—¶ï¼Œå¦‚æœé˜Ÿåˆ—æ˜¯æ»¡çš„ç›´æ¥äº§ç”Ÿå¼‚å¸¸`Queue.Full` ã€‚\n\n\n- `put_nowait(obj) `\n\n  ç­‰ä»·äº `put(obj, block=False)`.\n\n\n- `get([block[, timeout]])`\n  ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸€ä¸ªå¯¹è±¡ã€‚å¦‚æœé˜Ÿåˆ—æ˜¯ç©ºçš„åˆ™é˜»å¡è¯¥è¿›ç¨‹`timeout`é•¿æ—¶é—´ã€‚å¦‚æœ`timeout`æ—¶é—´ä»¥åé˜Ÿåˆ—è¿˜æ˜¯ç©ºçš„ï¼Œåˆ™äº§ç”Ÿå¼‚å¸¸`Queue.Empty` ã€‚`block` é»˜è®¤ `True` ï¼Œ`timeout` é»˜è®¤æ— ç©·å¤§(`None`)ã€‚`block` ä¸º `False`æ—¶ï¼Œå¦‚æœé˜Ÿåˆ—æ˜¯ç©ºçš„ç›´æ¥äº§ç”Ÿå¼‚å¸¸`Queue.Empty` ã€‚\n\n\n- `get_nowait()`\n\n  ç­‰ä»·äº `get(block=False)`.\n\nç”±äºé˜Ÿåˆ—ä¸­`feed`çº¿ç¨‹çš„å­˜åœ¨ï¼Œ`Queue`ä½¿ç”¨å¦‚ä¸‹ä¸‰ä¸ªå‡½æ•°æ¥å¯¹å…¶è¿›ç¨‹å¤„ç†ã€‚ï¼ˆæ ‡å‡†åº“ä¸­çš„é˜Ÿåˆ—æ²¡æœ‰å¦‚ä¸‹ä¸‰ä¸ªæ–¹æ³•ï¼‰\n\n- `close()`\n  è¯¥è¿›ç¨‹ä¸åœ¨å‘é˜Ÿåˆ—ä¸­å†™å…¥æ•°æ®ï¼Œå¹¶è°ƒç”¨`join_thread()`ï¼Œå°†bufferä¸­çš„æ•°æ®å†™å…¥`Pipe`\n  *æ³¨æ„ï¼šåªèƒ½åœ¨ç”Ÿäº§è€…ç«¯ä½¿ç”¨ã€‚å¦‚æœåœ¨æ¶ˆè´¹è€…ç«¯ä½¿ç”¨ï¼Œåˆ™æ¶ˆè´¹ä¹‹ä¸èƒ½ä»é˜Ÿåˆ—ä¸­getæ•°æ®ã€‚ä½†æ˜¯ç”Ÿäº§è€…ä»ç„¶å¯ä»¥å†™å…¥æ•°æ®ã€‚*\n\n\n- `join_thread()`\n\n  join `feed`çº¿ç¨‹ï¼Œç­‰å¾…bufferä¸­çš„æ•°æ®å†™å…¥Pipe\n\n\n- `cancel_join_thread()`\n\n  å‡è®¾æ¶ˆè´¹è€…ä¸åœ¨æ¶ˆè´¹æ•°æ®ï¼Œåˆ™ç”±äº`join_thread()`å¯èƒ½å¸¦æ¥ä¸€äº›æ­»é”é—®é¢˜ï¼Œå³ï¼ŒBufferçš„æ•°æ®æ— æ³•å†™å…¥Pipeä¸­ã€‚è¿™æ—¶å¯ä»¥ä½¿ç”¨ `cancel_join_thread()` æ¥ç»ˆæ­¢feedçº¿ç¨‹ã€‚æ³¨æ„ï¼š**æ­¤æ—¶bufferä¸­çš„æ•°æ®å°†ä¼šä¸¢å¤±**ã€‚\n\n\nå®ä¾‹ï¼š\n\n```python\nimport time\nfrom multiprocessing import Process, Queue, current_process\nfrom Queue import Empty\ndef produce(q):\n    for i in xrange(0, 20):\n        print \"process name: \" + current_process().name + ', put:' + str(i)\n        q.put(i)\n        time.sleep(0.1)\n    q.close()\n\n\ndef consum(q):\n    try_times = 0\n    while True:\n        try:\n            r = q.get(True, 1)\n            print \"process name: \" + current_process().name + ', get:' + str(r)\n        except Empty:\n            continue\n\n\nif __name__ == '__main__':\n    q = Queue(5)\n    producers = [Process(target=produce, name='p' + str(i), args=(q,)) for i in xrange(0, 3)]\n    consumers = [Process(target=consum, name='c' + str(i), args=(q,)) for i in xrange(0, 2)]\n    for c in consumers:\n        c.start()\n    for p in producers:\n        p.start()\n        p.join()\n    # ä¸è¦ç›´æ¥è¿™æ ·åšï¼Œæ­¤æ—¶è¿˜ä¸ç¡®å®šæ¶ˆè´¹ä¹‹æ˜¯å¦å·²ç»å¤„ç†å®Œæˆæ•°æ®ã€‚è¿˜æ˜¯æ¨èä½¿ç”¨JoinableQueue\n    for c in consumers:  \n        c.terminate()\n```\n\n### JoinableQueue\n\n`multiprocessing.Queue`æ˜¯æ¨¡ä»¿æ ‡å‡†åº“ä¸­çš„é˜Ÿåˆ—å†™çš„ï¼Œä½†æ˜¯æ²¡æœ‰`task_done()` å’Œ `join()` æ–¹æ³•ã€‚JoinableQueueç»§æ‰¿äº†`multiprocessing.Queue`å¹¶å®ç°äº†`task_done()` å’Œ `join()` æ–¹æ³•ã€‚\n\n`task_done()`\nç”±é˜Ÿåˆ—çš„æ¶ˆè´¹è€…è°ƒç”¨ã€‚æ¶ˆè´¹åˆ™è°ƒç”¨get()å¾—åˆ°ä¸€ä¸ªé˜Ÿåˆ—ä¸­çš„æ•°æ®(ä»»åŠ¡)ï¼Œå¤„ç†å®Œæˆè¿™ä¸ªæ•°æ®ä»¥åï¼Œè°ƒç”¨`task_done()`å‘Šè¯‰é˜Ÿåˆ—è¯¥ä»»åŠ¡å·²ç»å¤„ç†å®Œæ¯•ã€‚é˜Ÿåˆ—ä¸­çš„æ¯ä¸€ä¸ªå¯¹è±¡éƒ½å¯¹åº”ä¸€ä¸ª`task_done()`ã€‚\n\n`join()`\n\né˜»å¡è°ƒç”¨è¿›ç¨‹ï¼Œç›´åˆ°é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®(ä»»åŠ¡)è¢«æ¶ˆè´¹æ‰ã€‚å½“æœ‰æ•°æ®è¢«åŠ å…¥é˜Ÿåˆ—ï¼Œæœªå®Œæˆçš„ä»»åŠ¡æ•°å°±ä¼šå¢åŠ ã€‚å½“æ¶ˆè´¹è€…è°ƒç”¨task_done()ï¼Œæ„å‘³ç€æœ‰æ¶ˆè´¹è€…å–å¾—ä»»åŠ¡å¹¶å®Œæˆä»»åŠ¡ï¼Œæœªå®Œæˆçš„ä»»åŠ¡æ•°å°±ä¼šå‡å°‘ã€‚å½“æœªå®Œæˆçš„ä»»åŠ¡æ•°é™åˆ°0ï¼Œjoin()è§£é™¤é˜»å¡ã€‚\n\nå®ä¾‹\n\n```python\nimport time\nfrom multiprocessing import Process, JoinableQueue, current_process\nfrom Queue import Empty\n\ndef produce(q):\n    for i in xrange(0, 20):\n        print \"process name: \" + current_process().name + ', put:' + str(i)\n        q.put(i)\n        time.sleep(0.1)\n    q.close()\n\ndef consum(q):\n    try_times = 0\n    while True:\n        try:\n            r = q.get(True, 1)\n            print \"process name: \" + current_process().name + ', get:' + str(r)\n            q.task_done()  # æ•°æ®rå·²ç»å¤„ç†å®Œæ¯•\n        except Empty:\n            continue\n\nif __name__ == '__main__':\n    q = JoinableQueue(5)\n    producers = [Process(target=produce, name='p' + str(i), args=(q,)) for i in xrange(0, 3)]\n    consumers = [Process(target=consum, name='c' + str(i), args=(q,)) for i in xrange(0, 2)]\n    for c in consumers:\n        c.start()\n    for p in producers:\n        p.start()\n        p.join()\n    q.join()\n    print 'all task finished'\n    for c in consumers:\n        c.terminate()\n```\n\n# Synchronizationæœºåˆ¶\n\n(å¤šè¿›ç¨‹åŒæ­¥æ“ä½œå…±äº«èµ„æº)\n\n`Lock()`\nåœ¨å¤šè¿›ç¨‹pythonç¨‹åºä¸­åªæœ‰ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œï¼Œå¹¶é˜»å¡å…¶ä»–è¿›ç¨‹ã€‚\n\nåº”ç”¨åœºæ™¯ï¼šå½“å¤šä¸ªè¿›ç¨‹éœ€è¦æ“ä½œå…±äº«èµ„æºçš„æ—¶å€™ï¼Œä½¿ç”¨Lockæ¥é¿å…æ“ä½œçš„å†²çªã€‚\n\nå¦‚ä¸‹é¢ä¾‹å­ï¼Œ10ä¸ªè¿›ç¨‹åŒæ­¥å†™å…¥æ–‡ä»¶ã€‚å½“ä¸€ä¸ªè¿›ç¨‹è¿›ç¨‹å¼€å§‹å†™å…¥æ—¶ï¼ˆå³`lock.acquire()`æ‰§è¡Œåï¼‰ï¼Œé˜»å¡å…¶ä»–è¿›ç¨‹çš„æ“ä½œã€‚ç›´åˆ°è¯¥è¿›ç¨‹æ‰§è¡Œ`lock.release()` åï¼Œå…¶ä»–è¿›ç¨‹æ‰èƒ½è¿›è¡Œå†™å…¥ã€‚\n\n```python\nimport multiprocessing\nimport sys\n\ndef worker_with(lock, f, i):\n    with lock:\n        fs = open(f, \"a+\")\n        fs.write('Lock acquired via with ' + str(i) + '\\n')\n        fs.close()\n\ndef worker_no_with(lock, f, i):\n    lock.acquire()\n    try:\n        fs = open(f, \"a+\")\n        fs.write('Lock acquired via with ' + str(i) + '\\n')\n        fs.close()\n    finally:\n        lock.release()\n\nif __name__ == \"__main__\":\n    f = \"file.txt\"\n    lock = multiprocessing.Lock()\n    # writers = [multiprocessing.Process(target=worker_with, args=(lock, f, i)) for i in xrange(0, 10)]\n    # for w in writers:\n        # w.start()\n        # w.join()\n    writers = [multiprocessing.Process(target=worker_no_with, args=(lock, f, i)) for i in xrange(0, 10)]\n    for w in writers:\n        w.start()\n        w.join()\n```\n\n# è¿›ç¨‹é—´å…±äº«æ•°æ®\n\n## å…±äº«å†…å­˜\n\n![](multiprocessing-in-Python/share-data.png)\n\nåº”ç”¨åœºæ™¯ä¸¾ä¾‹ï¼šæ¯”å¦‚æˆ‘æƒ³ä½¿ç”¨ä¸€ä¸ªå¤šè¿›ç¨‹ç¨‹åºç»Ÿè®¡ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶çš„è¡Œæ•°ï¼ˆæ¯ä¸ªè¿›ç¨‹ä¸€æ¬¡ç»Ÿè®¡ä¸€ç¯‡æ–‡ç« çš„è¡Œæ•°ï¼‰ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¤šä¸ªè¿›ç¨‹åŒæ—¶è¯»å†™åŒä¸€ä¸ªèµ„æºã€‚\n\nPythonå¤šè¿›ç¨‹æœºåˆ¶ä½¿ç”¨çš„æ˜¯ä»å†…å­˜ä¸­ç”³è¯·ä¸€å—å†…å­˜ï¼Œè®©æ‰€æœ‰çš„è¿›ç¨‹èƒ½å¤ŸåŒæ—¶è¯»å†™è¿™å—å†…å®¹ã€‚ `multiprocessing`æä¾›äº†`multiprocessing.Value`  å’Œ`multiprocessing.Array` æ¥ä½œä¸ºå…±äº«å†…å­˜ã€‚\n\n`multiprocessing.Value`  å’Œ`multiprocessing.Array` æ˜¯è¿›ç¨‹å®‰å…¨çš„ï¼Œæ‰€ä»¥ä¸ç”¨ä½¿ç”¨`lock`ã€‚\n\nå®ä¾‹ï¼š\n\n```python\nfrom multiprocessing import Process, Value, Array\n\ndef f(n, a):\n    n.value += 1\n    print 'arr pre', arr[:]\n    for i in range(len(a)):\n        a[i] = -a[i]\n    print 'arr post', arr[:]\n\nif __name__ == '__main__':\n    num = Value('d', 0.0)\n    arr = Array('i', range(10))\n    ps = [Process(target=f, args=(num, arr)) for i in xrange(0, 10)]\n    for p in ps:\n        p.start()\n        p.join()\n    print num.value\n    print arr[:]\n```\n\næ³¨æ„ï¼šä¸Šé¢Valueå’ŒArrayçš„å®šä¹‰æ–¹å¼ã€‚Value å’Œ Array éƒ½éœ€è¦è®¾ç½®å…¶ä¸­å­˜æ”¾å€¼çš„ç±»å‹ï¼Œd æ˜¯ double ç±»å‹ï¼Œi æ˜¯ int ç±»å‹ï¼Œå…·ä½“çš„å¯¹åº”å…³ç³»åœ¨Python æ ‡å‡†åº“çš„ sharedctypes æ¨¡å—ä¸­æŸ¥çœ‹ã€‚å¦‚æœ‰éœ€è¦è¯·å‚è€ƒhttps://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.sharedctypesã€‚\n\n## å…±äº«è¿›ç¨‹\n\nä½¿ç”¨ä¸€ä¸ªè¿›ç¨‹æ¥ç®¡ç†éœ€è¦åœ¨å¤šè¿›ç¨‹ä¸­å…±äº«çš„æ•°æ®ã€‚å…¶ä»–è¿›ç¨‹å¯ä»¥å¯¹å…¶ç®¡ç†çš„æ•°æ®è¿›è¡Œæ“ä½œã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](multiprocessing-in-Python/share-process)\n\nä¸ºäº†ç†è§£multiprocessingä½¿ç”¨ä¸€ä¸ªè¿›ç¨‹æ¥å…±äº«æ•°æ®çš„æœºåˆ¶ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å¦‚ä¸‹å››è€…çš„å…³ç³»ï¼š\n\n> class multiprocessing.managers.BaseManager([address[, authkey]])\n>\n> Proxy \n>\n> class multiprocessing.managers.SyncManager  ï¼šæ˜¯ä¸€ä¸ªå·²ç»æ³¨å†Œäº†å¸¸ç”¨å…±äº«å¯¹è±¡çš„ BaseManagerï¼Œ æ˜¯BaseManagerçš„å­ç±»\n>\n> multiprocessing.Manager() ï¼šæ˜¯ä¸€ä¸ªèƒ½å¤Ÿè¿”å› å·²ç»startçš„ `SyncManager` å¯¹è±¡çš„å‡½æ•°\n\n\n\nBaseManageræ˜¯ç”¨æ¥ç®¡ç†å…±äº«å¯¹è±¡çš„è¿›ç¨‹ï¼Œé€šè¿‡å¯¹å¤–å¼€æ”¾ä»£ç† Proxy ä½¿å¾—å…¶ä»–è¿›ç¨‹åœ¨è¿›ç¨‹å®‰å…¨çš„æƒ…å†µä¸‹æ“ä½œå…±äº«å¯¹è±¡ã€‚\n\n`BaseManager` å…³é”®å‡½æ•°:\n\n```\nclass multiprocessing.managers.BaseManager([address[, authkey]])  # address BaseManagerè¿›ç¨‹è¿è¡Œçš„hostçš„ip:port ; authkey: å¯†ç \n```\nç”¨äºåˆå§‹åŒ– BaseManager å¯¹è±¡ã€‚\nè¯¥æ–¹æ³•éœ€è¦åœ¨ä¸¤ä¸ªåœ°æ–¹ä½¿ç”¨ï¼š\n1. Managerçš„åˆå§‹åŒ–ï¼Œåˆå§‹åŒ–ä¹‹åä½¿ç”¨ `start()` å¯åŠ¨ manager\n2. å½“æœ‰ä»¥ä¸ªè¿›ç¨‹éœ€è¦æ–¹ä½managerç®¡ç†çš„å¯¹è±¡æ—¶ï¼Œéœ€è¦ä½¿ç”¨è¯¥å‡½æ•°åˆå§‹åŒ– Managerï¼Œå…¶ address å’Œ authkey éœ€è¦å’Œè¢«è®¿é—®çš„ managerç›¸åŒã€‚åˆå§‹åŒ–åä½¿ç”¨ `connect()` è¿æ¥\n\n```\nstart([initializer[, initargs]])\n```\nStart a subprocess to start the manager. If initializer is not None then the subprocess will call initializer(*initargs) when it starts.\n\n```\nget_server()\n```\nè¿”å› manager å¯¹è±¡ï¼Œmanager å¯¹è±¡å¯ä»¥ä½¿ç”¨ serve_forever() å¯åŠ¨manager\n\n```\nconnect()\n```\nè¿æ¥ manager\n\n```\nshutdown()\n```\nStop the process used by the manager. This is only available if start() has been used to start the server process.\n\n\n```\nregister(typeid[, callable[, proxytype[, exposed[, method_to_typeid[, create_method]]]]])\n```\nç”¨äºæ³¨å†Œä»£ç† ã€‚æ³¨å†Œä¸€ä¸ªè·å–manageræ‰€ç®¡ç†å¯¹è±¡çš„proxyã€‚å…·ä½“ä½¿ç”¨æ–¹æ³•è§ä¾‹å­\n\ntypeidï¼šç”¨æˆ·è·å–è¢«ç®¡ç†å¯¹è±¡çš„proxy\n\ncallableæ˜¯ä¸€ä¸ªèƒ½å¤Ÿè¿”å›éœ€è¦ç®¡ç†å¯¹è±¡çš„å‡½æ•°\n\n\n\nå®ä¾‹ï¼šä½¿ç”¨ä¸€ä¸ªmasterè¿›ç¨‹åˆ†å‘ä»»åŠ¡ï¼Œslaveè¿›ç¨‹ç”¨äºå¤„ç†ä»»åŠ¡å¹¶å°†ä»»åŠ¡è¿”å›ã€‚masterå’Œslaveä½¿ç”¨manageré€šä¿¡\n\n```python\nimport random, time\nfrom Queue import Queue\nfrom multiprocessing.managers import BaseManager\n\nclass Master:\n    def __init__(self):\n        # æ´¾å‘å‡ºå»çš„ä½œä¸šé˜Ÿåˆ—\n        self.dispatched_job_queue = Queue()\n        # å®Œæˆçš„ä½œä¸šé˜Ÿåˆ—\n        self.finished_job_queue = Queue()\n\n    def get_dispatched_job_queue(self):\n        return self.dispatched_job_queue\n\n    def get_finished_job_queue(self):\n        return self.finished_job_queue\n\n    def start(self, tasks):\n        # æŠŠæ´¾å‘ä½œä¸šé˜Ÿåˆ—å’Œå®Œæˆä½œä¸šé˜Ÿåˆ—æ³¨å†Œåˆ°ç½‘ç»œä¸Š\n        BaseManager.register('get_dispatched_job_queue', callable=self.get_dispatched_job_queue)\n        BaseManager.register('get_finished_job_queue', callable=self.get_finished_job_queue)\n\n        # ç›‘å¬ç«¯å£å’Œå¯åŠ¨æœåŠ¡\n        manager = BaseManager(address=('localhost', 58881), authkey='jobs')\n        manager.start()\n        print('manager started...')\n\n        # ä½¿ç”¨ä¸Šé¢æ³¨å†Œçš„æ–¹æ³•è·å–é˜Ÿåˆ—\n        dispatched_jobs = manager.get_dispatched_job_queue()  # å®é™…ä¸Šæ˜¯ä¸€ä¸ª proxy\n        print(type(dispatched_jobs))\n        finished_jobs = manager.get_finished_job_queue()\n        \n        for t in TASKS:\n            print(t)\n            dispatched_jobs.put(t)\n\n        while not dispatched_jobs.empty():\n            res = finished_jobs.get()\n            print(res)\n        dispatched_jobs.put('EXIT')\n        time.sleep(1)\n        manager.shutdown()\n        \ndef mul(a, b):\n    time.sleep(0.5*random.random())\n    return a * b\n\ndef plus(a, b):\n    time.sleep(0.5*random.random())\n    return a + b\n\nTASKS = [(mul, (i, 7)) for i in range(10)] + \\\n        [(plus, (i, 8)) for i in range(10)]\n\nif __name__ == \"__main__\":\n    master = Master()\n    master.start(TASKS)\n```\n\n\n\n```python\nimport time, random\nfrom Queue import Queue\nfrom multiprocessing.managers import BaseManager\n\nclass Slave:\n\n    def __init__(self):\n        # æ´¾å‘å‡ºå»çš„ä½œä¸šé˜Ÿåˆ—\n        self.dispatched_job_queue = Queue()\n        # å®Œæˆçš„ä½œä¸šé˜Ÿåˆ—\n        self.finished_job_queue = Queue()\n\n    def start(self):\n        # æŠŠæ´¾å‘ä½œä¸šé˜Ÿåˆ—å’Œå®Œæˆä½œä¸šé˜Ÿåˆ—æ³¨å†Œåˆ°ç½‘ç»œä¸Š\n        BaseManager.register('get_dispatched_job_queue')\n        BaseManager.register('get_finished_job_queue')\n\n        # è¿æ¥master\n        server = 'localhost'\n        print('Connect to server %s...' % server)\n        manager = BaseManager(address=(server, 58881), authkey='jobs')\n        manager.connect()\n\n        # ä½¿ç”¨ä¸Šé¢æ³¨å†Œçš„æ–¹æ³•è·å–é˜Ÿåˆ—\n        dispatched_jobs = manager.get_dispatched_job_queue()\n        finished_jobs = manager.get_finished_job_queue()\n\n        # è¿è¡Œä½œä¸šå¹¶è¿”å›ç»“æœ\n        while True:\n            func_args = dispatched_jobs.get()\n            if func_args == 'EXIT':\n                break\n            func = func_args[0]\n            args = func_args[1]            \n            res = func(*args)\n            print(args, res)\n            finished_jobs.put(res)\n\n# ç”¨äºslaveå’Œmasteræœ‰æ—¶å€™æ˜¯åœ¨ä¸åŒ host (æˆ–è¿›ç¨‹)ä¸­å®Œæˆçš„ï¼Œæ‰€æœ‰mul å’Œpluséœ€è¦åœ¨masterå’Œslaveé—´å…±äº«\n# å®é™…åº”ç”¨ä¸­å¯ä»¥è®©masterå’Œslaveå…±äº« mulå’Œplus æ‰€åœ¨çš„æ¨¡å—çš„æ–¹æ³•è§£å†³\ndef mul(a, b):\n    time.sleep(0.5*random.random())\n    return a * b\n\ndef plus(a, b):\n    time.sleep(0.5*random.random())\n    return a + b\n            \nif __name__ == \"__main__\":\n    slave = Slave()\n    slave.start()\n```\n\n\n\n# è¿›ç¨‹æ± Pool\n\n## Pool\n\nå®šä¹‰ï¼š\n\n```python\nclass multiprocessing.Pool([processes[, initializer[, initargs[, maxtasksperchild]]]])\nprocesses:  # åŒæ—¶è¿è¡Œçš„è¿›ç¨‹(worker)æ•°é‡  \ninitializer:  # æ¯ä¸ªworkeråˆå§‹åŒ–å‡½æ•°\ninitargs:  # initializerçš„å‚æ•°\nmaxtasksperchild:  # æ¯ä¸ªworkeræ‰§è¡Œè¿™ä¹ˆå¤štaskåé€€å‡ºã€‚å› ä¸ºåœ¨Poolç»“æŸå‰ï¼Œæ‰€æœ‰çš„workeréƒ½æ˜¯liveçŠ¶æ€ï¼Œå ç”¨èµ„æºï¼Œé™åˆ¶è¯¥å€¼åï¼Œè®©workeré€€å‡ºï¼Œä»è€Œé‡Šæ”¾ä¸€å®šçš„èµ„æºï¼Œæˆ‘è‡ªå·±æµ‹è¯•åå‘ç°åœ¨æˆ‘çš„æœºå™¨ä¸Šæ²¡æœ‰ä»€ä¹ˆä½œç”¨\n```\n\nå…³é”®å‡½æ•°ï¼š\n\n`apply_async(func[, args[, kwds[, callback]]])`\ncallback: å›è°ƒå‡½æ•°ï¼Œå½“taskæ‰§è¡Œç»“æŸä»¥åï¼Œå¤„ç†è¿”å›å€¼ã€‚\nç”¨äºä¼ é€’ä¸å®šå‚æ•°ï¼Œå®ƒæ˜¯ *éé˜»å¡*çš„ä¸”æ”¯æŒç»“æœè¿”å›åè¿›è¡Œå›è°ƒã€‚\n\n`map(func, iterable[, chunksize])`\næ˜¯å†…ç½®å‡½æ•°mapçš„å¤šè¿›ç¨‹ç‰ˆæœ¬ï¼Œåœ¨æ‰€æœ‰çš„taskæ‰§è¡Œå®Œæˆä¹‹å‰ä¼š**é˜»å¡**ä¸»è¿›ç¨‹\n\n`map_async(func, iterable[, chunksize[, callback]])`\næ˜¯mapçš„å˜ä½“ï¼Œå®ƒæ˜¯ *éé˜»å¡*çš„ã€‚ä½†æ˜¯å¦‚æœä¼ å…¥äº†callbackï¼Œå½“å­è¿›ç¨‹ç»“æœreadyä»¥åï¼Œcallbackä¼šæ‰§è¡Œå¹¶é˜»å¡ä¸»è¿›ç¨‹(funcæ˜¯åœ¨å­è¿›ç¨‹ä¸­å¼‚æ­¥æ‰§è¡Œçš„)ã€‚callbackæ˜¯åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œçš„ã€‚\n\n`imap(func, iterable[, chunksize])`\nitertools.imap()çš„å¤šè¿›ç¨‹ç‰ˆæœ¬ã€‚è¿”å›IMapIteratorè¿­ä»£å™¨å¯¹è±¡\n\n`imap_unordered(func, iterable[, chunksize])`\nå’ŒimapåŠŸèƒ½ç›¸åŒï¼Œä½†æ˜¯å…¶ç»“æœæ˜¯æ— åºçš„ã€‚è¿”å›IMapUnorderedIterator è¿­ä»£å™¨å¯¹è±¡\n\n`close()`\næ‰§è¡Œè¯¥å‡½æ•°åï¼Œpoolä¸­ä¸èƒ½å†åŠ å…¥æ–°çš„task\n\n`terminate()`\nç›´æ¥ç»ˆæ­¢è¿›ç¨‹æ± ä¸­çš„æ‰€æœ‰taskï¼Œå¦‚æœæœ‰æœªæ‰§è¡Œç»“æŸçš„taskï¼Œå…¶ç»“æœå°†ä¼šä¸¢å¤±ã€‚\n\n`join()`\nç­‰å¾…æ‰€æœ‰taskç»“æŸï¼Œé˜»å¡ä¸»è¿›ç¨‹ã€‚æ‰§è¡Œ`join()`ä¹‹å‰å¿…é¡»å…ˆæ‰§è¡Œ`close()`å¦åˆ™ä¼šå‡ºé”™ã€‚\n\n## AsyncResult& MapResult\n\n```\nclass multiprocessing.pool.AsyncResult\n```\n\n`Pool.apply_async()` è¿”å›ç»“æœä¿å­˜åœ¨AsyncResultå¯¹è±¡ä¸­ï¼ŒAsyncResultæ¥æ”¶å¼‚æ­¥ç»“æœã€‚\n\n`get([timeout])`\nè·å–è¿›ç¨‹æ‰§è¡Œçš„ç»“æœï¼Œå¦‚æœç»“æœæ²¡æœ‰availableï¼Œåˆ™é˜»å¡ä¸»çº¿ç¨‹å¹¶ç›´åˆ° result is availableæˆ–è€…timeoutã€‚\n\n`wait([timeout])`\nç­‰å¾…è¿›ç¨‹è¿”å›ç»“æœã€‚ç­‰å¾…æ—¶é˜»å¡ä¸»çº¿ç¨‹ï¼Œç›´åˆ° result is availableæˆ–è€…timeoutã€‚\n\n`ready()`\nè¿”å›booleanå€¼ï¼Œè¡¨ç¤ºè¿›ç¨‹æ˜¯å¦å·²ç»è¿”å›ç»“æœ\n\n`successful()`\nå’Œread()ä½œç”¨ç›¸åŒï¼Œä½†æ˜¯å¦‚æœæœªreadyåˆ™ä¼šäº§ç”Ÿå¼‚å¸¸\n\n\n```\nclass MapResult(ApplyResult)\n```\n\n `Pool.map_async()`è¿”å›ç»“æœä¿å­˜åœ¨MapResultå¯¹è±¡ä¸­ï¼ŒMapResultç»§æ‰¿è‡ªApplyResultå¯¹å¤–æä¾›getã€waitã€readyå’Œsuccessfulå››ä¸ªæ–¹æ³•ã€‚MapResultæ¥æ”¶å¼‚æ­¥ç»“æœã€‚\n\nå…¶ä¸­`get()` è·å–çš„æ˜¯å¤šä¸ªè¿›ç¨‹ç»“æœç»„æˆçš„listçš„å¯¹è±¡ã€‚\n\n## IMapIterator&IMapUnorderedIterator \n\nimap,imap_unorderedè¿”å›çš„ç»“æœå¯¹è±¡\n\n`next([timeout])`\n\nç”¨äºè¿­ä»£è·å–ä¸‹ä¸€ä¸ªresult\n\n## å®ä¾‹ï¼š\n\n### close() ã€ terminate()ã€ join()çš„ä½¿ç”¨\n\n```python\nimport multiprocessing\nimport time\nimport random\nimport sys\n\ndef pow3(x):\n    time.sleep(0.01)\n    return x**3\nif __name__ == \"__main__\":\n  # Create pool\n  PROCESSES = 4\n  print 'Creating pool with %d processes\\n' % PROCESSES\n  pool = multiprocessing.Pool(PROCESSES)\n\n  print 'Testing close():'\n  results = [pool.apply_async(pow3, (i,)) for i in range(100)]\n  pool.close()\n  pool.join()\n  for r in results:\n      # print r.get()\n      assert r.get() is None\n  for worker in pool._pool:  # pool._pool æ˜¯è¿›ç¨‹Processçš„å¯¹è±¡é›†åˆ  # type(worker) == <class 'multiprocessing.process.Process'>\n      assert not worker.is_alive()\n  print '\\tclose() succeeded\\n'\n\n  print 'Testing terminate():'\n  pool = multiprocessing.Pool(2)\n  DELTA = 0.1\n  ignore = pool.apply(pow3, [2])\n  results = [pool.apply_async(time.sleep, [DELTA]) for i in range(100)]\n  pool.terminate()\n  pool.join()\n  for worker in pool._pool:\n      assert not worker.is_alive()\n  print '\\tterminate() succeeded\\n'\n```\n\n### apply_asyncã€ mapã€ imapã€ imap_unordered\n\n```python\nimport multiprocessing\nimport time\nimport random\nimport sys\n\n# Functions used by test code\ndef calculate(func, args):\n    result = func(*args)\n    return '%s says that %s%s = %s' % (\n        multiprocessing.current_process().name,\n        func.__name__, args, result\n        )\n\ndef calculatestar(args):\n    return calculate(*args)\n\ndef mul(a, b):\n    time.sleep(0.5*random.random())\n    return a * b\n\ndef plus(a, b):\n    time.sleep(0.5*random.random())\n    return a + b\n\nif __name__ == \"__main__\":\n    print 'cpu_count() = %d\\n' % multiprocessing.cpu_count()\n    # Create pool\n    PROCESSES = 4\n    pool = multiprocessing.Pool(PROCESSES)\n\n    # Tasks\n    TASKS = [(mul, (i, 7)) for i in range(10)] + \\\n            [(plus, (i, 8)) for i in range(10)]\n\n    results = [pool.apply_async(calculate, t) for t in TASKS]\n    print 'Ordered results using pool.apply_async():'\n    print type(results)\n    for r in results:\n        print type(r), '\\t', r.get()  # æ¯ä¸ªç»“æœéƒ½æ˜¯ ApplyResult å¯¹è±¡\n    print\n    imap_it = pool.imap(calculatestar, TASKS)\n    print 'Ordered results using pool.imap():'\n    print type(imap_it)# IMapIterator è¿­ä»£å™¨å¯¹è±¡\n    for x in imap_it:\n        print '\\t', x\n    print\n    imap_unordered_it = pool.imap_unordered(calculatestar, TASKS)\n    print 'Unordered results using pool.imap_unordered():'\n    print type(imap_unordered_it) # IMapUnorderedIterator è¿­ä»£å™¨å¯¹è±¡\n    for x in imap_unordered_it:\n        print '\\t', x\n    print\n    print 'Ordered results using pool.map() --- will block till complete:'\n    map_it = pool.map(calculatestar, TASKS) \n    print type(map_it)  # ç”±äºMapæ˜¯é˜»å¡çš„ï¼Œæ‰€æœ‰è¿”å›ç»“æœçš„ list\n    for x in map_it:\n        print '\\t', x\n    print\n```\n\n### imapã€ imap_unorderedç»“æœè¿­ä»£å™¨\n\n```python\nimport multiprocessing\nimport time\nimport random\nimport sys\n\n# Functions used by test code\ndef calculate(func, args):\n    result = func(*args)\n    return '%s says that %s%s = %s' % (\n        multiprocessing.current_process().name,\n        func.__name__, args, result\n        )\n\ndef calculatestar(args):\n    return calculate(*args)\n\ndef mul(a, b):\n    time.sleep(0.5*random.random())\n    return a * b\n\ndef plus(a, b):\n    time.sleep(0.5*random.random())\n    return a + b\n\nif __name__ == \"__main__\":\n    PROCESSES = 4\n    print 'Creating pool with %d processes\\n' % PROCESSES\n    pool = multiprocessing.Pool(PROCESSES)\n\n    # Tasks\n    TASKS = [(mul, (i, 7)) for i in range(10)] + \\\n            [(plus, (i, 8)) for i in range(10)]\n\n    print 'Testing IMapIterator.next() with timeout:',\n    it = pool.imap(calculatestar, TASKS)\n    while 1:\n        sys.stdout.flush()\n        try:\n            sys.stdout.write('\\n\\t%s' % it.next(0.01))\n        except StopIteration:\n            break\n        except multiprocessing.TimeoutError:\n            sys.stdout.write('.')\n\n    print 'Testing IMapUnorderedIterator.next() with timeout:',\n    it = pool.imap_unordered(calculatestar, TASKS)\n    while 1:\n        sys.stdout.flush()\n        try:\n            sys.stdout.write('\\n\\t%s' % it.next(0.01))\n        except StopIteration:\n            break\n        except multiprocessing.TimeoutError:\n            sys.stdout.write('.')\n```\n\n### å›è°ƒå‡½æ•°çš„ä½¿ç”¨\n\n```python\nimport multiprocessing\nimport time\nimport random\nimport sys\n\n# Functions used by test code\ndef mul(a, b):\n    time.sleep(0.5*random.random())\n    return a * b\n\ndef pow3(x):\n    return x**3\n\nif __name__ == \"__main__\":\n    # Create pool\n    PROCESSES = 4\n    print 'Creating pool with %d processes\\n' % PROCESSES\n    pool = multiprocessing.Pool(PROCESSES)\n    print 'pool = %s' % pool\n\n    # Testing callback\n    A = []\n    B = [56, 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]\n\n    # wait\n    r = pool.apply_async(mul, (7, 8), callback=A.append) # \n    r.wait() # ä½¿ç”¨waitç­‰å¾…å­è¿›ç¨‹çš„ç»“æœreadyã€‚\n    print 'try1:', A\n    r = pool.map_async(pow3, range(10), callback=A.extend)\n    r.wait()\n    print 'map_async result', r.get()\n    print 'try2:', A\n\n    print 'no wait ========='\n    A = []\n\n    ## no wait\n    r = pool.apply_async(mul, (7, 8), callback=A.append) # \n    print 'try1:', A\n    r = pool.map_async(pow3, range(10), callback=A.extend)\n    print 'try2:', A\n    for i in xrange(5):\n        print A\n        time.sleep(1)\n```\n\n# æ‚é¡¹\n\n- `multiprocessing.active_children()`\n\n  Return list of all live children of the current process.Calling this has the side effect of â€œjoiningâ€ any processes which have already finished.\n\n\n- `multiprocessing.cpu_count()`\n\n  Return the number of CPUs in the system. May raise `NotImplementedError`.\n\n\n- `multiprocessing.current_process()`\n  Return the `Process` object corresponding to the current process.\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[multiprocessingâ€” Process-based â€œthreadingâ€ interface](https://docs.python.org/2/library/multiprocessing.html)\n\nä½¿ç”¨è¿›ç¨‹å…±äº«å®ç°æœºå™¨ä¹‹é—´é€šä¿¡\n\nhttps://www.cnblogs.com/sherlockhomles/p/8421075.html","tags":["Python"],"categories":["Python"]},{"title":"ã€è½¬ã€‘gitå›¾è§£","url":"%2Fblog%2Fgit-illustration.html","content":"\n æœ¬æ–‡å†…å®¹[æ¥æº](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg) ï¼Œä¸ºæŸ¥æ‰¾æ–¹ä¾¿å’Œé˜²æ­¢ä¸¢å¤±ï¼Œç‰¹æ­¤å¤‡ä»½ã€‚\n\n\n\ngitåˆ†ä¸º å·¥ä½œç›®å½•ã€æš‚å­˜ç›®å½•(ä¹Ÿå«åšç´¢å¼•)å’Œä»“åº“ ä¸‰ä¸ªéƒ¨åˆ†ï¼Œgitå‘½ä»¤å°±æ˜¯å›´ç»•ç€ä¸‰ä¸ªéƒ¨åˆ†è¿›è¡Œçš„ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n  ![](git-illustration/basic-usage.svg.png)\n\nä¸Šé¢çš„å››æ¡å‘½ä»¤åœ¨å·¥ä½œç›®å½•ã€æš‚å­˜ç›®å½•(ä¹Ÿå«åšç´¢å¼•)å’Œä»“åº“ä¹‹é—´å¤åˆ¶æ–‡ä»¶ã€‚\n\n*   `git add files` æŠŠå½“å‰æ–‡ä»¶æ”¾å…¥æš‚å­˜åŒºåŸŸã€‚\n*   `git commit` ç»™æš‚å­˜åŒºåŸŸç”Ÿæˆå¿«ç…§å¹¶æäº¤ã€‚\n*   `git reset -- files` ç”¨æ¥å°†æš‚å­˜åŒºçš„æ–‡ä»¶æ¢å¤åˆ°æœ€åä¸€æ¬¡commitæ—¶çš„çŠ¶æ€ã€‚æˆ–è€…æ’¤é”€æœªæäº¤çš„ `git add` æ“ä½œã€‚\n*   `git checkout -- files` æŠŠæ–‡ä»¶ä»æš‚å­˜åŒºåŸŸå¤åˆ¶åˆ°å·¥ä½œç›®å½•ï¼Œç”¨æ¥ä¸¢å¼ƒæœ¬åœ°ä¿®æ”¹ã€‚\n\nä½ å¯ä»¥ç”¨ `git reset -p`, `git checkout -p`, or   `git add -p`è¿›å…¥äº¤äº’æ¨¡å¼ã€‚\n\nä¹Ÿå¯ä»¥è·³è¿‡æš‚å­˜åŒºåŸŸç›´æ¥ä»ä»“åº“å–å‡ºæ–‡ä»¶æˆ–è€…ç›´æ¥æäº¤ä»£ç ã€‚\n\n  ![](git-illustration/basic-usage-2.svg.png)\n\n*   `git commit -a ` ç›¸å½“äºè¿è¡Œ `git add` æŠŠæ‰€æœ‰å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶åŠ å…¥æš‚å­˜åŒºåŸŸå†è¿è¡Œ `git commit` ã€‚\n*   `git commit files` è¿›è¡Œä¸€æ¬¡åŒ…å«æœ€åä¸€æ¬¡æäº¤åŠ ä¸Šå·¥ä½œç›®å½•ä¸­æ–‡ä»¶å¿«ç…§çš„æäº¤ã€‚å¹¶ä¸”æ–‡ä»¶è¢«æ·»åŠ åˆ°æš‚å­˜åŒºåŸŸã€‚\n*   `git checkout HEAD -- files` å°†å·¥ä½œç›®å½•å’Œæš‚å­˜åŒºéƒ½åŒæ­¥åˆ°ä»“åº“æœ€åä¸€æ¬¡è¢«æäº¤çš„çŠ¶æ€ã€‚\n\n## çº¦å®š\n\nåæ–‡ä¸­ä»¥ä¸‹é¢çš„å½¢å¼ä½¿ç”¨å›¾ç‰‡ã€‚\n\n  ![](git-illustration/conventions.svg.png)\n\nç»¿è‰²çš„5ä½å­—ç¬¦è¡¨ç¤ºæäº¤çš„IDï¼Œåˆ†åˆ«æŒ‡å‘çˆ¶èŠ‚ç‚¹ã€‚åˆ†æ”¯ç”¨æ©˜è‰²æ˜¾ç¤ºï¼Œåˆ†åˆ«æŒ‡å‘ç‰¹å®šçš„æäº¤ã€‚å½“å‰åˆ†æ”¯ç”±é™„åœ¨å…¶ä¸Šçš„`HEAD`æ ‡è¯†ã€‚\nè¿™å¼ å›¾ç‰‡é‡Œæ˜¾ç¤ºæœ€å5æ¬¡æäº¤ï¼Œ`ed489`æ˜¯æœ€æ–°æäº¤ã€‚ `master`åˆ†æ”¯æŒ‡å‘æ­¤æ¬¡æäº¤ï¼Œå¦ä¸€ä¸ª`maint`åˆ†æ”¯æŒ‡å‘ç¥–çˆ¶æäº¤èŠ‚ç‚¹ã€‚\n\n## å‘½ä»¤è¯¦è§£\n\n### Diff\n\næœ‰è®¸å¤šç§æ–¹æ³•æŸ¥çœ‹ä¸¤æ¬¡æäº¤ä¹‹é—´çš„å˜åŠ¨ã€‚ä¸‹é¢æ˜¯ä¸€äº›ç¤ºä¾‹ã€‚\n\n```\ngit diff \ngit diff --cached\ngit diff HEAD\ngit diff maint\ngit diff b325c da985\n```\n\n  ![](git-illustration/diff.svg.png)\n\n### Commit\n\næäº¤æ—¶ï¼Œgitç”¨æš‚å­˜åŒºåŸŸçš„æ–‡ä»¶åˆ›å»ºä¸€ä¸ªæ–°çš„æäº¤ï¼Œå¹¶æŠŠæ­¤æ—¶çš„èŠ‚ç‚¹è®¾ä¸ºçˆ¶èŠ‚ç‚¹ã€‚ç„¶åæŠŠå½“å‰åˆ†æ”¯æŒ‡å‘æ–°çš„æäº¤èŠ‚ç‚¹ã€‚ä¸‹å›¾ä¸­ï¼Œå½“å‰åˆ†æ”¯æ˜¯`master`ã€‚\nåœ¨è¿è¡Œå‘½ä»¤ä¹‹å‰ï¼Œ`master`æŒ‡å‘`ed489`ï¼Œæäº¤åï¼Œ`master`æŒ‡å‘æ–°çš„èŠ‚ç‚¹`f0cec`å¹¶ä»¥`ed489`ä½œä¸ºçˆ¶èŠ‚ç‚¹ã€‚\n\n  ![](git-illustration/commit-master.svg.png)\n\nå³ä¾¿å½“å‰åˆ†æ”¯æ˜¯æŸæ¬¡æäº¤çš„ç¥–çˆ¶èŠ‚ç‚¹ï¼Œgitä¼šåŒæ ·æ“ä½œã€‚ä¸‹å›¾ä¸­ï¼Œåœ¨`master`åˆ†æ”¯çš„ç¥–çˆ¶èŠ‚ç‚¹`maint`åˆ†æ”¯è¿›è¡Œä¸€æ¬¡æäº¤ï¼Œç”Ÿæˆäº†`1800b`ã€‚\nè¿™æ ·ï¼Œ`maint`åˆ†æ”¯å°±ä¸å†æ˜¯`master`åˆ†æ”¯çš„ç¥–çˆ¶èŠ‚ç‚¹ã€‚æ­¤æ—¶ï¼Œ[åˆå¹¶](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#merge) (æˆ–è€… [è¡åˆ](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#rebase)) æ˜¯å¿…é¡»çš„ã€‚\n\n  ![](git-illustration/commit-maint.svg.png)\n\nå¦‚æœæƒ³æ›´æ”¹ä¸€æ¬¡æäº¤ï¼Œä½¿ç”¨  `git commit --amend`ã€‚gitä¼šä½¿ç”¨ä¸å½“å‰æäº¤ç›¸åŒçš„çˆ¶èŠ‚ç‚¹è¿›è¡Œä¸€æ¬¡æ–°æäº¤ï¼Œæ—§çš„æäº¤ä¼šè¢«å–æ¶ˆã€‚\n\n  ![](git-illustration/commit-amend.svg.png)\n\nå¦ä¸€ä¸ªä¾‹å­æ˜¯[åˆ†ç¦»HEADæäº¤](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#detached),åæ–‡è®²ã€‚\n\n### Checkout\n\ncheckoutå‘½ä»¤ç”¨äºä»å†å²æäº¤ï¼ˆæˆ–è€…æš‚å­˜åŒºåŸŸï¼‰ä¸­æ‹·è´æ–‡ä»¶åˆ°å·¥ä½œç›®å½•ï¼Œä¹Ÿå¯ç”¨äºåˆ‡æ¢åˆ†æ”¯ã€‚\n\nå½“ç»™å®šæŸä¸ªæ–‡ä»¶åï¼ˆæˆ–è€…æ‰“å¼€-pé€‰é¡¹ï¼Œæˆ–è€…æ–‡ä»¶åå’Œ-pé€‰é¡¹åŒæ—¶æ‰“å¼€ï¼‰æ—¶ï¼Œgitä¼šä»æŒ‡å®šçš„æäº¤ä¸­æ‹·è´æ–‡ä»¶åˆ°æš‚å­˜åŒºåŸŸå’Œå·¥ä½œç›®å½•ã€‚æ¯”å¦‚ï¼Œ`git checkout HEAD~ foo.c`ä¼šå°†æäº¤èŠ‚ç‚¹`HEAD~`(å³å½“å‰æäº¤èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹)ä¸­çš„`foo.c`å¤åˆ¶åˆ°å·¥ä½œç›®å½•å¹¶ä¸”åŠ åˆ°æš‚å­˜åŒºåŸŸä¸­ã€‚ï¼ˆå¦‚æœå‘½ä»¤ä¸­æ²¡æœ‰æŒ‡å®šæäº¤èŠ‚ç‚¹ï¼Œåˆ™ä¼šä»æš‚å­˜åŒºåŸŸä¸­æ‹·è´å†…å®¹ã€‚ï¼‰æ³¨æ„å½“å‰åˆ†æ”¯ä¸ä¼šå‘ç”Ÿå˜åŒ–ã€‚\n\n  ![](git-illustration/checkout-files.svg.png)\n\nå½“ä¸æŒ‡å®šæ–‡ä»¶åï¼Œè€Œæ˜¯ç»™å‡ºä¸€ä¸ªï¼ˆæœ¬åœ°ï¼‰åˆ†æ”¯æ—¶ï¼Œé‚£ä¹ˆ`HEAD`æ ‡è¯†ä¼šç§»åŠ¨åˆ°é‚£ä¸ªåˆ†æ”¯ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬â€œåˆ‡æ¢â€åˆ°é‚£ä¸ªåˆ†æ”¯äº†ï¼‰ï¼Œç„¶åæš‚å­˜åŒºåŸŸå’Œå·¥ä½œç›®å½•ä¸­çš„å†…å®¹ä¼šå’Œ`HEAD`å¯¹åº”çš„æäº¤èŠ‚ç‚¹ä¸€è‡´ã€‚æ–°æäº¤èŠ‚ç‚¹ï¼ˆä¸‹å›¾ä¸­çš„`a47c3`ï¼‰ä¸­çš„æ‰€æœ‰æ–‡ä»¶éƒ½ä¼šè¢«å¤åˆ¶ï¼ˆåˆ°æš‚å­˜åŒºåŸŸå’Œå·¥ä½œç›®å½•ä¸­ï¼‰ï¼›åªå­˜åœ¨äºè€çš„æäº¤èŠ‚ç‚¹ï¼ˆ`ed489`ï¼‰ä¸­çš„æ–‡ä»¶ä¼šè¢«åˆ é™¤ï¼›**ä¸å±äºä¸Šè¿°ä¸¤è€…çš„æ–‡ä»¶ä¼šè¢«å¿½ç•¥ï¼Œä¸å—å½±å“ã€‚**\n\n  ![](git-illustration/checkout-branch.svg.png)\n\nå¦‚æœæ—¢æ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œä¹Ÿæ²¡æœ‰æŒ‡å®šåˆ†æ”¯åï¼Œè€Œæ˜¯ä¸€ä¸ªæ ‡ç­¾ã€è¿œç¨‹åˆ†æ”¯ã€SHA-1å€¼æˆ–è€…æ˜¯åƒ_master~3_ç±»ä¼¼çš„ä¸œè¥¿ï¼Œå°±å¾—åˆ°ä¸€ä¸ªåŒ¿ååˆ†æ”¯ï¼Œç§°ä½œ`detached HEAD`ï¼ˆè¢«åˆ†ç¦»çš„`HEAD`æ ‡è¯†ï¼‰ã€‚è¿™æ ·å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨å†å²ç‰ˆæœ¬ä¹‹é—´äº’ç›¸åˆ‡æ¢ã€‚æ¯”å¦‚è¯´ä½ æƒ³è¦ç¼–è¯‘1.6.6.1ç‰ˆæœ¬çš„gitï¼Œä½ å¯ä»¥è¿è¡Œ`git checkout v1.6.6.1`ï¼ˆè¿™æ˜¯ä¸€ä¸ªæ ‡ç­¾ï¼Œè€Œéåˆ†æ”¯åï¼‰ï¼Œç¼–è¯‘ï¼Œå®‰è£…ï¼Œç„¶ååˆ‡æ¢å›å¦ä¸€ä¸ªåˆ†æ”¯ï¼Œæ¯”å¦‚è¯´`git checkout master`ã€‚ç„¶è€Œï¼Œå½“æäº¤æ“ä½œæ¶‰åŠåˆ°â€œåˆ†ç¦»çš„HEADâ€æ—¶ï¼Œå…¶è¡Œä¸ºä¼šç•¥æœ‰ä¸åŒï¼Œè¯¦æƒ…è§åœ¨[ä¸‹é¢](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#detached)ã€‚\n\n  ![](git-illustration/checkout-detached.svg.png)\n\n### HEADæ ‡è¯†å¤„äºåˆ†ç¦»çŠ¶æ€æ—¶çš„æäº¤æ“ä½œ\n\nå½“`HEAD`å¤„äºåˆ†ç¦»çŠ¶æ€ï¼ˆä¸ä¾é™„äºä»»ä¸€åˆ†æ”¯ï¼‰æ—¶ï¼Œæäº¤æ“ä½œå¯ä»¥æ­£å¸¸è¿›è¡Œï¼Œä½†æ˜¯ä¸ä¼šæ›´æ–°ä»»ä½•å·²å‘½åçš„åˆ†æ”¯ã€‚(ä½ å¯ä»¥è®¤ä¸ºè¿™æ˜¯åœ¨æ›´æ–°ä¸€ä¸ªåŒ¿ååˆ†æ”¯ã€‚)\n\n  ![](git-illustration/commit-detached.svg.png)\n\nä¸€æ—¦æ­¤åä½ åˆ‡æ¢åˆ°åˆ«çš„åˆ†æ”¯ï¼Œæ¯”å¦‚è¯´`master`ï¼Œé‚£ä¹ˆè¿™ä¸ªæäº¤èŠ‚ç‚¹ï¼ˆå¯èƒ½ï¼‰å†ä¹Ÿä¸ä¼šè¢«å¼•ç”¨åˆ°ï¼Œç„¶åå°±ä¼šè¢«ä¸¢å¼ƒæ‰äº†ã€‚æ³¨æ„è¿™ä¸ªå‘½ä»¤ä¹‹åå°±ä¸ä¼šæœ‰ä¸œè¥¿å¼•ç”¨`2eecb`ã€‚\n\n  ![](git-illustration/checkout-after-detached.svg.png)\n\nä½†æ˜¯ï¼Œå¦‚æœä½ æƒ³ä¿å­˜è¿™ä¸ªçŠ¶æ€ï¼Œå¯ä»¥ç”¨å‘½ä»¤`git checkout -b name`æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†æ”¯ã€‚\n\n  ![](git-illustration/checkout-b-detached.svg.png)\n\n### Reset\n\nresetå‘½ä»¤æŠŠå½“å‰åˆ†æ”¯æŒ‡å‘å¦ä¸€ä¸ªä½ç½®ï¼Œå¹¶ä¸”æœ‰é€‰æ‹©çš„å˜åŠ¨å·¥ä½œç›®å½•å’Œç´¢å¼•ã€‚ä¹Ÿç”¨æ¥åœ¨ä»å†å²ä»“åº“ä¸­å¤åˆ¶æ–‡ä»¶åˆ°ç´¢å¼•ï¼Œè€Œä¸åŠ¨å·¥ä½œç›®å½•ã€‚\n\nå¦‚æœä¸ç»™é€‰é¡¹ï¼Œé‚£ä¹ˆå½“å‰åˆ†æ”¯æŒ‡å‘åˆ°é‚£ä¸ªæäº¤ã€‚å¦‚æœç”¨`--hard`é€‰é¡¹ï¼Œé‚£ä¹ˆå·¥ä½œç›®å½•ä¹Ÿæ›´æ–°ï¼Œå¦‚æœç”¨`--soft`é€‰é¡¹ï¼Œé‚£ä¹ˆéƒ½ä¸å˜ã€‚\n\n  ![](git-illustration/reset-commit.svg.png)\n\nå¦‚æœæ²¡æœ‰ç»™å‡ºæäº¤ç‚¹çš„ç‰ˆæœ¬å·ï¼Œé‚£ä¹ˆé»˜è®¤ç”¨`HEAD`ã€‚è¿™æ ·ï¼Œåˆ†æ”¯æŒ‡å‘ä¸å˜ï¼Œä½†æ˜¯ç´¢å¼•ä¼šå›æ»šåˆ°æœ€åä¸€æ¬¡æäº¤ï¼Œå¦‚æœç”¨`--hard`é€‰é¡¹ï¼Œå·¥ä½œç›®å½•ä¹ŸåŒæ ·ã€‚\n\n  ![](git-illustration/reset.svg.png)\n\nå¦‚æœç»™äº†æ–‡ä»¶å(æˆ–è€… `-p`é€‰é¡¹), é‚£ä¹ˆå·¥ä½œæ•ˆæœå’Œå¸¦æ–‡ä»¶åçš„[checkout](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#checkout)å·®ä¸å¤šï¼Œé™¤äº†ç´¢å¼•è¢«æ›´æ–°ã€‚\n\n  ![](git-illustration/reset-files.svg.png)\n\n### Merge\n\nmerge å‘½ä»¤æŠŠä¸åŒåˆ†æ”¯åˆå¹¶èµ·æ¥ã€‚åˆå¹¶å‰ï¼Œç´¢å¼•å¿…é¡»å’Œå½“å‰æäº¤ç›¸åŒã€‚å¦‚æœå¦ä¸€ä¸ªåˆ†æ”¯æ˜¯å½“å‰æäº¤çš„ç¥–çˆ¶èŠ‚ç‚¹ï¼Œé‚£ä¹ˆåˆå¹¶å‘½ä»¤å°†ä»€ä¹ˆä¹Ÿä¸åšã€‚\n  å¦ä¸€ç§æƒ…å†µæ˜¯å¦‚æœå½“å‰æäº¤æ˜¯å¦ä¸€ä¸ªåˆ†æ”¯çš„ç¥–çˆ¶èŠ‚ç‚¹ï¼Œå°±å¯¼è‡´_fast-forward_åˆå¹¶ã€‚æŒ‡å‘åªæ˜¯ç®€å•çš„ç§»åŠ¨ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ–°çš„æäº¤ã€‚\n\n  ![](git-illustration/merge-ff.svg.png)\n\nå¦åˆ™å°±æ˜¯ä¸€æ¬¡çœŸæ­£çš„åˆå¹¶ã€‚é»˜è®¤æŠŠå½“å‰æäº¤(`ed489` å¦‚ä¸‹æ‰€ç¤º)å’Œå¦ä¸€ä¸ªæäº¤(`33104`)ä»¥åŠä»–ä»¬çš„å…±åŒç¥–çˆ¶èŠ‚ç‚¹(`b325c`)è¿›è¡Œä¸€æ¬¡[ä¸‰æ–¹åˆå¹¶](http://en.wikipedia.org/wiki/Three-way_merge)ã€‚ç»“æœæ˜¯å…ˆä¿å­˜å½“å‰ç›®å½•å’Œç´¢å¼•ï¼Œç„¶åå’Œçˆ¶èŠ‚ç‚¹`33104`ä¸€èµ·åšä¸€æ¬¡æ–°æäº¤ã€‚\n\n  ![](git-illustration/merge.svg.png)\n\n### Cherry Pick\n\ncherry-pickå‘½ä»¤\"å¤åˆ¶\"ä¸€ä¸ªæäº¤èŠ‚ç‚¹å¹¶åœ¨å½“å‰åˆ†æ”¯åšä¸€æ¬¡å®Œå…¨ä¸€æ ·çš„æ–°æäº¤ã€‚\n\n  ![](git-illustration/cherry-pick.svg.png)\n\n### Rebase\n\nè¡åˆæ˜¯åˆå¹¶å‘½ä»¤çš„å¦ä¸€ç§é€‰æ‹©ã€‚åˆå¹¶æŠŠä¸¤ä¸ªçˆ¶åˆ†æ”¯åˆå¹¶è¿›è¡Œä¸€æ¬¡æäº¤ï¼Œæäº¤å†å²ä¸æ˜¯çº¿æ€§çš„ã€‚è¡åˆåœ¨å½“å‰åˆ†æ”¯ä¸Šé‡æ¼”å¦ä¸€ä¸ªåˆ†æ”¯çš„å†å²ï¼Œæäº¤å†å²æ˜¯çº¿æ€§çš„ã€‚\n  æœ¬è´¨ä¸Šï¼Œè¿™æ˜¯çº¿æ€§åŒ–çš„è‡ªåŠ¨çš„ [cherry-pick](http://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg#cherry-pick)\n\n  ![](git-illustration/rebase.svg.png)\n\nä¸Šé¢çš„å‘½ä»¤éƒ½åœ¨`topic`åˆ†æ”¯ä¸­è¿›è¡Œï¼Œè€Œä¸æ˜¯`master`åˆ†æ”¯ï¼Œåœ¨`master`åˆ†æ”¯ä¸Šé‡æ¼”ï¼Œå¹¶ä¸”æŠŠåˆ†æ”¯æŒ‡å‘æ–°çš„èŠ‚ç‚¹ã€‚æ³¨æ„æ—§æäº¤æ²¡æœ‰è¢«å¼•ç”¨ï¼Œå°†è¢«å›æ”¶ã€‚\n\nè¦é™åˆ¶å›æ»šèŒƒå›´ï¼Œä½¿ç”¨`--onto`é€‰é¡¹ã€‚ä¸‹é¢çš„å‘½ä»¤åœ¨`master`åˆ†æ”¯ä¸Šé‡æ¼”å½“å‰åˆ†æ”¯ä»`169a6`ä»¥æ¥çš„æœ€è¿‘å‡ ä¸ªæäº¤ï¼Œå³`2c33a`ã€‚\n\n  ![](git-illustration/rebase-onto.svg.png)\n\nåŒæ ·æœ‰`git rebase --interactive`è®©ä½ æ›´æ–¹ä¾¿çš„å®Œæˆä¸€äº›å¤æ‚æ“ä½œï¼Œæ¯”å¦‚ä¸¢å¼ƒã€é‡æ’ã€ä¿®æ”¹ã€åˆå¹¶æäº¤ã€‚æ²¡æœ‰å›¾ç‰‡ä½“ç°è¿™äº›ï¼Œç»†èŠ‚çœ‹è¿™é‡Œ:[git-rebase(1)](http://www.kernel.org/pub/software/scm/git/docs/git-rebase.html#_interactive_mode)\n\n\n\n\nåŸæ–‡é“¾æ¥ï¼šhttp://marklodato.github.io/visual-git-guide/index-zh-cn.html?no-svg","tags":["git"],"categories":["git"]},{"title":"DSSMç›¸å…³è®ºæ–‡é˜…è¯»","url":"%2Fblog%2FDSSM.html","content":"\nDSSMæ˜¯åœ¨2013å¹´æå‡ºçš„ï¼Œåº”ç”¨åœºæ™¯æ˜¯IRä¸­çš„Ranké—®é¢˜ï¼Œå³ï¼Œç»™å®šä¸€ä¸ªqueryåï¼Œè®¡ç®—doc(url)ä¸è¿™ä¸ªqueryçš„è¯­ä¹‰ç›¸å…³æ€§ã€‚\n\n# DSSM\n\n## æ¨¡å‹ç»“æ„\n\nå…ˆä¸ŠDSSMçš„æ¨¡å‹ç»“æ„å›¾\n\n![DSSM](DSSM/DSSM-model.png)\n\nå¯ä»¥å°†æ¨¡å‹åˆ†ä¸ºä¸¤æ­¥ç†è§£ï¼š\n\n1. ç¬¬ä¸€æ­¥æ˜¯feature representationè¿‡ç¨‹ï¼Œä½¿ç”¨DNNç¥ç»ç½‘ç»œå°†æ–‡æœ¬çš„é«˜ç»´ç¨€ç–å‘é‡ æŠ•å½±åˆ°æ–‡æœ¬çš„ä½ç»´è¯­ä¹‰ç‰¹å¾å‘é‡ã€‚DNNç½‘ç»œæ¥æ”¶queryæˆ–è€…docçš„term vector(å¦‚one-hotç¼–ç )ï¼Œä¸ºäº†ä½¿one-hotç¼–ç è¿™ç§é«˜ç»´ç‰¹å¾èƒ½å¤Ÿåœ¨æ¨¡å‹ä¸­è®¡ç®—ï¼Œæ–‡ç« ä½¿ç”¨äº†ä¸€ä¸ªå°æŠ€å·§(Word Hashing)å°†é«˜ç»´ç¨€ç–ç‰¹å¾è¡¨ç¤ºä¸ºä½ç»´ç¨€ç–ç‰¹å¾ å¾—åˆ° $l_1$å±‚ã€‚ä»$l_1$åˆ°$y$æ˜¯æ ‡å‡†çš„ä¸‰å±‚DNNç¥ç»ç½‘ç»œã€‚$y$æ˜¯ä¸€ä¸ªå…·æœ‰128ç»´çš„ç‰¹å¾çš„å‘é‡ï¼Œå®ƒæ˜¯åŸå§‹çš„queryæˆ–è€…docçš„ä½ç»´è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚\n   å°†queryæˆ–è€…docçš„åŸå§‹Term Vectorè¡¨ç¤ºä¸º$x$ ï¼Œ$y$ä½œä¸ºDNNç½‘ç»œçš„è¾“å‡ºï¼Œ$l_i$ä½œä¸ºDNNç½‘ç»œçš„éšè—å±‚èŠ‚ç‚¹å€¼ï¼Œ$W_i$æ˜¯DNNç½‘ç»œçš„ç¬¬$i$å±‚çš„æƒé‡çŸ©é˜µï¼ŒDNNç½‘ç»œçš„å±‚æ•°è®°ä¸º$N$ã€‚é‚£ä¹ˆä»$x$åˆ°$y$çš„å‰é¦ˆè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\n   $$\n   \\begin {aligned}\n   l_1 &= W_1x \\\\\n   \\\\\n   l_i &=f(W_i l_{i-1}+b_i) , i =2,\\dots,N-1 \\\\\n   \\\\\n   y &=f(W_N l_{N-1}+b_N) \\\\\n   \\end {aligned}\n   $$\n\n   å…¶ä¸­$f$ä¸ºç¥ç»ç½‘ç»œä¸­éšè—å±‚å’Œè¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯$\\tanh$:\n   $$\n   f(x) = \\frac {1-e^{-2x}}{1+e^{-2x}}\n   $$\n\n2. ç¬¬äºŒæ­¥æ˜¯è¯­ä¹‰ç›¸å…³æ€§çš„è®¡ç®—è¿‡ç¨‹ï¼Œä½¿ç”¨ç¬¬ä¸€æ­¥å¾—åˆ°çš„queryå’Œdocçš„è¯­ä¹‰å‘é‡è¡¨ç¤º$y$è®¡ç®—queryå’Œdocçš„ç›¸å…³ç³»æ•°ã€‚è®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼š\n   $$\n   R(Q,D)=\\cos(y_Q,y_D) = \\frac{y_Q^{\\mathrm{T}}y_D}{\\Arrowvert y_Q\\Arrowvert \\Arrowvert y_D\\Arrowvert} \n   $$\n   å…¶ä¸­ $y_Q$ å’Œ $y_D$ åˆ†åˆ«è¡¨ç¤ºqueryå’Œdocçš„è¯­ä¹‰å‘é‡è¡¨ç¤º(ç¬¬ä¸€æ­¥çš„è¾“å‡º)ã€‚å½“ç»™å®šä¸€ä¸ªqueryå’Œå€™é€‰docåï¼Œå°±å¯ä»¥æŒ‰ç…§ä¸Šé¢çš„æ–¹æ³•è®¡ç®—å…¶ç›¸å…³æ€§å¾—åˆ†ï¼Œå¹¶æŒ‰ç…§ç›¸å…³æ€§å¤§å°æ’åº(ä¸ªäººæ„Ÿè§‰ç”Ÿäº§ç¯å¢ƒä¸‹ä¸ä¼šè¿™æ ·å¹²ï¼Œå› ä¸ºç”Ÿäº§ç¯å¢ƒä¸­é¢ä¸´çš„æƒ…å†µæ›´åŠ å¤šæ ·ã€‚å°†DSSMçš„ç›¸å…³æ€§å¾—åˆ†ä½œä¸ºæœ€ç»ˆltræ¨¡å‹çš„è¾“å…¥ç‰¹å¾è¿˜æ˜¯æ›´åŠ å¯è¡Œçš„)ã€‚\n\n\n## æŸå¤±å‡½æ•°\n\nä¸Šé¢ä»‹ç»çš„æ˜¯DSSMæ¨¡å‹çš„å‰é¦ˆç½‘ç»œç»“æ„ã€‚DSSMåšä¸ºä¸€ç§ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¿˜éœ€è¦æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•°ä½œä¸ºè®­ç»ƒçš„ç›®æ ‡ã€‚\n\nDSSMæ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªqueryåˆ—è¡¨å’Œæ¯ä¸ªqueryå¯¹åº”çš„ç”¨æˆ·ç‚¹å‡»çš„docåºåˆ—ã€‚è®ºæ–‡å¥–ç»™å®šä¸€ä¸ªquery $Q$ ,ç”¨æˆ·ç‚¹å‡»ä¸€ä¸ªç‰¹å®šdoc $D$ çš„æ¦‚ç‡å®šä¹‰ä¸ºï¼š\n$$\np(D|Q) = \\frac{\\exp(\\gamma R(Q,D))}{\\sum_{D^{'} \\in \\boldsymbol {D} } \\exp(\\gamma R(Q,D^{'} ))}\n$$\n\nå…¶ä¸­$\\gamma$æ˜¯ä¸€ä¸ªsoftmaxå‡½æ•°çš„å¹³æ»‘ç³»æ•°ï¼Œæ˜¯ä¸€ä¸ªç»éªŒå‚æ•°ã€‚\n\næ³¨æ„: åˆ†æ¯ä¸­æœ‰ä¸€ä¸ªç²—ä½“çš„$\\boldsymbol{D}$ è¡¨ç¤ºä¸$Q$å¯¹åº”çš„çš„æ‰€æœ‰docçš„é›†åˆ, è¯¥é›†åˆåŒ…å«å¦‚ä¸‹å†…å®¹ï¼š\n\n1. åœ¨è®­ç»ƒæ ·æœ¬ä¸­ä¸ $Q$ ç›¸å…³çš„æ‰€æœ‰docçš„é›†åˆ(æ­£ä¾‹)ï¼Œä¸‹æ–‡ä¸­ç”¨$D^+$è¡¨ç¤ºä¸ $Q$ ç›¸å…³çš„doc\n2. åœ¨è®­ç»ƒæ ·æœ¬ä¸­ï¼Œç›¸å¯¹äºæ¯ä¸ª $Q$ ,è¿˜éšæœºä»æ•°æ®åº“ä¸­é€‰æ‹©äº†4ä¸ªè´Ÿä¾‹(ä¸ $Q$ ä¸ç›¸å…³çš„doc)ï¼Œä¸‹æ–‡ä¸­ç”¨ $D^-$è¡¨ç¤º ã€‚\n\né›†åˆ $\\boldsymbol{D}$åŒ…å«ä¸$Q$å¯¹åº”çš„æ‰€æœ‰$D^+$ å’Œ$D^-$ ï¼Œå³ï¼š\n\n$$\nD^+ \\in \\boldsymbol{D} \\\\\nD^- \\in \\boldsymbol{D}\n$$\n\næ¨¡å‹çš„ç›®æ ‡å‡½æ•°å°±æ˜¯ç»™å®š$Q$æ—¶æ‰€æœ‰æ­£ä¾‹çš„æœ€å¤§ä¼¼ç„¶ã€‚æŸå¤±å‡½æ•°å–å…¶è´Ÿå¯¹æ•°ï¼š\n$$\n\\begin {aligned}\nL(\\Lambda) &= - \\log \\prod_{(Q,D^+)} p(D^+ | Q) \\\\\n                     &= - \\sum_{(Q,D^+)} log p(D^+ | Q)\n\\end {aligned}\n$$\n\n## Word Hashing\n\nç”±äºDSSMçš„è¾“å…¥æ•°æ®æ˜¯ one-hot ç¼–ç ï¼Œå¦‚æœç›´æ¥å°†å…¶æ”¾å…¥åœ£ç»ç½‘ç»œä¸­è®¡ç®—å°†ä¼šå¸¦æ¥ä¸¤ä¸ªé—®é¢˜:\n\n1.  vocabularyå¤ªå¤§ï¼Œè¾“å…¥å±‚çš„ç¨€ç–å‘é‡å°†ä¼šå…·æœ‰éå¸¸é«˜çš„ç»´åº¦ã€‚\n2.  ä¼šå‡ºç° oov (out of vocabulary)çš„é—®é¢˜\n\nWord Hashingçš„è®¾è®¡å°±æ˜¯ä¸ºäº†è§£å†³ä»¥ä¸Šé—®é¢˜çš„ã€‚ä»–æ˜¯åŸºäºå­—æ¯çº§åˆ«çš„n-gramè®¾è®¡çš„ã€‚å¦‚å•è¯ goodï¼š\n\n1. åœ¨å•è¯ä¸¤ç«¯åŠ å…¥ä¸´ç•Œç¬¦å·   #good# ã€‚\n2. æŒ‰ç…§n-gramå°†å…¶åˆ†å‰²ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¦‚trigrams  å°†å¾—åˆ°[#go, goo, ood, od#]å››ä¸ªéƒ¨åˆ†ã€‚\n3. å•è¯good å°†ç”¨[#go, goo, ood, od#] çš„å‘é‡è¡¨ç¤ºã€‚\n\nè¿™ç§æ–¹å¼å¯ä»¥éå¸¸æœ‰æ•ˆçš„è§£å†³ vocabulary å¤ªå¤§çš„é—®é¢˜ï¼Œå› æ­¤è‹±æ–‡å•è¯æ‰26ä¸ªï¼Œ3ä¸ªå­—æ¯çš„ç»„åˆéƒ½æ˜¯æœ‰é™çš„)ã€‚å¦å¤–ä¹Ÿä¸ä¼šå‡ºç° oov é—®é¢˜ï¼Œ ä¸¾ä¸ªä¾‹å­ï¼šdiscriminativeï¼Œdiscriminateï¼Œdiscrimination ä¸‰ä¸ªå•è¯çš„æ„æ€å¾ˆåƒï¼Œä»–ä»¬çš„Word Hashingä¸­ä¹Ÿæœ‰å¤§éƒ¨åˆ†æ˜¯ç›¸åŒçš„ã€‚\n\nè¿™æ ·ä¸¤ä¸ªä¸åŒçš„å•è¯ä¹Ÿæœ‰å¯èƒ½å…·æœ‰ç›¸åŒçš„tri-gramsï¼Œé’ˆå¯¹è¿™ä¸ªé—®é¢˜paperé‡Œé¢åšäº†ç»Ÿè®¡ï¼Œè¿™ä¸ªå†²çªçš„æ¦‚ç‡éå¸¸çš„ä½ï¼Œ500Kä¸ªwordå¯ä»¥é™åˆ°30kç»´ï¼Œå†²çªçš„æ¦‚ç‡ä¸º0.0044%ã€‚\n\nWord Hashingåœ¨è‹±æ–‡åœºæ™¯ä¸‹å¾ˆæœ‰æ•ˆï¼Œä½†æ˜¯ä¸­æ–‡åœºæ™¯ä¼°è®¡å°±æ²¡æœ‰è¿™æ ·çš„æ•ˆæœäº†å§ã€‚\n\n\n\n# CLSM\n\nCSSMçš„ä¸€ä¸ªé‡è¦ä¸è¶³æ˜¯å®ƒå°†æ•´ä¸ªquery/docä½œä¸ºä¸€ä¸ªbagå¤„ç†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°termçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸¢å¤±ã€‚å¦‚. â€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€å°†ä¼šæ˜ å°„åˆ°åŒä¸€ä¸ªy(ç‰¹å¾å‘é‡)ã€‚\n\nCLSM(Convolutional latent semantic model)äº2014å¹´æå‡ºï¼Œåˆç§°CDSSMã€‚å®ƒæ˜¯è§£å†³äº†CSSMçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜ã€‚åœ¨æ¨¡å‹ç»“æ„ä¸Šï¼Œå®ƒå°†DSSMçš„DNNæ¢æˆäº†CNNï¼Œè¿™ä¹Ÿæ˜¯å…¶åç§°ä¸­Convolutional çš„ç”±æ¥ã€‚\n\n## æ¨¡å‹ç»“æ„\n\n![](DSSM/CLSM-model.png)\n\nå…¥ä¸Šå›¾æ‰€ç¤ºï¼Œå¯¹äºä¸€ä¸ªquery/docçš„è¯­ä¹‰å‘é‡$y$ çš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºå¦‚ä¸‹5ä¸ªæ­¥éª¤ï¼š\n\n(0)  é¢„å¤„ç†é˜¶æ®µï¼šå°†å¥å­çš„å¼€å¤´å’Œç»“å°¾åŠ ä¸Špadingç¬¦ï¼Œ ä¸Šå›¾ä¸­çš„`<s>`\n\n(1)  a word-n-gram layer obtained by running a contextual sliding  window  over  the  input  word  sequence  (i.e.,  a query  or  a document),\næŒ‰ç…§å›ºå®šå¤§å°(ä¸Šå›¾ä¸­æ˜¯3)çš„æ»‘åŠ¨çª—å£ä»è¾“å…¥å¥å­ä¸­æå–word-n-gramå±‚\n\n(2)  $l_t$ :a  letter-trigram  layer  that  transforms  each  word-tri-gram  into  a  letter-trigram  representation  vector,  \nå°†word-n-gramå±‚çš„æ¯ä¸ªwordç”¨3-gramçš„word hashingè¡¨ç¤º(æ¯ä¸ªwordå¯ä»¥è¡¨ç¤ºä¸º30Kç»´çš„å‘é‡)ï¼Œword-n-gramå±‚($n=3$)ä¸­çš„æ¯ä¸ªbagå¯ä»¥è¡¨ç¤ºæˆ$30K \\times 3 = 90K$ç»´åº¦çš„å‘é‡\n\n(3)  $h_t$ :a convolutional layer that extracts contextual features for each word with  its neighboring words  defined by  a window\nä»¥å›ºå®šå¤§å°çš„çª—å£å¯¹letter-trigram  layeræ¯ä¸ªbagçš„90Kç»´åº¦çš„å‘é‡åšå·ç§¯ï¼Œæ¯ä¸ªbagæ˜ å°„åˆ°300ç»´çš„å‘é‡ä¸Šå¾—åˆ°convolutional layerã€‚å·å‡ å±‚çš„æ¿€æ´»å‡½æ•°ä½¿ç”¨$tanh$\n$$\ntanh(x) = \\frac {1-e^{-2x}}{1+e^{-2x}}\n$$\nåˆ°æ­¤æ—¶ä¸ºæ­¢word-n-gram layerã€letter-trigram  layerå’Œconvolutional layerä¸‰å±‚ï¼Œæ¯ä¸€å±‚åŒ…å«bagçš„æ•°é‡æ˜¯å’Œå¥å­çš„é•¿åº¦æœ‰å…³çš„ï¼Œå¥å­è¶Šé•¿bagæ•°é‡è¶Šå¤š\n\n(4)  $v$ :a max-pooling layer that discovers and combines salient word-n-gram  features  to  form  a  fixed-length  sentence-level feature vector, \n$$\nv(i) = \\max_{t=1,\\dots,T} \\{h_t(i)\\} , i=1,\\dots,K\n$$\nå…¶ä¸­$v(i)$ä¸º$v$çš„ç¬¬$i$ä¸ªç»´åº¦ï¼Œ$T$ä¸ºå·å‡ å±‚åŒ…å«çš„å‘é‡çš„ä¸ªæ•°ï¼ŒKä¸ºå·ç§¯å±‚æ¯ä¸ªå‘é‡çš„ç»´åº¦(éƒ½æ˜¯ä¸€æ ·çš„)ï¼ŒKä¹Ÿæ˜¯$v$çš„ç»´åº¦\n\n(5)  a semantic layer that extracts  a  high-level semantic  feature  vector  for  the  input  word  sequence\næœ€åé€šè¿‡ä¸€ä¸ªéçº¿æ€§æ˜ å°„å¾—åˆ°è¯­ä¹‰å‘é‡$y$ ï¼š\n$$\ny=\\tanh(W_s \\cdot v)\n$$\n\n## æŸå¤±å‡½æ•°\n\nä¸DSSMç›¸åŒ\n\n\n\n---\n\nåˆ†å‰²çº¿ï¼Œcodingå·¥ä½œç¹å¿™ï¼Œå…ˆå†™è¿™äº›ï¼Œå›å¤´åœ¨è¡¥ï¼\n\n# DSSM-LSTM \n\nå¦‚ä¸Šæ–‡æ‰€è¿°ï¼ŒDSSMä¸èƒ½å¤Ÿå¾ˆå¥½çš„æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ‰€æœ‰ä½¿ç”¨LSTMæ›¿æ¢DSSMçš„DNNä½¿å…¶èƒ½å¤Ÿæ•æ‰åˆ°æ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n\n\n\n\n\n# MV-DSSM\n\nè¯¥æ¨¡å‹ä¸»è¦æ˜¯ç”¨äºæ¨èç³»ç»Ÿã€‚\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\nDSSM: [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)\n\nCLSM: [A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf)\n\nDSSM-LSTM:[Semantic Modelling with Long-Short-Term Memory for Information Retrieval](https://arxiv.org/pdf/1412.6629.pdf)\n\nMV-DSSM:[A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems](http://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp1159-songA.pdf)","tags":["NLP"],"categories":["NLP"]},{"title":"å…³äºpythonä¸­çš„MROé—®é¢˜","url":"%2Fblog%2FMRO-in-python.html","content":"\nä»Šå¤©çªç„¶çœ‹åˆ°ä¸€ç¯‡æ–‡ç« [Python: super æ²¡é‚£ä¹ˆç®€å•](https://mozillazg.com/2016/12/python-super-is-not-as-simple-as-you-thought.html) ï¼Œå—å…¶å¯å‘ï¼Œç„¶åæŸ¥é˜…äº†ä¸€äº›å…¶ä»–æ–‡ç« ï¼Œæ•´ç†å¦‚ä¸‹ã€‚\n\n# é—®é¢˜çš„äº§ç”Ÿ\n\npythonæ˜¯ä¸€ä¸ªå¯ä»¥å¤šé‡ç»§æ‰¿çš„ç¼–ç¨‹è¯­è¨€ï¼Œæœ¬æ–‡ä»¥å¦‚ä¸‹å¤šé‡ç»§æ‰¿çš„äº‹ä¾‹æå‡ºé—®é¢˜ã€‚\n\n```python\nclass A(object):\n    def __init__(self):\n        self.n = 2\n\n    def add(self, m):\n        print('self is {0} @A.add'.format(self))\n        self.n += m\n\n\nclass B(A):\n    def __init__(self):\n        self.n = 3\n\n    def add(self, m):\n        print('self is {0} @B.add'.format(self))\n        super(B, self).add(m)\n        self.n += 3\n\n        \nclass C(A):\n    def __init__(self):\n        self.n = 4\n\n    def add(self, m):\n        print('self is {0} @C.add'.format(self))\n        super(C, self).add(m)\n        self.n += 4\n\n\nclass D(B, C):\n    def __init__(self):\n        self.n = 5\n\n    def add(self, m):\n        print('self is {0} @D.add'.format(self))\n        super(D, self).add(m)\n        self.n += 5\n\n\nd = D()\nd.add(2)\nprint(d.n)\n```\n\nç»“æœå¦‚ä¸‹ï¼š\n\n```shell\nself is <__main__.D object at 0x10ce10e48> @D.add\nself is <__main__.D object at 0x10ce10e48> @B.add\nself is <__main__.D object at 0x10ce10e48> @C.add\nself is <__main__.D object at 0x10ce10e48> @A.add\n19\n```\n\nä»å­ç±»Då¼€å§‹ï¼Œè°ƒç”¨é¡ºä¸ºä»€ä¹ˆä¼šæ˜¯D->B->C->Aï¼Ÿ\n\nåœ¨Bä¸­æ‰§è¡Œäº†`super(C, self).add(m)`ï¼ŒBçš„çˆ¶ç±»æ˜¯Aï¼Œä¸ºä»€ä¹ˆå´å…ˆæ‰§è¡Œäº†Cä¸­çš„addæ–¹æ³•ï¼Ÿ\n\nå¸¦ç€è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›å…¥æ­£é¢˜ã€‚\n\n# MRO\n\nåœ¨æ­£å¼è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ä¹‹å‰ï¼Œå…ˆä»‹ç»ä¸€ä¸ªåŸºç¡€çŸ¥è¯†ã€‚ç”±äºpythonçš„è¿™ç§å¤šé‡ç»§æ‰¿æœºåˆ¶ï¼Œå¤šç»§æ‰¿çš„è¯­è¨€å¾€å¾€ä¼šé‡åˆ°ä»¥ä¸‹ä¸¤ç±»äºŒä¹‰æ€§çš„é—®é¢˜ï¼š\n\n1. æœ‰ä¸¤ä¸ªåŸºç±»Aå’ŒBï¼ŒAå’ŒBéƒ½å®šä¹‰äº†æ–¹æ³•f()ï¼ŒCç»§æ‰¿Aå’ŒBï¼Œé‚£ä¹ˆè°ƒç”¨Cçš„f()æ–¹æ³•æ—¶ä¼šå‡ºç°ä¸ç¡®å®šã€‚\n2. æœ‰ä¸€ä¸ªåŸºç±»Aï¼Œå®šä¹‰äº†æ–¹æ³•f()ï¼ŒBç±»å’ŒCç±»ç»§æ‰¿äº†Aç±»ï¼ˆçš„f()æ–¹æ³•ï¼‰ï¼ŒDç±»ç»§æ‰¿äº†Bå’ŒCç±»ï¼Œé‚£ä¹ˆå‡ºç°ä¸€ä¸ªé—®é¢˜ï¼ŒDä¸çŸ¥é“åº”è¯¥ç»§æ‰¿Bçš„f()æ–¹æ³•è¿˜æ˜¯Cçš„f()æ–¹æ³•ã€‚\n\nä¸ºäº†è§£å†³è¿™ç§äºŒä¹‰æ€§é—®é¢˜ï¼Œpythonä½¿ç”¨MRO(Method Resolution Order)æ¥å¤„ç†ã€‚\n\n**MROæ˜¯ä¸€ä¸ªæœç´¢åˆ—è¡¨ï¼Œå¦‚æœä¸€ä¸ªç±»Cä¸­çš„æ–¹æ³•foo()æ˜¯ä»ä»–çš„ç¥–å…ˆç±»ä¸­ç»§æ‰¿æ¥çš„ï¼Œå½“æˆ‘ä»¬è¦è°ƒç”¨Cçš„è¿™ä¸ªç»§æ‰¿æ¥æ–¹æ³•foo()çš„æ—¶å€™ï¼Œpythonéœ€è¦æŒ‰ç…§ä¸€å®šçš„é¡ºåºéå†å…¶ç¥–å…ˆç±»æ¥æ‰¾åˆ°è¿™ä¸ªæ–¹æ³•çš„å…·ä½“å®ç°ã€‚ MROå°±æ˜¯è¿™ä¹ˆä¸€ä¸ªæŸ¥æ‰¾é¡ºåºçš„åˆ—è¡¨ã€‚æŒ‰ç…§è¿™ä¸ªé¡ºåºé€ä¸€æŸ¥çœ‹å„ç¥–å…ˆç±»ï¼Œæ‰¾åˆ°çš„ ç¬¬ä¸€ä¸ªå®ç°äº†çš„foo() å°†ä½œä¸ºCçš„foo()æ–¹æ³•ã€‚**\n\nåœ¨pythonå†å²ä¸Šï¼ŒMROä½¿ç”¨è¿‡DFS(Depth-First-Search)ã€BFS(Breadth-First-Search)å’ŒC3ç®—æ³•å®ç°ã€‚ä»python2.3å¼€å§‹éƒ½æ˜¯ä½¿ç”¨C3å®ç°çš„MROã€‚åœ¨python2.xä¸­å¯ä»¥é€šè¿‡`D__mro__` æ¥æ‹¿åˆ°Dçš„MROåˆ—è¡¨ `(__main__.D, __main__.B, __main__.C, __main__.A, object)` ã€‚\n\nC3ç®—æ³•æ˜¯ä¸€ç§ä½¿ç”¨å›¾è®ºçš„æ‹“æ‰‘æ’åºæ–¹æ³•ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¦‚æœæœ‰å¦‚ä¸‹çš„ç±»ç»§æ‰¿å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‹“æ‰‘æ’åºç®—æ³•æ¥å¾—åˆ°ç±»Açš„MROåˆ—è¡¨ã€‚\n\n![](MRO-in-python/C3.png)\n\n1. æ‰¾å…¥åº¦ä¸º0çš„ç‚¹ï¼Œåªæœ‰ä¸€ä¸ªAï¼ŒæŠŠAæ‹¿å‡ºæ¥ï¼ŒåŠ å…¥åˆ—è¡¨ï¼›                                               -------> [A]\n2. æŠŠAç›¸å…³çš„è¾¹å‰ªæ‰ï¼Œå†æ‰¾ä¸‹ä¸€ä¸ªå…¥åº¦ä¸º0çš„ç‚¹ï¼Œæœ‰ä¸¤ä¸ªç‚¹(B, C)ï¼Œå–æœ€å·¦åŸåˆ™ï¼Œæ‹¿Bï¼›-------> [A, B]\n3. å‰ªæ‰ä¸Bç›¸å…³çš„è¾¹ï¼Œè¿™æ—¶å€™å…¥åº¦ä¸º0çš„ç‚¹æœ‰Eå’ŒCï¼Œå–æœ€å·¦ï¼›                                          -------> [A, B, E]\n4. å‰ªæ‰ä¸Eç›¸å…³çš„è¾¹ï¼Œè¿™æ—¶åªæœ‰Cç‚¹å…¥åº¦ä¸º0ï¼Œå–Cï¼›                                                          -------> [A, B, E, C]\n5. å‰ªæ‰ä¸Cç›¸å…³çš„è¾¹ï¼Œå¾—åˆ°ä¸¤ä¸ªå…¥åº¦ä¸º0çš„ç‚¹ï¼ˆD, Fï¼‰ï¼Œå–æœ€å·¦Dï¼›                                  -------> [A, B, E, C, D] \n6. ç„¶åå‰ªDç›¸å…³çš„è¾¹ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªå…¥åº¦ä¸º0çš„å°±æ˜¯Fï¼Œç„¶åæ˜¯objectã€‚é‚£ä¹ˆæœ€åçš„æ’åºå°±ä¸º[A, B, E, C, D, F, object] ã€‚\n\n# super()çš„ä½œç”¨\n\nå½“æˆ‘ä»¬è°ƒç”¨ `super()` çš„æ—¶å€™ï¼Œå®é™…ä¸Šæ˜¯å®ä¾‹åŒ–äº†ä¸€ä¸ª `super` ç±»ã€‚\n\nåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ `super` åŒ…å«äº†ä¸¤ä¸ªéå¸¸é‡è¦çš„ä¿¡æ¯: ä¸€ä¸ª MROåˆ—è¡¨ä»¥åŠ MRO ä¸­çš„ä¸€ä¸ªç±»ã€‚\n\nå¦‚ï¼šåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­self æŒ‡çš„æ˜¯å¯¹è±¡d(Dçš„å®ä¾‹)ï¼Œå®ä¾‹`super(B, self)` å¯ä»¥é€šè¿‡`type(self).__mro__`å¾—åˆ°MROåˆ—è¡¨`(D, B, C, A, object)`ï¼ŒBæŒ‡ç¤ºäº†å½“å‰çš„ç±»Båœ¨MROåˆ—è¡¨ä¸­çš„ä½ç½®ã€‚\n\nå½“åœ¨ç±»Bä¸­è°ƒç”¨`super(B, self).add(m)`æ—¶ï¼ŒPythonä¸ªè§£é‡Šå™¨å°†ä¼šæŒ‰ç…§MROåˆ—è¡¨ä¸­ä»Bå¼€å§‹å‘åæŸ¥æ‰¾å«æœ‰addæ–¹æ³•çš„ç±»ï¼Œå³æŒ‰ç…§`C, A, object`çš„é¡ºåºã€‚\n\né€šè¿‡ä»¥ä¸Šè§£é‡Šä¹Ÿå°±å†³äº†å¼€å¤´æå‡ºçš„ä¸¤ä¸ªé—®é¢˜ã€‚\n\n# å¤šé‡ç»§æ‰¿äºŒä¹‰æ€§ä¸¾ä¾‹\n\né™¤äº†æ˜¾ç¤ºçš„ä½¿ç”¨superä»¥å¤–ï¼Œå¦‚æœå­ç±»Dçš„å¤šä¸ªçˆ¶ç±»éƒ½å®ç°äº†åŒä¸€ä¸ªæ–¹æ³•addï¼Œé‚£ä¹ˆè°ƒç”¨å­ç±»Dçš„addæ–¹æ³•æ—¶åˆ°åº•è°ƒç”¨äº†å“ªä¸€ä¸ªçˆ¶ç±»ä¸­å®ç°çš„addæ–¹æ³•å‘¢ï¼Ÿå…¶å®Pythonä¹Ÿæ˜¯æŒ‰ç…§MROçš„é¡ºåºæ¥ç¡®å®šçš„ã€‚\n\n```python\nclass A(object):\n    def __init__(self):\n        self.n = 2\n\nclass B(A):\n    def __init__(self):\n        self.n = 3        \n        \n    def add(self):\n        print('self is {0} @B.add'.format(self))\n        self.n += 3\n        \nclass C(A):\n    def __init__(self):\n        self.n = 4\n        \n    def add(self):\n        print('self is {0} @C.add'.format(self))\n        self.n += 4\n  \n\nclass D(C, B):\n    def __init__(self):\n        self.n = 5\n        \nclass E(B, C):\n    def __init__(self):\n        self.n = 5\n\nd = D()\nd.add()\nprint d.n   #  9\n\ne = E()\ne.add()\nprint e.n  #  8\n```\n\né‡æ–°ä¿®æ”¹æ–‡ç« å¼€å¤´éƒ¨åˆ†çš„ä»£ç ï¼Œå¦‚ä¸Šæ‰€ç¤ºï¼Œæ­¤æ—¶ï¼ŒDåŒæ—¶ç»§æ‰¿äº†Bå’ŒCï¼ŒEåŒæ—¶ç»§æ‰¿äº†Cå’ŒBã€‚\n\næŒ‰ç…§MROåˆ—è¡¨ï¼Œd.add()è°ƒç”¨çš„æ˜¯ç±»Cä¸­å®šä¹‰çš„addæ–¹æ³•ã€‚  (D, B, C, A, object)\n\ne.add()è°ƒç”¨çš„æ˜¯ç±»Bä¸­å®šä¹‰çš„addæ–¹æ³•ã€‚(D, C, B, A, object) \n\n\n\n# å‚è€ƒèµ„æ–™\n\n[Python: super æ²¡é‚£ä¹ˆç®€å•](https://mozillazg.com/2016/12/python-super-is-not-as-simple-as-you-thought.html) \n\n[ä½ çœŸçš„ç†è§£Pythonä¸­MROç®—æ³•å—ï¼Ÿ](http://python.jobbole.com/85685/)","tags":["Python"],"categories":["Python"]},{"title":"å°†ä¸€å°æœºå™¨è¿æ¥åˆ°github","url":"%2Fblog%2Fmachine-connect-github.html","content":"\nå½“æˆ‘ä»¬æœ‰ä¸€å°æ–°çš„æœºå™¨ä»¥åï¼Œå¸Œæœ›å’Œä¸ªäººgithubå»ºç«‹sshè¿æ¥ï¼Œå¹¶ä¸”å’Œå…¶ä¸­çš„æŸä¸€ä¸ªrepoè¿æ¥åˆ°ä¸€èµ·ã€‚è¿‡ç¨‹è™½ç„¶ç®€å•ï¼Œä½†æ˜¯æœ‰æ—¶å€™ä¹Ÿä¼šå¿˜æ‰ï¼Œæ¯•ç«Ÿè¿™ä¸ªåŠ¨ä½œä¸æ˜¯å¤ªå¸¸ç”¨ã€‚æœ¬æ–‡å°†è¿™ä¸ªè¿‡ç¨‹è®°å½•ä¸‹æ¥ï¼Œå¤‡æŸ¥ã€‚\n\n1. åœ¨æ–°æœºå™¨ä¸Šç”Ÿæˆä¸€ä¸ªsshç§˜é’¥å’Œå…¬é’¥\n\n   ```shell\n   ssh-keygen -t rsa -C \"weirping@work-VM\"  # -t åŠ å¯†ç®—æ³•ï¼Œ -C comment\n   ```\n\n2. å°†ç”Ÿæˆçš„å…¬é’¥å¤åˆ¶åˆ°githubä¸­ï¼Œè¿™é‡Œæœ‰ä¸¤ä¸­æ–¹æ¡ˆ\n\n   - å…¨å±€ï¼šè¯¥æœºå™¨å¯ä»¥ç®¡ç†githubè´¦æˆ·ä¸­çš„æ‰€æœ‰repo\n     åœ¨ https://github.com/settings/profile ä¸­çš„ â€œSSH and GPG keysâ€ä¸­æ·»åŠ \n   - å•ä¸ªrepo: è¯¥æœºå™¨åªèƒ½ç®¡ç†ç‰¹å®šçš„repo\n     åœ¨repoçš„settings->Deploy keys ä¸­æ·»åŠ \n\n3. åœ¨æœºå™¨ä¸Šå°†githubä¸­çš„ä»£ç cloneåˆ°æœ¬åœ°\n\n   ```shell\n   git clone git@github.com:Weirping/configs-bak.git\n   ```\n\n4. å¦‚æœæœ¬åœ°å·²ç»å­˜åœ¨ä¸€ä¸ªæœ¬åœ°repoï¼Œgithubè´¦æˆ·ä¸­æ˜¯ä¸€ä¸ªç©ºrepoã€‚æ€ä¹ˆå°†æˆ‘æœ¬åœ°çš„repoåŒæ­¥åˆ°çº¿ä¸Šå‘¢ï¼Ÿ\n\n   - æœ¬åœ°å»ºç«‹repoï¼Œ git init -> git add  [file] -> git commit -m \"\" \n\n   - å°†æœ¬åœ°ä»“åº“å’Œçº¿ä¸Šä»“åº“å…³è” \n\n     ```shell\n     git remote add origin git@github.com:Weirping/configs-bak.git\n     ```\n\n   - æ•°æ®åŒæ­¥\n\n     ```shell\n     git pull origin master # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå¦åˆ™å¯èƒ½æ— æ³•push\n     git push origin master \n     ```\n\n     å› ä¸ºä»–ä»¬ï¼ˆæœ¬åœ°çš„å’Œè¿œç¨‹çš„ï¼‰æ˜¯ä¸¤ä¸ªä¸åŒçš„é¡¹ç›®ï¼Œpullæ—¶å¯èƒ½æŠ¥å¦‚ä¸‹é”™è¯¯: refusing to merge unrelated historiesã€‚\n\n     è¦æŠŠä¸¤ä¸ªä¸åŒçš„é¡¹ç›®åˆå¹¶ï¼Œéœ€è¦æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š\n\n     ```shell\n     git pull origin master --allow-unrelated-histories\n     ```\n\n     â€‹","tags":["git"],"categories":["git"]},{"title":"ä»Lagrange multiplieråˆ°KKTæ¡ä»¶","url":"%2Fblog%2Ffrom-Lagrange-Multiplier-to-KKT-Conditions.html","content":"\næœ€è¿‘çœ‹ä¹¦çš„è¿‡ç¨‹ä¸­é‡åˆ°äº†å‡ æ¬¡éœ€è¦ç”¨åˆ°KKTæ¡ä»¶çš„åœ°æ–¹ï¼Œæ¯æ¬¡éƒ½è¦é‡æ–°ç¿»ä¹¦ã€‚æœ¬æ–‡å•ç‹¬å°†è¿™ä¸€å—çš„çŸ¥è¯†ç‚¹æ‹‰å‡ºæ¥åšä¸€ä¸‹ç¬”è®°ï¼Œå¤‡æŸ¥ã€‚\n\n*note : æœ¬æ–‡ä¸­çš„æ‰€æœ‰å‘é‡é»˜è®¤éƒ½æ˜¯åˆ—å‘é‡*\n\n# æ¦‚è¿°\n\nLagrange multiplierå’ŒKKTéƒ½æ˜¯ç”¨æ¥è§£å†³éçº¿æ€§è§„åˆ’é—®é¢˜çš„ã€‚éçº¿æ€§è§„åˆ’çš„æ•°å­¦æ¨¡å‹å¦‚ä¸‹ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\  f(x) \\\\\ns.t. & \\  h_i(x) = 0 (i=1,2,\\dots ,m) \\\\\n     & \\  g_i(x) \\ge 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right .\n$$\nå¯ä»¥çœ‹å‡ºåœ¨è¿™ä¸ªé—®é¢˜ä¸­å­˜åœ¨ä¸¤ç§çº¦æŸï¼šç­‰å¼çº¦æŸå’Œä¸ç­‰å¼çº¦æŸ ã€‚æŠŠæ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶çš„ç‚¹ç§°åš**å¯è¡Œç‚¹** ( å¯è¡Œè§£ ) , æ‰€æœ‰å¯è¡Œç‚¹çš„é›†åˆç§°åš**å¯è¡ŒåŸŸ**ã€‚\n\n- æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•(Lagrange multiplier)æ˜¯ç”¨äºè§£å†³åªå«ç­‰å¼çº¦æŸçš„éçº¿æ€§è§„åˆ’é—®é¢˜çš„ã€‚\n- KKTæ¡ä»¶æ˜¯ç”¨äºè§£å†³åŒ…å«ä¸ç­‰å¼çº¦æŸé—®é¢˜çš„ã€‚\n\nå…¶ä¸­ç­‰å¼çº¦æŸå¯ä»¥ä½¿ç”¨ä¸ç­‰å¼çº¦æŸç­‰ä»·è¡¨ç¤ºï¼š\n\n$$\nh_i(x) = 0 \\iff \n\\left \\{\n\\begin {aligned}\nh_i(x) \\ge 0 \\\\\n-h_i(x) \\ge 0 \\\\\n\\end {aligned}\n\\right .\n$$\n\nå› è€Œ , ä¹Ÿå¯å°†éçº¿æ€§è§„åˆ’çš„æ•°å­¦æ¨¡å‹å†™æˆä»¥ä¸‹å½¢å¼:\n\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ g_i(x) \\ge 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right .\n$$\n\n# Lagrange multiplier\n\næ‹‰æ ¼æœ—æ—¥ç®—å­(Lagrange multiplier)åœ¨é«˜ç­‰æ•°å­¦ä¸­æœ‰æ¯”è¾ƒè¯¦ç»†çš„è®²è§£ï¼Œæœ¬æ–‡å…ˆç®€è¦çš„è®°å½•ä¸€ä¸‹ã€‚æ‹‰æ ¼æœ—æ—¥ç®—å­æ˜¯ç”¨æ¥è§£å†³å…·æœ‰ç­‰å¼çº¦æŸçš„æå€¼é—®é¢˜çš„ï¼Œå¦‚ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ h_i(x) = 0 (i=1,2,\\dots ,m)\\\\\n\\end {aligned}\n\\right .\n$$\nå³åœ¨$m$ä¸ªç­‰å¼çº¦æŸä¸‹å¯»æ‰¾èƒ½å¤Ÿä½¿$f(x)$æœ€å°åŒ–çš„$x$ ã€‚ä¸Šå¼ä¹Ÿå¯ä»¥å†™æˆå‘é‡å½¢å¼ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ H(x) = 0  \\\\\n\\end {aligned}\n\\right .\n$$\nå…¶ä¸­ï¼š\n$$\nH(x) = \n\\begin{Bmatrix}\nh_1(x) \\\\\nh_2(x) \\\\\n\\vdots \\\\\nh_m(x) \\\\\n\\end{Bmatrix}\n=\\begin{Bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\end{Bmatrix}\n$$\næ±‚è§£è¿™ä¸ªç­‰å¼çº¦æŸçš„æœ€ä¼˜åŒ–é—®é¢˜çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n\nç¬¬ä¸€æ­¥ï¼šæ„å»ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼Œå°†æ±‚$f(x)$æœ€å°å€¼è½¬å˜ä¸ºæ±‚æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„æœ€å°å€¼$L(x, \\lambda)$\n$$\nL(x, \\lambda) = f(x) - \\lambda^{\\mathrm T} H(x)\n$$\nç¬¬äºŒæ­¥ï¼šå‡è®¾ç‚¹$\\hat x$èƒ½å¤Ÿä½¿ f(x) åœ¨æ»¡è¶³çº¦æŸçš„æ¡ä»¶ä¸‹å–çš„æœ€å°å€¼ï¼Œé‚£ä¹ˆåœ¨è¯¥ç‚¹å¤„æ»¡è¶³æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„ **ä¸€é˜¶å¿…è¦æ¡ä»¶**ã€‚å³ï¼Œå¯¹$L(x, \\lambda)$å…³äº$x$å’Œ$\\lambda$æ±‚åå¯¼æ•°ï¼Œå¹¶ä»¤å…¶ä¸º0\n$$\n\\left \\{\n\\begin {aligned}\n \\nabla_x f(\\hat x) -  J_H (\\hat x)  \\lambda = 0 \\\\\n H(\\hat x) = 0 \\\\\n \\end {aligned}\n\\right .  \\tag1\n$$\n\nå…¶ä¸­$ J_H (x)$æ˜¯ç­‰å¼çº¦æŸçš„é›…å…‹æ¯”çŸ©é˜µï¼Œå®ƒçš„æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªç­‰å¼çº¦æŸå…³äº$x$çš„æ¢¯åº¦ã€‚\n$$\nJ_H (x) = \n\\begin {Bmatrix}\n(\\nabla_x h_1(x))^{\\mathrm{T}} \\\\\n(\\nabla_x h_2(x))^{\\mathrm{T}} \\\\\n\\vdots \\\\\n(\\nabla_x h_m(x))^{\\mathrm{T}} \\\\\n\\end {Bmatrix}\n$$\n\n\né€šè¿‡æ±‚è§£(1)å¼å¾—åˆ°èƒ½å¤Ÿä½¿$f(x)$æœ€å°åŒ–çš„$\\hat x$ ï¼Œå’Œ$f(x)$çš„æœ€å°å€¼ã€‚\n\n**é€šè¿‡ä¸Šå¼å¯ä»¥çœ‹å‡ºåœ¨æå°å€¼ç‚¹å¤„ $\\nabla_x f(x) =  J_H (x) \\lambda$ ï¼Œå³ï¼Œè¯¥ç‚¹å¤„å‡½æ•°$f(x)$çš„æ¢¯åº¦å¯ä»¥ç”¨æ‰€æœ‰ç­‰å¼çº¦æŸçš„æ¢¯åº¦å‘é‡çº¿æ€§è¡¨ç¤ºã€‚** ï¼ˆè¿™å¥è¯æ˜¯ä¸ºäº†å’ŒKKTæ¡ä»¶é—®é¢˜çš„è®ºè¿°ç›¸å¯¹åº”ã€‚ï¼‰\n\nä¸‹é¢ä»¥ä¸¤ç§æƒ…å†µå¯¹è¯¥ç»“è®ºåšä»¥è¯´æ˜ï¼š\n\n- å¦‚æœç­‰å¼çº¦æŸåªæœ‰ä¸€ä¸ªï¼šå¦‚\n\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ h_1(x) = 0 \\\\\n\\end {aligned}\n\\right .\n$$\n\nåˆ™å¯ä»¥å¾—åˆ°å…¶æ‹‰ä¸ªæœ—æ—¥å‡½æ•°çš„ä¸€é˜¶æ¡ä»¶å¦‚ä¸‹ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n \\nabla_x f(\\hat x) - \\lambda  \\nabla_x h_1(\\hat x) = 0 \\\\\n h_1(\\hat x) = 0 \\\\\n \\end {aligned}\n\\right .\n$$\næ­¤æ—¶æå€¼ç‚¹å¤„å‡½æ•°$f(x)$çš„æ¢¯åº¦å’Œç­‰å¼çº¦æŸ$h_1(x)$çš„æ¢¯åº¦æ–¹å‘ç›¸åŒã€‚\n\n- å¦‚æœç­‰å¼çº¦æŸæœ‰ä¸¤ä¸ªï¼šå¦‚\n\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ h_1(x) = 0 \\\\\n     & \\ h_2(x) = 0 \\\\\n\\end {aligned}\n\\right .\n$$\n\nåˆ™å¯ä»¥å¾—åˆ°å…¶æ‹‰ä¸ªæœ—æ—¥å‡½æ•°çš„ä¸€é˜¶æ¡ä»¶å¦‚ä¸‹ï¼š\n$$\n\\left \\{\n\\begin {aligned} \n\\nabla_x f(\\hat x) - \\lambda_1  \\nabla_x h_1(\\hat x) - \\lambda_2  \\nabla_x h_2(\\hat x) & = 0 \\\\\n                                                                           h_1(\\hat x) & = 0 \\\\\n                                                                           h_2(\\hat x) & = 0 \\\\\n\\end {aligned}\n\\right .\n$$\næ­¤æ—¶æå€¼ç‚¹å¤„å‡½æ•°$f(x)$çš„æ¢¯åº¦å¯ä»¥ç”¨ç­‰å¼çº¦æŸ$h_1(x)$ä¸$h_2(x)$çš„æ¢¯åº¦å‘é‡çº¿æ€§è¡¨ç¤ºã€‚\n\n\n\n# KKT\n\nåœ¨æ•°å­¦ä¸­ï¼ŒKKTæ¡ä»¶æ˜¯ï¼š\n\n- ä¸€ä¸ªéçº¿æ€§è§„åˆ’é—®é¢˜æœ‰æœ€ä¼˜åŒ–è§£çš„ä¸€ä¸ª**ä¸€é˜¶å¿…è¦æ¡ä»¶**ã€‚\n- ä¸€ä¸ªå¹¿ä¹‰åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ã€‚\n- ç”¨äºè§£å†³åŒ…å«ä¸ç­‰å¼çº¦æŸçš„æœ€ä¼˜åŒ–é—®é¢˜ã€‚\n\nå­˜åœ¨è¿™æ ·ä¸€ä¸ªéçº¿æ€§è§„åˆ’é—®é¢˜ï¼š\n\n$$\n\\left \\{\n\\begin {aligned}\n\\min & \\ f(x) \\\\\ns.t. & \\ g_i(x) \\ge 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right . \\tag 2\n$$\n\nå‡å®š$\\hat x$æ˜¯éçº¿æ€§è§„åˆ’çš„æå°ç‚¹ï¼Œè¯¥ç‚¹å¯èƒ½ä½äºå¯è¡ŒåŸŸçš„å†…éƒ¨ï¼Œä¹Ÿå¯èƒ½å¤„äºå¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Šã€‚\n\nè‹¥ä¸ºå‰è€…ï¼Œè¿™äº‹å®ä¸Šæ˜¯ä¸ªæ— çº¦æŸä¼˜åŒ–é—®é¢˜, æœ€ä¼˜ç‚¹ $\\hat x$å¿…æ»¡è¶³æ¡ä»¶ $\\nabla_x f(\\hat x) = 0$ ï¼›\n\nè‹¥ä¸ºåè€…ï¼Œæƒ…å†µå°±å¤æ‚å¾—å¤šäº†ï¼Œç°åœ¨æˆ‘ä»¬æ¥è®¨è®ºåä¸€ç§æƒ…å½¢ã€‚\n\n![KKT](from-Lagrange-Multiplier-to-KKT-Conditions/KKT.png)\n\nè®¾æœ€ä¼˜ç‚¹ä¸º$\\hat x$ã€‚æˆ‘ä»¬å…ˆè®¨è®ºä¸¤ä¸ªæ¯”è¾ƒç®€å•çš„ç‰¹æ®Šæƒ…å†µï¼š$\\hat x$è½åœ¨ä¸€ä¸ªä¸ç­‰å¼çº¦æŸçš„è¾¹ç•Œä¸Šï¼›$\\hat x$å¤„åœ¨ä¸¤ä¸ªä¸ç­‰å¼çº¦æŸçš„å…±åŒä½œç”¨ä¸‹ã€‚\n\n1. å½“$\\hat x$è½åœ¨ä¸€ä¸ªä¸ç­‰å¼çº¦æŸçš„è¾¹ç•Œä¸Šï¼Œå³$\\hat x$åªå¤„åœ¨ä¸€ä¸ªä¸ç­‰å¼çº¦æŸçš„ä½œç”¨ä¸‹ï¼Œå‡è®¾è¿™ä¸ªä¸ç­‰å¼çº¦æŸå°±æ˜¯$g_1(x) \\ge 0$ ï¼Œæ­¤æ—¶æœ€ä¼˜ç‚¹$\\hat x$å¿…å®šè½åœ¨å‡½æ•°$g_1(x) = 0$ ä¸Šã€‚æ­¤æ—¶å¯ä»¥å°†é—®é¢˜è½¬åŒ–ä¸ºç­‰å¼çº¦æŸçš„éçº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚\n\n$$\n  \\left \\{\n  \\begin {aligned}\n  \\min & \\ f(x) \\\\\n  s.t. & \\ g_1(x) = 0 \\\\\n  \\end {aligned}\n  \\right .\n$$\n\n  åˆ™å¯ä»¥å¾—åˆ°å…¶æ‹‰ä¸ªæœ—æ—¥å‡½æ•°çš„ **ä¸€é˜¶å¿…è¦æ¡ä»¶** å¦‚ä¸‹ï¼š\n$$\n\\left \\{\n\\begin {aligned}\n\\nabla_x f(\\hat x) - \\lambda_1  \\nabla_x g_1(\\hat x) & = 0 \\\\\n                                            g_1(\\hat x) & = 0 \\\\\n\\end {aligned}\n\\right .\n$$\n  å¦‚å³å›¾æ‰€ç¤ºï¼Œè™šçº¿ä¸º$f(x)$çš„ç­‰å€¼çº¿ï¼Œå®çº¿ä¸ºçº¦æŸå‡½æ•°$g_1(x)=0$ ï¼Œå¯è¡ŒåŸŸåœ¨çº¦æŸå‡½æ•°å³ä¾§ã€‚ç”±äºæœ€ä¼˜ç‚¹$\\hat x$åœ¨å¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Šï¼Œæ­¤æ—¶åœ¨ç‚¹$\\hat x$å¤„æ²¡æœ‰å¯è¡Œä¸‹é™æ–¹å‘ï¼Œæ‰€ä»¥$\\nabla_x g_1(\\hat x)$å¿…ä¸$- \\nabla_x f(\\hat x)$åœ¨ä¸€æ¡ç›´çº¿ä¸Šä¸”æ–¹å‘ç›¸åã€‚ç»“åˆä¸Šå¼å¯çŸ¥æ­¤æ—¶ $\\lambda_1 \\ge 0$ ï¼Œæ‰€ä»¥ï¼š\n$$\n\\left \\{\n  \\begin {aligned}\n   \\nabla_x f(\\hat x) - \\lambda_1  \\nabla_x g_1(\\hat x) & = 0 \\\\\n                                            g_1(\\hat x) & = 0 \\\\\n                                              \\lambda_1 & \\ge 0 \\\\\n   \\end {aligned}\n  \\right .\n$$\n\n2. è‹¥$\\hat x$ç‚¹æœ‰ä¸¤ä¸ªèµ·ä½œç”¨çº¦æŸ ï¼Œä¾‹å¦‚æœ‰ $g_1 (\\hat x) = 0$ å’Œ $g_2 (\\hat x) = 0$ã€‚æ­¤æ—¶å¯ä»¥å°†é—®é¢˜è½¬åŒ–ä¸ºç­‰å¼çº¦æŸçš„éçº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚\n\n$$\n  \\left \\{\n  \\begin {aligned}\n  \\min & \\ f(x) \\\\\n  s.t. & \\ g_1(x) = 0 \\\\\n       & \\ g_2(x) = 0 \\\\\n  \\end {aligned}\n  \\right .\n$$\n\n  åˆ™å¯ä»¥å¾—åˆ°å…¶æ‹‰ä¸ªæœ—æ—¥å‡½æ•°çš„ä¸€é˜¶æ¡ä»¶å¦‚ä¸‹ï¼š\n$$\n  \\left \\{\n  \\begin {aligned} \n   \\nabla_x f(\\hat x) - \\lambda_1  \\nabla_x h_1(\\hat x) - \\lambda_2  \\nabla_x h_2(\\hat x) & = 0 \\\\\n                                                                              g_1(\\hat x) & = 0 \\\\\n                                                                              g_2(\\hat x) & = 0 \\\\\n  \\end {aligned}\n  \\right .\n$$\n  å¦‚å·¦å›¾æ‰€ç¤ºï¼Œè™šçº¿ä¸º$f(x)$çš„ç­‰å€¼çº¿ï¼Œå®çº¿ä¸ºçº¦æŸå‡½æ•°$g_1(x)=0$ å’Œ$g_2(x)=0$ï¼Œå¯è¡ŒåŸŸåœ¨çº¦æŸå‡½æ•°å³ä¾§ã€‚ç”±äºæœ€ä¼˜ç‚¹$\\hat x$åœ¨ç”±$g_1(x)=0$ å’Œ$g_2(x)=0$å…±åŒå›´æˆçš„è¾¹ç•Œä¸Šï¼Œæ‰€ä»¥$- \\nabla_x f(\\hat x)$ä¸$\\nabla_x g_1(\\hat x)$çš„å¤¹è§’å¿…å®šå¤§çº¦90åº¦ï¼Œä¹Ÿå°±æ˜¯è¯´$\\nabla_x f(\\hat x)$å¿…å®šå¤„åœ¨$\\nabla_x g_1(\\hat x)$ä¸$\\nabla_x g_2(\\hat x)$ç»„æˆçš„å¤¹è§’ä¹‹å†…ã€‚ç»“åˆä¸Šå¼å¯çŸ¥æ­¤æ—¶ $\\lambda_1 \\ge 0 ,  \\lambda_1 \\ge 0$ ï¼Œæ‰€ä»¥ï¼š\n$$\n  \\left \\{\n  \\begin {aligned} \n  \\nabla_x f(\\hat x) - \\lambda_1  \\nabla_x h_1(\\hat x) - \\lambda_2  \\nabla_x h_2(\\hat x) & = 0 \\\\\n                                                                             g_1(\\hat x) & = 0 \\\\\n                                                                             g_2(\\hat x) & = 0 \\\\\n                                                            \\lambda_1 \\ge 0 ,  \\lambda_2 & \\ge 0 \\\\\n  \\end {aligned}\n  \\right .\n$$\n\n3. è‹¥$\\hat x$ç‚¹æœ‰Kä¸ªèµ·ä½œç”¨çº¦æŸ($0 \\le K \\le l$) ï¼Œæ­¤æ—¶\n\n   $$\n   \\left \\{\n   \\begin {aligned}\n    \\nabla_x f(\\hat x) - \\sum_k^K \\lambda_k  \\nabla_x g_k(\\hat x) &= 0 \\\\\n    g_k(\\hat x) &= 0, (k = 1,2 \\dots K)\\\\\n    \\lambda_k & \\ge 0, (k = 1,2 \\dots K)\\\\\n    \\end {aligned}\n   \\right .\n   $$\n\n\n\n\nä»¥ä¸Šæ¡ä»¶åªæ˜¯é’ˆå¯¹æœ€ä¼˜ç‚¹$\\hat x$å¤„äºå¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Šè®ºè¿°çš„ï¼Œè¿˜æœ‰ä¸€ç§æƒ…å†µå°±æ˜¯æœ€ä¼˜ç‚¹å¤„åœ¨å¯è¡ŒåŸŸå†…éƒ¨ã€‚å°†ä¸¤ç§æƒ…å†µï¼ˆæœ€ä¼˜ç‚¹å¤„åœ¨å¯è¡ŒåŸŸå†…éƒ¨å’Œè¾¹ç•Œä¸Šï¼‰åˆå¹¶èµ·æ¥è¡¨ç¤ºå¦‚ä¸‹ï¼š\n$$\n\\left \\{\\begin {aligned} \\nabla_x f(\\hat x) - \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\ng_i(\\hat x) &\\ge 0, (i = 1,2 \\dots l) \\\\\n\\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l)  \\\\\n\\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\ \n\\end {aligned}\\right .\n$$\nå¯ä»¥çœ‹å‡ºå½“ $g_i(\\hat x) = 0$ æ—¶ï¼Œå®ƒå¯èƒ½æ˜¯å¯¹äºæœ€ä¼˜ç‚¹$\\hat x$èµ·ä½œç”¨çš„çº¦æŸ ï¼Œ $\\lambda_i$å¯ä»¥ä¸ä¸ºé›¶ï¼›å½“$g_i(\\hat x) \\ne 0$ æ—¶ï¼Œå³$g_i(\\hat x) \\gt 0$ ï¼Œæ­¤æ—¶å®ƒä¸€å®šä¸æ˜¯å¯¹æœ€ä¼˜ç‚¹$\\hat x$èµ·ä½œç”¨çš„çº¦æŸï¼Œæ­¤æ—¶å¿…æœ‰$\\lambda_i = 0$ ã€‚\n\nç°å¯å°†åº“æ© - å¡”å…‹æ¡ä»¶å™è¿°å¦‚ä¸‹ :è®¾ $\\hat x$ æ˜¯éçº¿æ€§è§„åˆ’å¼(2)çš„æå°ç‚¹ , è€Œä¸”åœ¨ $\\hat x$ ç‚¹çš„å„èµ·ä½œç”¨çº¦æŸçš„æ¢¯åº¦çº¿æ€§æ— å…³ï¼Œåˆ™å­˜åœ¨å‘é‡ $\\Lambda = (\\lambda_1 ,\\lambda_2, \\dots, \\lambda_l)$ ï¼Œ ä½¿ä¸‹è¿°æ¡ä»¶æˆç«‹ :\n$$\n\\left \\{\n\\begin {aligned}\n\\nabla_x f(\\hat x) - \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\ng_i(\\hat x) &\\ge 0, (i = 1,2 \\dots l) \\\\\n\\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l) \\\\\n\\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\\n\\end {aligned}\n\\right .\n$$\n\nåº“æ©-å¡”å…‹æ¡ä»¶æ˜¯éçº¿æ€§è§„åˆ’é¢†åŸŸä¸­æœ€é‡è¦çš„ç†è®ºæˆæœä¹‹ä¸€ï¼Œæ˜¯ç¡®å®šæŸç‚¹ä¸ºæœ€ä¼˜ç‚¹çš„**å¿…è¦æ¡ä»¶**ã€‚ä½†ä¸€èˆ¬æ¥è¯´å®ƒå¹¶ä¸æ˜¯å……åˆ†æ¡ä»¶ï¼Œå› è€Œæ»¡è¶³è¿™ä¸ªæ¡ä»¶çš„ç‚¹ä¸ä¸€å®šå°±æ˜¯æœ€ä¼˜ç‚¹ã€‚\n\n**å¯¹äºå‡¸è§„åˆ’é—®é¢˜ï¼ŒKKTæ¡ä»¶æ—¢æ˜¯æœ€ä¼˜ç‚¹å­˜åœ¨çš„å¿…è¦æ¡ä»¶ï¼ŒåŒæ—¶ä¹Ÿæ˜¯å……åˆ†æ¡ä»¶ã€‚**\n\n# KKTæ¡ä»¶æ€»ç»“\n\nå¦‚æœ¬å¼€å¤´éƒ¨åˆ†æ‰€è¿°ï¼Œå«æœ‰ä¸ç­‰å¼çº¦æŸçš„éçº¿æ€§ä¼˜åŒ–é—®é¢˜å¯ä»¥æœ‰å¤šç§è¡¨è¿°ï¼Œæœ¬èŠ‚ç›´æ¥åˆ—å‡ºå„ç§è¯¥è¡¨è¿°çš„å’Œå…¶å¯¹é¥®é«˜çš„KKTæ¡ä»¶ã€‚\n$$\n\\left \\{\n\\begin {aligned}\n      & \\min f(x) \\\\\ns.t. & g_i(x) \\ge 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right . \n\\Rightarrow\n\\left \\{\n\\begin {aligned}\n\\nabla_x f(\\hat x) - \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\n                                                                                     g_i(\\hat x) &\\ge 0, (i = 1,2 \\dots l)  \\\\\n                                                                  \\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l) \\\\\n                                                                                    \\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\\n\\end {aligned}\n\\right .\n$$\n\n$$\n\\left \\{\n\\begin {aligned}\n      & \\min f(x) \\\\\ns.t. & g_i(x) \\le 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right . \n\\Rightarrow\n\\left \\{\n\\begin {aligned}\n \\nabla_x f(\\hat x) + \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\n                                                                                      g_i(\\hat x) &\\le 0, (i = 1,2 \\dots l) \\\\\n                                                                    \\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l) \\\\\n                                                                                      \\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\\n \\end {aligned}\n\\right .\n$$\n\n$$\n\\left \\{\n\\begin {aligned}\n      & \\min f(x) \\\\\ns.t. & h_j(x) = 0 (j=1,2,\\dots ,m) \\\\\n      & g_i(x) \\ge 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right .\n\\Rightarrow\n\\left \\{\n\\begin {aligned}\n \\nabla_x f(\\hat x) - \\sum_j^m \\gamma_j \\nabla_x h_j(\\hat x) - \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\n                                                                                                                                                                h_j(\\hat x) &= 0 (j=1,2,\\dots ,m) \\\\\n                                                                                                                                                                g_i(\\hat x) &\\ge 0, (i = 1,2 \\dots l) \\\\\n                                                                                                                                             \\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l) \\\\\n                                                                                                                                                               \\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\\n \\end {aligned}\n\\right .\n$$\n\n$$\n\\left \\{\n\\begin {aligned}\n      & \\min f(x) \\\\\ns.t. & h_j(x) = 0 (j=1,2,\\dots ,m) \\\\\n      & g_i(x) \\le 0 (i=1,2,\\dots ,l) \\\\\n\\end {aligned}\n\\right .\n\\Rightarrow\n\\left \\{\n\\begin {aligned}\n \\nabla_x f(\\hat x) + \\sum_j^m \\gamma_j \\nabla_x h_j(\\hat x) + \\sum_i^l \\lambda_i  \\nabla_x g_i(\\hat x) &= 0 \\\\\n                                                                                                                                                                  h_j(\\hat x) &= 0 (j=1,2,\\dots ,m) \\\\\n                                                                                                                                                                  g_i(\\hat x) &\\le 0, (i = 1,2 \\dots l) \\\\\n                                                                                                                                               \\lambda_i g_i(\\hat x) &= 0, (i = 1,2 \\dots l) \\\\\n                                                                                                                                                                 \\lambda_i & \\ge 0, (i = 1,2 \\dots l) \\\\\n \\end {aligned}\n\\right .\n$$\n\nå…ˆåˆ—å‡ºè¿™å››ç§è¡¨è¿°ã€‚\n\n# å‚è€ƒèµ„æ–™\n\nè¿ç­¹å­¦-ç¬¬ä¸‰ç‰ˆ\n","tags":["Math"],"categories":["Math"]},{"title":"èŒƒæ•°æ­£åˆ™åŒ–-ç¥ç»ç½‘ç»œ","url":"%2Fblog%2FNorm_Regularization_for_DeepLearning-Neural_networks.html","content":"\n# 0 æ¦‚è¿°\n\næœºå™¨å­¦ä¹ çš„ä¸­å¿ƒé—®é¢˜æ˜¯åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå‡ºä¸€ä¸ªèƒ½å¤Ÿåœ¨æ–°çš„è¾“å…¥æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹ã€‚ç°æœ‰çš„å¾ˆå¤šç­–ç•¥æ˜¯åœ¨ç‰ºç‰²è®­ç»ƒè¯¯å·®æ¥å‡å°æ³›åŒ–è¯¯å·®ï¼Œè¿™äº›ç­–ç•¥è¢«ç»Ÿç§°ä¸ºæ­£åˆ™åŒ–ã€‚\n\nå’Œä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç®—æ³•ä¸€æ ·ï¼Œç¥ç»ç½‘ç»œçš„èŒƒæ•°æ­£åˆ™åŒ–æ–¹æ³•ä¹Ÿæ˜¯é€šè¿‡é™åˆ¶æ¨¡å‹å‚æ•°æ¥å‡å°‘è¿‡æ‹Ÿåˆçš„ã€‚ä½†æ˜¯ç¥ç»ç½‘ç»œçš„èŒƒæ•°æ­£åˆ™åŒ–ä¹Ÿæœ‰è‡ªå·±çš„ç‰¹ç‚¹ï¼Œä¸ºäº†è®ºè¿°æ–¹ä¾¿æœ¬æ–‡å°†ç¥ç»ç½‘ç»œçš„å‚æ•°åˆ†ä¸ºä¸¤ç±»ï¼š\n\n- $\\boldsymbol w$ ï¼šä»¿å°„å˜æ¢æƒé‡å‚æ•°ï¼Œæ¨¡å‹å‘é‡ä¸¤å±‚ç¥ç»å…ƒçš„ç›¸äº’ä½œç”¨çš„å‚æ•°ï¼Œä¸åŒ…å«åæ‰§(bias)ï¼›\n- $\\boldsymbol \\beta$ ï¼šåæ‰§å‚æ•°ï¼›\n- $\\boldsymbol \\theta$ ï¼šåŒ…å«ä»¿å°„å˜æ¢æƒé‡å‚æ•°$w$å’Œåæ‰§å‚æ•°(bias)ã€‚\n\nåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ­£åˆ™åŒ–åªé’ˆå¯¹ä»¿å°„å˜æ¢æƒé‡å‚æ•°$\\boldsymbol w$ï¼Œè€Œæ¯ä¸€å±‚çš„åæ‰§å‚æ•°ä¸åšæ­£åˆ™åŒ–å¤„ç†ã€‚è¿™æ˜¯å› ä¸ºï¼š\n\n- æ¯ä¸ªåç½®ä»…æ§åˆ¶ä¸€ä¸ªå•å˜é‡(æƒé‡å‚æ•°æ§åˆ¶ä¸¤ä¸ªå˜é‡)ï¼Œè¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬ä¸å¯¹å…¶è¿›è¡Œæ­£åˆ™åŒ–ä¹Ÿä¸ä¼šå¯¼è‡´å¤ªå¤§çš„åå·®\n- åç½®å‚æ•°çš„æ­£åˆ™åŒ–å¯èƒ½ä¼šå¯¼è‡´æ˜æ˜¾çš„æ¬ æ‹Ÿåˆã€‚\n\næ­£åˆ™åŒ–ä¹‹å‰çš„ç›®æ ‡å‡½æ•°è®°ä¸º $J(\\boldsymbol \\theta, \\boldsymbol X, y)$ ï¼Œæ­£åˆ™åŒ–åçš„ç›®æ ‡å‡½æ•°è®°ä¸º$\\tilde J(\\boldsymbol \\theta, \\boldsymbol X, y)$ ï¼Œåˆ™:\n\n$$\n\\tilde{J}(\\boldsymbol \\theta,\\boldsymbol  X, y) = J(\\boldsymbol \\theta, \\boldsymbol X, y) + \\alpha \\Omega(\\boldsymbol \\theta)\n$$\n\n\nå…¶ä¸­$\\Omega$æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œ$\\alpha \\ge 0$ æ˜¯è¶…å‚ï¼Œ$\\alpha$è¶Šå¤§ï¼Œæ­£åˆ™åŒ–åŠ›åº¦è¶Šå¤§ã€‚æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„æ­£åˆ™åŒ–é¡¹æ˜¯p-èŒƒæ•° ($L^p$)ï¼š$||\\boldsymbol x||_p=(\\sum_i |x_i|^p)^{\\frac1p}, p \\ge 1$\n\n- $p=2$æ—¶ï¼Œ$L^2$ ç§°ä¸º2èŒƒæ•°ï¼Œæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼Œ $||\\boldsymbol x||_2=(\\sum_i |x_i|^2)^{\\frac12}$ ã€‚å®é™…åº”ç”¨å½“ä¸­ï¼Œå¸¸ç”¨çš„æ˜¯å®ƒçš„å¹³æ–¹ï¼Œ $||\\boldsymbol x||_2^2=\\sum_i |x_i|^2 = \\boldsymbol x^{\\mathrm{T}} \\boldsymbol x$ ã€‚\n- $p=1$æ—¶ï¼Œ$L^1$ ç§°ä¸º1èŒƒæ•°ï¼Œ$||\\boldsymbol x||_1=\\sum_i |x_i|$ ã€‚å½“åŒºåˆ†å…ƒç´  æ°å¥½æ˜¯é›¶ å’Œ éé›¶(ä½†å€¼å¯èƒ½å¾ˆå°) å¯¹äºæ¨¡å‹å¾ˆé‡è¦çš„æ—¶å€™ä½¿ç”¨è¿™ç§èŒƒæ•°ã€‚\n- $p=\\infty$æ—¶ï¼Œ$L^\\infty$ ç§°ä¸ºæœ€å¤§èŒƒæ•°ï¼Œ$||\\boldsymbol x ||_ \\infty=\\max_i|x_i|$ ã€‚\n- â€œ$L^0â€‹$èŒƒæ•°â€è¡¨ç¤ºå‘é‡ä¸­éé›¶å…ƒç´ çš„ä¸ªæ•°ï¼Œä½†æ˜¯è¿™ä¸ªæœ¯è¯­åœ¨æ•°å­¦æ„ä¹‰ä¸Šæ˜¯ä¸å¯¹çš„ã€‚\n\n\n# 1 $L^2$èŒƒæ•°æ­£åˆ™åŒ–\n\n$L^2$èŒƒæ•°æ­£åˆ™åŒ–å°†ç›®æ ‡å‡½æ•°$\\tilde J$ä¸­æ­£åˆ™åŒ–é¡¹å®šä¹‰ä¸ºå‚æ•°çš„2èŒƒæ•°çš„å¹³æ–¹ï¼Œ$\\Omega(\\boldsymbol \\theta) = \\frac 12 ||\\boldsymbol w||_2^2 = \\frac 12 \\boldsymbol w ^ {\\mathrm{T}} \\boldsymbol w$ ã€‚\n\nä¸‹é¢ä»ä¸‰ä¸ªè§’åº¦ç†è§£$L^2$èŒƒæ•°æ­£åˆ™åŒ–ï¼š\n\n1. **ä»æ¢¯åº¦ä¸‹é™çš„è§’åº¦**\n\nç›®æ ‡å‡½æ•°æ±‚å…³äº$w$çš„æ¢¯åº¦ï¼Œæœ€ç»ˆæ±‚çš„$w$çš„æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å…¬å¼è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\n$$\n\\begin {aligned}\n& \\tilde{J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) = J(\\boldsymbol w ,\\boldsymbol \\beta, \\boldsymbol X, y) +   \\frac \\alpha2 \\boldsymbol w ^ {\\mathrm{T}} \\boldsymbol w \\\\\n\\\\\n& \\nabla_w \\tilde{J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) = \\nabla_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)  + \\alpha \\boldsymbol  w \\\\\n\\\\\n& w  \\gets w - \\epsilon(\\nabla_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) + \\alpha  w) \\\\\n\\\\\n&  w  \\gets (1-\\epsilon\\alpha) w - \\epsilon \\nabla_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)\n\\end {aligned}\n$$\n\nå…¶ä¸­$\\epsilon$ä¸ºå­¦ä¹ ç‡ã€‚ç›®æ ‡å‡½æ•° ${J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)$ ä¸åŠ æ­£åˆ™åŒ–æ—¶çš„æ¢¯åº¦ä¸‹é™æ›´æ–°å…¬å¼ä¸ºï¼š\n\n$$\nw  \\gets   w - \\epsilon \\nabla_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)\n$$\n\né€šè¿‡ä»¥ä¸Šä¸¤ä¸ªå…¬å¼çš„å¯¹æ¯”å¯ä»¥çœ‹å‡ºï¼Œåœ¨æ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œ$L^2$èŒƒæ•°æ­£åˆ™åŒ–å…ˆå°†$w$ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°å› å­$(1-\\epsilon\\alpha) $ ($w$çš„ç¼©æ”¾)ï¼Œç„¶åæ‰§è¡Œéæ­£åˆ™åŒ–çš„æ¢¯åº¦ä¸‹é™ã€‚\n\n2. **ä»æ¨¡å‹è®­ç»ƒçš„æ•´ä½“è§’åº¦(è§£æè§£çš„æ¨å¯¼)**\n\nå‡è®¾éæ­£åˆ™åŒ–çš„ç›®æ ‡å‡½æ•° ${J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)$ åœ¨ $\\boldsymbol {\\hat w}$ å¤„å–å¾—æœ€å°å€¼ï¼Œå³ï¼š\n\n$$\n\\boldsymbol {\\hat w} = \\arg\\min_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)\n$$\n\n**ä¸ºäº†ç®€åŒ–åˆ†æï¼Œæˆ‘ä»¬å°†ç›®æ ‡å‡½æ•°è¿‘ä¼¼ä¸ºå…¶åœ¨$\\hat w$å¤„çš„äºŒé˜¶æ³°å‹’å±•å¼€ã€‚** ï¼ˆå½“ç›®æ ‡å‡½æ•°æ­£å¥½ä¸º$w$çš„äºŒæ¬¡å‡½æ•°æ—¶ $\\hat J = J$ï¼‰\n\nå¯¹${J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)$ åœ¨ $\\boldsymbol {\\hat w}$ å¤„åšäºŒé˜¶æ³°å‹’è¿‘ä¼¼å¹¶ä½¿ç”¨$L^2$èŒƒæ•°æ­£åˆ™åŒ–ï¼Œè®°ä¸º$\\hat J(\\boldsymbol \\theta)$ :\n$$\n\\begin {aligned}\n& \\hat J(\\boldsymbol \\theta) = J(\\boldsymbol {\\hat w}) +\\frac12(\\boldsymbol {w} - \\boldsymbol {\\hat w})^{\\mathrm{T}}\\boldsymbol H(\\boldsymbol {w} - \\boldsymbol {\\hat w}) +   \\frac \\alpha2 \\boldsymbol w ^ {\\mathrm{T}} \\boldsymbol w \\\\\n& \\nabla_w \\hat J(\\boldsymbol \\theta) = \\boldsymbol H(\\boldsymbol {w} - \\boldsymbol {\\hat w})  + \\alpha \\boldsymbol w\n\\end {aligned}\n$$\n\nå…¶ä¸­$\\boldsymbol H$ æ˜¯ç›®æ ‡å‡½æ•° ${J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)$ åœ¨$\\boldsymbol {\\hat w}$ å¤„çš„Hessian matrixã€‚$ \\frac \\alpha2 \\boldsymbol w ^ {\\mathrm{T}} \\boldsymbol w$ ä¸ºæ­£åˆ™åŒ–é¡¹ã€‚\n\næ³¨æ„ï¼šå› ä¸º$\\boldsymbol {\\hat w} = \\arg\\min_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y)$ \n\n- è¯¥å¼ä¸­æ²¡æœ‰ä¸€é˜¶é¡¹ã€‚$\\nabla J(\\boldsymbol {\\hat w}) = 0$ ã€‚\n- $\\boldsymbol H$ æ˜¯åŠæ­£å®šçŸ©é˜µã€‚\n\nä»¤$\\tilde {\\boldsymbol w} = \\arg\\min_w \\hat J(\\boldsymbol \\theta)$ ï¼Œå³åœ¨$\\tilde {\\boldsymbol w}$å¤„$\\hat J(\\boldsymbol \\theta)$å–æœ€å°å€¼ã€‚æ­¤æ—¶ï¼š\n\n$$\n\\begin {aligned}\n& \\boldsymbol H(\\tilde {\\boldsymbol {w}} - \\boldsymbol {\\hat w}) + \\alpha \\tilde {\\boldsymbol {w}}  =  0 \\\\\n& \\tilde {\\boldsymbol {w}}  = (\\boldsymbol H + \\alpha \\boldsymbol I)^{-1} \\boldsymbol H  \\boldsymbol {\\hat w}\n\\end {aligned}\n$$\n\nå½“$\\alpha \\to 0$ æ—¶ï¼Œ$\\tilde{\\boldsymbol {w}} \\to \\boldsymbol {\\hat w}$ ã€‚å› ä¸º$\\boldsymbol H$ æ˜¯å®å¯¹ç§°çŸ©é˜µï¼Œæ‰€ä»¥å®ƒå¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µï¼š\n\n$$\n\\boldsymbol {H} = \\boldsymbol {Q} \\boldsymbol {\\Lambda} \\boldsymbol {Q }^{\\mathrm T}\n$$\n\n$\\boldsymbol {\\Lambda}$ ä¸º$\\boldsymbol H$çš„ç‰¹å¾å€¼ç»„æˆçš„å¯¹è§’é˜µï¼Œ $\\boldsymbol {Q}$ä¸º$\\boldsymbol H$çš„ç‰¹å¾å‘é‡ç»„æˆçš„æ ‡å‡†æ­£äº¤é˜µ(æ­¤æ—¶$\\boldsymbol {Q}^{-1} = \\boldsymbol {Q} ^ {\\mathrm {T}}$)ã€‚æ‰€ä»¥å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n\n$$\n\\begin {aligned}\n\\tilde {\\boldsymbol {w}}  &= (\\boldsymbol H + \\alpha \\boldsymbol I)^{-1} \\boldsymbol H  \\boldsymbol {\\hat w} \\\\\n                                             &= (\\boldsymbol {Q} \\boldsymbol {\\Lambda} \\boldsymbol {Q }^{\\mathrm T} + \\alpha \\boldsymbol I)^{-1} \\boldsymbol {Q} \\boldsymbol {\\Lambda} \\boldsymbol {Q }^{\\mathrm T}  \\boldsymbol {\\hat w} \\\\\n                                             &=\\boldsymbol {Q} (\\boldsymbol {\\Lambda} + \\alpha \\boldsymbol I)^{-1}\\boldsymbol {\\Lambda} \\boldsymbol {Q }^{\\mathrm T}\\boldsymbol {\\hat w}\\\\\n\\end {aligned}\n$$\n\n$L^2$èŒƒæ•°æ­£åˆ™åŒ–çš„æ•ˆæœæ˜¯æ²¿ç€ç”±$\\boldsymbol H$çš„ç‰¹å¾å‘é‡æ‰€å®šä¹‰çš„è½´ç¼©æ”¾$\\boldsymbol {\\hat w}$ ï¼Œå³å°†ä¸$\\boldsymbol H$çš„ç¬¬$i$ä¸ªç‰¹å¾å‘é‡ å¯¹é½çš„$\\boldsymbol {\\hat w}$çš„åˆ†é‡ ä¹˜ä»¥$\\frac {\\lambda_i}{\\lambda_i + \\alpha}$å¾—åˆ°æ­£åˆ™åŒ–åçš„$\\tilde {\\boldsymbol {w}}$çš„ç¬¬$i$ä¸ªåˆ†é‡ã€‚å½“$\\lambda_i \\gg \\alpha$ æ—¶ï¼Œ$\\frac {\\lambda_i}{\\lambda_i + \\alpha} \\to 1, \\tilde{w}_i \\to {\\hat w_i}$ ï¼›å½“$\\lambda_i \\ll \\alpha$ æ—¶$\\frac {\\lambda_i}{\\lambda_i + \\alpha} \\to 0, \\tilde{w}_i \\to 0$ ã€‚\n\n> $\\boldsymbol {H}$ çš„ç‰¹å¾å€¼ $\\Lambda$çš„ç‰©ç†å«ä¹‰ï¼šHessiançŸ©é˜µçš„ç‰¹å¾å€¼å…¶å¯¹åº”çš„ç‰¹å¾å‘é‡æ–¹å‘ä¸Šçš„å‡½æ•°å˜åŒ–ç‡ã€‚ç‰¹å¾å€¼è¶Šå¤§ï¼Œå…¶å¯¹åº”æ–¹å‘ä¸Šçš„å˜åŒ–ç‡è¶Šå¤§ï¼Œç‰¹å¾å€¼è¶Šå°ï¼Œå…¶å¯¹åº”æ–¹å‘ä¸Šçš„å‡½æ•°å˜åŒ–ç‡è¶Šå°ã€‚(å½“å‰ç†è§£è¿˜ä¸é€å½»ï¼Œéœ€è¦åŠ å¼º)\n\n3. **å‡ ä½•è§’åº¦**\n\n![L2_regularization](Norm_Regularization_for_DeepLearning-Neural_networks/L2_regularization.png)\n\nè“çº¿æ¤­åœ†è¡¨ç¤ºæ²¡æœ‰æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°çš„ç­‰å€¼çº¿ã€‚çº¢çº¿åœ†åœˆè¡¨ç¤º$L^2$æ­£åˆ™åŒ–é¡¹çš„ç­‰å€¼çº¿ã€‚åœ¨$\\tilde w$ç‚¹ï¼ŒåŸå§‹ç›®æ ‡å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹çš„å’Œè¾¾åˆ°æœ€å°å€¼ã€‚ç›®æ ‡å‡½æ•°$J$çš„Hessiançš„ç¬¬ä¸€ç»´(æ¨ªå‘)ç‰¹å¾å€¼å¾ˆå°ã€‚å½“æ¨ªå‘å¹³ç§»åŠ¨æ—¶ï¼Œç›®æ ‡å‡½æ•°ä¸ä¼šå¢åŠ å¾—å¤ªå¤šã€‚å› ä¸ºç›®æ ‡å‡½æ•°å¯¹è¿™ä¸ªæ–¹å‘æ²¡æœ‰å¼ºçƒˆçš„åå¥½ï¼Œæ‰€ä»¥æ­£åˆ™åŒ–é¡¹å¯¹è¯¥è½´å…·æœ‰æ›´å¼ºçƒˆçš„å½±å“ã€‚æ­£åˆ™åŒ–é¡¹å°†$w^1$æ‹‰å‘é›¶ã€‚è€Œç›®æ ‡å‡½æ•°å¯¹æ²¿ç€ç¬¬äºŒç»´çš„ç§»åŠ¨éå¸¸æ•æ„Ÿã€‚å¯¹åº”çš„ç‰¹å¾å€¼è¾ƒå¤§ï¼Œè¡¨ç¤ºé«˜æ›²ç‡ã€‚å› æ­¤ï¼Œæƒé‡è¡°å‡å¯¹$w^2$çš„ä½ç½®å½±å“ç›¸å¯¹è¾ƒå°ã€‚\n\nå¯ä»¥ç†è§£ä¸º**åŸå§‹ç›®æ ‡å‡½æ•°$J$** å’Œ**æ­£åˆ™åŒ–é¡¹**ç«äº‰çš„è¿‡ç¨‹ã€‚åŸå§‹ç›®æ ‡å‡½æ•°$J$å¸Œæœ›æœ€ä¼˜ç‚¹è½åœ¨$\\hat w$å¤„ï¼Œæ­£åˆ™åŒ–é¡¹å¸Œæœ›æœ€ä¼˜ç‚¹è½åœ¨åŸç‚¹å¤„ã€‚åœ¨$w$çš„å„ä¸ªç»´åº¦ä¸Šä»–ä»¬é€šè¿‡æ¯”è¾ƒå„è‡ªçš„æ›²ç‡ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰æ¥å†³å®šè°èµ·ä¸»è¦ä½œç”¨ï¼Œè°çš„æ›²ç‡å¤§ï¼Œè°å°±èµ·ä¸»å¯¼ä½œç”¨ã€‚\n\nnote: $L^2$æ­£åˆ™åŒ–é¡¹åœ¨åŸç‚¹å¤„å„æ–¹å‘ä¸Šçš„æ›²ç‡æ˜¯ç›¸åŒçš„ã€‚\n\n\n\n# 2 $L^1$èŒƒæ•°\n\nä¸‹é¢ä»ä¸¤ä¸ªè§’åº¦ç†è§£$L^1$èŒƒæ•°æ­£åˆ™åŒ–ï¼š\n\n1. **ä»æ¨¡å‹è®­ç»ƒçš„æ•´ä½“è§’åº¦(è§£æè§£çš„æ¨å¯¼)**\n\n$L^1$èŒƒæ•°æ­£åˆ™åŒ–å°†ç›®æ ‡å‡½æ•°$\\tilde J$ä¸­æ­£åˆ™åŒ–é¡¹å®šä¹‰ä¸ºå‚æ•°$\\boldsymbol w$çš„å„ç»´åº¦çš„ç»å¯¹å€¼ä¹‹å’Œï¼Œ$\\Omega(\\boldsymbol \\theta) =||\\boldsymbol w||_1 = \\sum_i |w_i|$ ã€‚æ­£åˆ™åŒ–åçš„ç›®æ ‡å‡½æ•°å’Œå…¶å¯¼æ•°ä¸ºï¼š\n\n$$\n\\DeclareMathOperator{\\sign}{sign}\n\\begin {aligned}\n& \\tilde{J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) = J(\\boldsymbol w ,\\boldsymbol \\beta, \\boldsymbol X, y) +  \\alpha ||\\boldsymbol w||_1 \\\\\n& \\nabla_w  \\tilde{J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) = \\nabla_w J(\\boldsymbol w ,\\boldsymbol \\beta, \\boldsymbol X, y) +  \\alpha \\sign(\\boldsymbol w)\n\\end {aligned}\n$$\n\nå…¶ä¸­è‹¥$w \\gt 0$ï¼Œåˆ™$sign(w)= +1$ï¼›è‹¥$w \\lt 0$ï¼Œåˆ™$sign(w) = -1$ï¼›è‹¥$w=0$ï¼Œåˆ™$sign(w)=0$ã€‚\n\nå‡è®¾$\\boldsymbol {\\hat w} = \\arg\\min_w {J}(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) $ï¼Œä¸ºäº†è¯´æ˜é—®é¢˜éœ€è¦å¯¹åŸå§‹ç›®æ ‡å‡½æ•°åšè¿‘ä¼¼å¤„ç†ï¼Œæœªæ­£åˆ™åŒ–çš„ç›®æ ‡å‡½æ•° $J(\\boldsymbol w ,\\boldsymbol \\beta,\\boldsymbol  X, y) $ åœ¨$\\boldsymbol {\\hat w}$å¤„çš„äºŒé˜¶æ³°å‹’è¿‘ä¼¼ä¸ºï¼š\n\n$$\nJ(\\boldsymbol {w}) \\approx  J(\\boldsymbol {\\hat w}) +\\frac12(\\boldsymbol {w} - \\boldsymbol {\\hat w})^{\\mathrm{T}}\\boldsymbol H(\\boldsymbol {w} - \\boldsymbol {\\hat w})\n$$\n\næ­¤æ—¶\n\n$$\n\\begin {aligned}\n& \\tilde{J}(\\boldsymbol w) \\approx     J(\\boldsymbol {\\hat w}) +\\frac12(\\boldsymbol {w} - \\boldsymbol {\\hat w})^{\\mathrm{T}}\\boldsymbol H(\\boldsymbol {w} - \\boldsymbol {\\hat w})  +  \\alpha ||\\boldsymbol w||_1 \\\\\n\\end {aligned}\n$$\n\nå…¶ä¸­$H$æ˜¯å…³äº$w$çš„HessiançŸ©é˜µã€‚\n\n)ç”±äº$\\sign$çš„å­˜åœ¨ï¼Œæˆ‘ä»¬ç›´æ¥ä»¤æ˜¯æ­£åˆ™åŒ–çš„äºŒé˜¶è¿‘ä¼¼çš„æ¢¯åº¦ä¸º0è¿˜æ˜¯å¾ˆéš¾ç›´è§‚çš„è¯´æ˜é—®é¢˜çš„ã€‚(å‡è®¾$\\tilde w$ æ˜¯æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€ç»ˆçœ‹åˆ°çš„æ˜¯$\\tilde {w}_i$å’Œ$\\hat w_i$çš„ä»£æ•°å…³ç³»)ã€‚ä¸ºäº†å¾—åˆ°æ›´ç›´è§‚çš„ä»£æ•°å…³ç³»ï¼Œæˆ‘ä»¬å¯¹HessiançŸ©é˜µè¿›ä¸€æ­¥ç®€åŒ–ï¼šå‡è®¾$H$çš„æ‰€æœ‰éå¯¹è§’å…ƒç´ éƒ½ä¸º0ï¼Œå³ï¼Œ$H$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µ(å½“è®­ç»ƒæ•°æ®é›†çš„å„ç»´åº¦çš„ç‰¹å¾æ˜¯ä¸ç›¸å…³æ—¶ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Œæ¯”å¦‚ä½¿ç”¨äº†PCAç®—æ³•å¯¹è®­ç»ƒæ•°æ®åšäº†é¢„å¤„ç†)ï¼Œæ­¤æ—¶ï¼š\n$$\n\\DeclareMathOperator{\\diag}{diag}\nH = \\diag (H_{1,1},H_{2,2},\\dots,H_{n,n})\n$$\n\næ­¤æ—¶æ­£åˆ™åŒ–çš„ç›®æ ‡å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥è¿‘ä¼¼ä¸ºï¼š\n\n$$\n\\begin {aligned}\n\\tilde{J}(\\boldsymbol w) &\\approx  \\hat{J}(\\boldsymbol w) \\\\\n                                           &=  J(\\boldsymbol {\\hat w}) +\\frac12(\\boldsymbol {w} - \\boldsymbol {\\hat w})^{\\mathrm{T}}\\boldsymbol H(\\boldsymbol {w} - \\boldsymbol {\\hat w})  +  \\alpha ||\\boldsymbol w||_1 \\\\\n                                           &= J(\\boldsymbol {\\hat w}) + \\sum_i[\\frac 12 H_{i,i}(w_i-\\hat w_i)^2] +  \\alpha \\sum_i|w_i| \\\\\n                                           &= J(\\boldsymbol {\\hat w}) + \\sum_i[\\frac 12 H_{i,i}(w_i-\\hat w_i)^2 +  \\alpha |w_i|] \n\\end {aligned} \\tag 1\n$$\n\næ­¤æ—¶æˆ‘ä»¬è®¨è®ºè¿™ä¸€ä¸ªç‰¹æ®Šçš„ç›®æ ‡å‡½æ•° $\\hat{J}(\\boldsymbol w) $ ï¼Œå¯»æ‰¾ä¸€ä¸ª$\\tilde w$ä½¿ $\\hat{J}(\\boldsymbol w) $å–æœ€å°å€¼å³ï¼š\n\n$$\n\\tilde w = \\arg\\min_w \\hat{J}(\\boldsymbol w)\n$$\n\nå¯¹$\\hat{J}(\\boldsymbol w) $å…³äº$w$çš„æ¯ä¸ªæ–¹å‘åˆ†åˆ«ç‹¬ç«‹æ±‚å¯¼ï¼Œå¹¶ä½¿ä¹‹ä¸º0ï¼Œå¾—ï¼š\n\n$$\nH_{i,i}(w_i-\\hat w_i)+\\alpha \\cdot sign(w_i)=0 \\tag 2\n$$\n\né’ˆå¯¹ä¸Šé¢çš„å…¬å¼(1)å¯ä»¥å¾—åˆ°å¦‚ä¸‹ç»“è®º(æ³¨æ„ï¼šå½“å‰çš„ç›®æ ‡æ˜¯å¯»æ‰¾ä¸€ä¸ª$\\tilde w$ä½¿ $\\hat{J}(\\boldsymbol w) $å–æœ€å°å€¼)ï¼š\n\n1. å¯ä»¥çœ‹åˆ°å¼(1)ä¸­çš„äºŒæ¬¡å‡½æ•°æ˜¯å…³äº$\\hat w$å¯¹ç§°çš„ï¼Œæ‰€ä»¥è‹¥è¦ä½¿å¼(1)æœ€å°ï¼Œé‚£ä¹ˆå¿…æœ‰ï¼š$|w_i|\\lt|\\hat w|$ï¼Œå› ä¸ºåœ¨äºŒæ¬¡é¡¹$\\frac 12 H_{i,i}(w_i-\\hat w_i)^2$ä¸å˜çš„å‰æä¸‹ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—$\\alpha|w_i|$æ›´å°ã€‚\n2. $sign(w_i)=sign(\\hat w_i)$æˆ–$w_i=0$ï¼Œå› ä¸ºåœ¨$\\alpha|w_i|$ä¸å˜çš„æƒ…å†µä¸‹ï¼Œ$sign(w_i)=sign(\\hat w_i)$æˆ–$w_i=0$å¯ä»¥ä½¿å¼(1)æ›´å°ã€‚\n\n\nå½“$sign(w_i)=sign(\\hat w_i)$æ—¶ç»“åˆå…¬å¼(2)ï¼Œå¾—ï¼š\n\n$$\n\\begin {aligned}\n& H_{i,i}(w_i-\\hat w_i)+\\alpha \\cdot sign(\\hat w_i) = 0 \\\\\nw_i &= \\hat w_i - \\frac {\\alpha}{H_{i,i}} \\sign(\\hat w_i) \\\\\n       & = \\sign(\\hat w_i) |\\hat w_i|  - \\frac {\\alpha}{H_{i,i}} \\sign(\\hat w_i) \\\\\n       & = \\sign(\\hat w_i)(|\\hat w_i| -  \\frac {\\alpha}{H_{i,i}})\n\\end {aligned}\n$$\n\nä½†æ˜¯è¿™ä¸ªå…¬å¼ä¸èƒ½æ»¡è¶³$sign(w_i)=sign(\\hat w_i)$è¿™ä¸ªæ¡ä»¶ï¼Œå› ä¸º$|\\hat w_i| \\lt \\frac{\\alpha}{H_{i,i}}$æ—¶ï¼Œ$sign(w_i)= - sign(\\hat w_i)$ï¼Œè¿™å°±æ˜¯$w_i=0$æ—¶çš„æ¡ä»¶ã€‚ä½¿ $\\hat{J}(\\boldsymbol w) $å–æœ€å°å€¼çš„$\\tilde w$ çš„ç¬¬$i$ç»´æ˜¯ï¼š\n\n$$\n\\tilde w_i = w_i = \\left \\{ \n      \\begin {aligned}\n             &\\sign(\\hat w_i)(|\\hat w_i| -  \\frac {\\alpha}{H_{i,i}})& , & |\\hat w_i| >  \\frac {\\alpha}{H_{i,i}} \\\\\n             &0                                                                            & , & |\\hat w_i| \\le  \\frac {\\alpha}{H_{i,i}} \\\\\n      \\end {aligned}\n\\right .\n$$\n\nå°†ä¸Šå¼ä¸­çš„åˆ†æ®µå‡½æ•°åˆå¹¶ä¸€ä¸‹å¾—åˆ°å¦‚ä¸‹å…¬å¼ï¼š\n\n$$\n\\tilde w_i = sign(\\hat w_i) \\max\\left\\{ |\\hat w_i| - \\frac{\\alpha}{H_{i,i}},0 \\right\\}\n$$\n\næ ¹æ®ä»¥ä¸Šå…¬å¼æ€»ç»“å¦‚ä¸‹ï¼š\n- 1.è‹¥$|\\hat w_i| \\leq \\frac{\\alpha}{H_{i,i}}$ï¼Œ$\\tilde w_i=0$ ã€‚æ­¤æ—¶$\\hat{J}(\\boldsymbol w) $ä¸­çš„æ­£åˆ™åŒ–é¡¹èµ·åˆ°çš„ä¸»å¯¼ä½œç”¨ã€‚\n- 2.è‹¥$|\\hat w_i| \\gt \\frac{\\alpha}{H_{i,i}}â€‹$ï¼Œ$\\tilde w_i = \\sign(\\hat w_i)(|\\hat w_i| -  \\frac {\\alpha}{H_{i,i}})â€‹$ ï¼Œæ­£åˆ™åŒ–ç»“æœ$\\tilde w_iâ€‹$æ˜¯å°†$\\hat w_iâ€‹$æ²¿ç€ç¬¬$iâ€‹$ä¸ªåæ ‡è½´å‘è¯¥è½´çš„åŸç‚¹æ–¹å‘ç§»åŠ¨$\\frac{\\alpha}{H_{i,i}}â€‹$å¾—åˆ°çš„ã€‚\n\n\n2. **å‡ ä½•è§’åº¦**\n\n\n\n![L1_regularization](Norm_Regularization_for_DeepLearning-Neural_networks/L1_regularization.png)\n\n\n\nè“çº¿æ¤­åœ†è¡¨ç¤ºæ²¡æœ‰æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°çš„ç­‰å€¼çº¿ã€‚çº¢çº¿åœ†åœˆè¡¨ç¤º$L^2$æ­£åˆ™åŒ–é¡¹çš„ç­‰å€¼çº¿ã€‚åœ¨$\\tilde w$ç‚¹ï¼ŒåŸå§‹ç›®æ ‡å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹çš„å’Œè¾¾åˆ°æœ€å°å€¼ã€‚\n\n\n# 3 å…³äºè¿‘ä¼¼\n\nåœ¨è¿™ä¸€èŠ‚ä»‹ç»$L^1$èŒƒæ•°å’Œ$L^2$èŒƒæ•°æ­£åˆ™åŒ–é¡¹å¯¹ç›®æ ‡å‡½æ•°çš„å½±å“æ—¶éƒ½é‡‡ç”¨äº†è¿‘ä¼¼ã€‚æˆ‘ä¸ªäººå¯¹äºè¿™ä¸€ç‚¹æ˜¯å¾ˆç–‘æƒ‘çš„ï¼Œæˆ‘æ€è€ƒçš„ç»“è®ºæ˜¯è¿™æ ·çš„ï¼š\n\n- æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼šå‡è®¾$\\tilde w$ æ˜¯æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€ç»ˆçœ‹åˆ°çš„æ˜¯$\\tilde {w}_i$å’Œ$\\hat w_i$çš„ä»£æ•°å…³ç³»ã€‚\n- è¿‘ä¼¼çš„ä½œç”¨æ˜¯ä½¿ç”¨**ç‰¹æ®Šæƒ…å½¢ä¸‹çš„ç›®æ ‡å‡½æ•°** æ¥ä»£æ›¿ **ä¸€èˆ¬åŒ–çš„ç›®æ ‡å‡½æ•°**è¿›è¡Œé—®é¢˜è®ºè¿°çš„ï¼Œå› ä¸ºä¸€èˆ¬åŒ–çš„ç›®æ ‡å‡½æ•°å¾ˆéš¾ç›´è§‚çš„è¯´æ˜é—®é¢˜(å³ï¼Œ$\\tilde {w}_i$å’Œ$\\hat w_i$çš„ä»£æ•°å…³ç³»)ã€‚\n- åœ¨$L^2$èŒƒæ•°æ­£åˆ™åŒ–é—®é¢˜ä¸­ï¼Œç‰¹æ®Šæƒ…å½¢ä¸‹çš„ç›®æ ‡å‡½æ•°æŒ‡çš„æ˜¯åŸå§‹ç›®æ ‡å‡½æ•°æ­£å¥½æ˜¯äºŒæ¬¡å‡½æ•°(quadratic)ã€‚\n- åœ¨$L^1$èŒƒæ•°æ­£åˆ™åŒ–é—®é¢˜ä¸­ï¼Œç‰¹æ®Šæƒ…å½¢ä¸‹çš„ç›®æ ‡å‡½æ•°æŒ‡çš„æ˜¯åŸå§‹ç›®æ ‡å‡½æ•°æ­£å¥½æ˜¯äºŒæ¬¡å‡½æ•°(quadratic)ï¼Œå¹¶ä¸”è®­ç»ƒæ•°æ®é›†çš„å„ç»´åº¦çš„ç‰¹å¾æ˜¯ä¸ç›¸å…³çš„ï¼ˆæ¯”å¦‚ä½¿ç”¨äº†PCAç®—æ³•å¯¹è®­ç»ƒæ•°æ®åšäº†é¢„å¤„ç†ï¼‰ã€‚\n- æˆ‘ä»¬å¾—å‡ºçš„ç»“è®ºéƒ½æ˜¯åœ¨è¿™äº›ç‰¹æ®Šæƒ…å½¢ä¸‹å¾—å‡ºçš„ã€‚\n\nè¿™ç§æƒ³æ³•å¯èƒ½ä¸å¯¹ï¼Œå…ˆæ”¾åœ¨è¿™é‡Œï¼Œå¦‚æœä»¥åæœ‰æ›´å¥½çš„ç†è§£åœ¨å›æ¥ä¿®æ”¹ã€‚\n\n\n\n# 4 tensorflowå®ç°ä»£ç \n\nä¸‹é¢ä½¿ç”¨æˆ‘ä» [https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit](https://tensorflow.google.cn/tutorials/keras/overfit_and_underfit)ä¸­æŠ½å–å‡ºæ¥çš„éƒ¨åˆ†ä»£ç å±•ç¤º æ­£åˆ™åŒ–åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æ³›åŒ–ä½œç”¨ï¼Œé¡ºå¸¦ä¹Ÿå±•ç¤ºä¸€ä¸‹dropout åœ¨æ¨¡å‹æ³›åŒ–ä¸­çš„ä½œç”¨\n\n\n\n```python\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# download data\nNUM_WORDS = 10000\n(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\ndef multi_hot_sequences(sequences, dimension):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, word_indices in enumerate(sequences):\n        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n    return results\ntrain_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\ntest_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n```\n\n\n```python\nbaseline_model = keras.Sequential([\n    # `input_shape` is only required here so that `.summary` works.\n    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(16, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nbaseline_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nbaseline_history = baseline_model.fit(train_data,\n                                      train_labels,\n                                      epochs=15,\n                                      batch_size=512,\n                                      validation_data=(test_data, test_labels),\n                                      verbose=0)\n```\n\n```python\nl2_model = keras.models.Sequential([\n    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n                       activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n                       activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nl2_model.compile(optimizer='adam',\n                 loss='binary_crossentropy',\n                 metrics=['accuracy', 'binary_crossentropy'])\n\nl2_model_history = l2_model.fit(train_data, train_labels,\n                                epochs=15,\n                                batch_size=512,\n                                validation_data=(test_data, test_labels),\n                                verbose=0)\n```\n\n\n```python\ndpt_model = keras.models.Sequential([\n    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n    keras.layers.Dropout(rate=0.5),\n    keras.layers.Dense(16, activation=tf.nn.relu),\n    keras.layers.Dropout(rate=0.5),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\ndpt_model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy','binary_crossentropy'])\n\ndpt_model_history = dpt_model.fit(train_data, train_labels,\n                                  epochs=20,\n                                  batch_size=512,\n                                  validation_data=(test_data, test_labels),\n                                  verbose=0)\n```\n\n```python\ndef plot_history(histories, key='binary_crossentropy'):\n    plt.figure(figsize=(16, 10))\n    for name, history in histories:\n        val = plt.plot(history.epoch, history.history['val_' + key],\n                       '--', label=name.title() + ' Val')\n        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n                 label=name.title() + ' Train')\n    plt.xlabel('Epochs')\n    plt.ylabel(key.replace('_', ' ').title())\n    plt.legend()\n    plt.xlim([0, max(history.epoch)])\n```\n\n\n```python\nplot_history([('baseline', baseline_history),\n              ('l2', l2_model_history)])\n```\n\n\n![png](Norm_Regularization_for_DeepLearning-Neural_networks/L2.png)\n\n```python\nplot_history([('baseline', baseline_history),\n              ('dropout', dpt_model_history)])\n```\n\n\n![png](Norm_Regularization_for_DeepLearning-Neural_networks/dropout.png)\n\nå¯ä»¥å‘ç°åŠ å…¥æ­£åˆ™åŒ–é¡¹çš„æ¨¡å‹ç›¸å¯¹äºbaselineçš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚\n\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\nDeep Learning\n\nçº¿æ€§ä»£æ•°-ç¬¬äº”ç‰ˆ-åŒæµ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Machine Learning"],"categories":["Neural Network"]},{"title":"è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°å’Œæ¨¡å‹æŸå¤±å‡½æ•°-ç¥ç»ç½‘ç»œ","url":"%2Fblog%2FOutput_Units_and_Cost_Functions-Neural_networks.html","content":"\næŸå¤±å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œè®¾è®¡ä¸­çš„é‡è¦ä¸€ç¯ã€‚åœ¨ç¥ç»ç½‘ç»œçš„è®¾è®¡ä¸­ï¼Œè¾“å‡ºå±‚çš„ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°å’Œæ¨¡å‹æŸå¤±å‡½æ•°æ˜¯æ ¹æ®å®é™…é—®é¢˜è®¾è®¡çš„ã€‚æœ¬æ–‡ä»¥æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸ºç†è®ºåŸºç¡€ï¼Œé˜è¿°å›å½’é—®é¢˜ã€äºŒåˆ†ç±»é—®é¢˜å’Œå¤šåˆ†ç±»é—®é¢˜ä¸­è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°å’Œæ¨¡å‹æŸå¤±å‡½æ•°çš„é€‰æ‹©ã€‚\n\nç¬¦å·å®šä¹‰ï¼š\n\n- è®­ç»ƒé›†ç‰¹å¾$\\matrix {X}=\\{x^{(1)},x^{(2)} \\dots x^{(N)} \\}$ ï¼Œç¬¬$n$ä¸ªæ ·æœ¬çš„å±æ€§ç‰¹å¾å‘é‡ä¸º$x^{(n)}$ ï¼›\n\n- è®­ç»ƒé›†ç›®æ ‡å€¼ $T=\\{t^{(1)}, t^{(2)} \\dots t^{(N)} \\}$ ï¼Œç¬¬$n$ä¸ªæ ·æœ¬çš„labelå€¼ä¸º$t^{(n)}$ ï¼›\n  å¦‚æœæ˜¯å›å½’é—®é¢˜ï¼š$t^{(n)}$ä¸ºä¸€ä¸ªå®æ•°ï¼›\n  å¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼š$t^{(n)}$ä¸ºä¸€ä¸ªone-hotç¼–ç å‘é‡ï¼Œå¦‚äºŒåˆ†ç±»ä¸­ç¬¬$i$ä¸ªæ ·æœ¬å±äºç¬¬0ä¸ªç±»åˆ«æ—¶$t^{(n)}=(1, 0)$ æˆ–è€…$t_0^{(n)}=1, t_1^{(n)}=0$ ã€‚\n\n- $X$å’Œ$T$å…±åŒç»„æˆäº†è®­ç»ƒé›†ã€‚\n\n- é€šè¿‡ä¸€ä¸ªæ¨¡å‹$y=y(x, w)$ è¿›è¡Œé¢„æµ‹ï¼šæ ¹æ®æ ·æœ¬çš„å±æ€§ç‰¹å¾$x^{(n)}$é¢„æµ‹æ ·æœ¬çš„labelå€¼ï¼Œé¢„æµ‹å€¼ä¸º$y$ ï¼Œé¢„æµ‹å€¼çš„é›†åˆä¸º$Y=\\{y^{(1)},y^{(2)} \\dots y^{(N)} \\}$\n  å¦‚æœæ˜¯å›å½’é—®é¢˜ï¼šé¢„æµ‹å€¼$y$ä¸ºä¸€ä¸ªå®æ•°ï¼›\n  å¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼Œé¢„æµ‹å€¼$y$ä¸ºä¸€ä¸ªå‘é‡ï¼Œå„ç»´åº¦é€šå¸¸ä¸ºæ¦‚ç‡å€¼ã€‚å¦‚é€»è¾‘å›å½’(äºŒåˆ†ç±»)(0.3, 0.7)ã€softmax(å¤šåˆ†ç±»)(0.1, 0.2, 0.7)ã€‚\n\n- æŸå¤±å‡½æ•°$E(\\theta) = g(T, Y)$ æ˜¯ç”¨æ¥è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼å’Œæ ·æœ¬çœŸå®å€¼ç›¸è¿‘ç¨‹åº¦çš„ï¼Œæ¨¡å‹é¢„æµ‹å€¼å’Œæ ·æœ¬çœŸå®å€¼è¶Šç›¸è¿‘ï¼Œåˆ™æ¨¡å‹è¶Šå¥½ã€‚å…¶ä¸­$Y$ æ˜¯é€šè¿‡å‚æ•°ä¸º$\\theta$çš„æ¨¡å‹é¢„æµ‹å¾—åˆ°çš„ã€‚ï¼ˆå½“ç„¶è¿˜æœ‰æ­£åˆ™åŒ–é—®é¢˜ï¼Œæœ¬æ–‡å…ˆä¸è€ƒè™‘æ­£åˆ™åŒ–ï¼‰\n\n  â€‹\n\n  note: *ä¸‹æ–‡ä¸­çš„ä¸Šæ ‡è¡¨ç¤ºæ ·æœ¬åœ¨æ ·æœ¬åœ¨æ ·æœ¬é›†ä¸­çš„åºå·ï¼Œä¸‹æ ‡è¡¨ç¤ºå‘é‡çš„åºå·ã€‚åœ¨å¿…è¦çš„æ—¶å€™ä¸ºäº†ä½¿å…¬å¼ç®€æ´ä¼šçœç•¥ä¸Šæ ‡*\n\n\n\n# 1. Maximum Likehook Estimator\n\næœ€å¤§ä¼¼ç„¶å‡½æ•°åº”è¯¥æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€ä¸ºå¸¸è§çš„æŸå¤±å‡½æ•°äº†ã€‚\n\n**æœ€å¤§ä¼¼ç„¶ç†è®ºè®¤ä¸ºï¼Œæ ·æœ¬ä»æœä»åˆ†å¸ƒ$p(\\theta)$ çš„æ€»ä½“éšæœºæŠ½å–çš„($\\theta$æ˜¯åˆ†å¸ƒçš„å‚æ•°ï¼ŒæœªçŸ¥çš„)ï¼š**\n**é‚£ä¹ˆæ ·æœ¬é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬ä¹Ÿæœä»äºåˆ†å¸ƒ$p(\\theta)$ ã€‚** å³ï¼š\n$$\np(t) = p(t|\\theta)\n$$\n**è¿™ä¸ªåˆ†å¸ƒ$p(\\theta)$æ˜¯èƒ½å¤Ÿä½¿æ ·æœ¬çš„è”åˆåˆ†å¸ƒæœ€å¤§çš„åˆ†å¸ƒã€‚** å³ï¼š\n$$\np(T| \\theta) =\\prod_{n=1}^N {p(t^{(n)}|\\theta)}\n$$\n\nè¯¥å…¬å¼ç§°ä¸º**ä¼¼ç„¶å‡½æ•°(Likehook Function)**ã€‚ç”±äºåˆ†å¸ƒ$p(\\theta)$æ˜¯ä»¥$\\theta$ä¸ºå‚æ•°çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æœªçŸ¥æ•°$\\theta$æ˜¯èƒ½å¤Ÿä½¿ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„$\\theta_{ML}$:\n$$\n\\theta_{ML}={\\arg\\max}_\\theta \\prod_{n=1}^N {p(t^{(n)}|\\theta)}\n$$\n\n\nå¯¹ä¸¤è¾¹å–å¯¹æ•°å¾— **å¯¹æ•°ä¼¼ç„¶å‡½æ•° (Log Likehook Function)**:\n$$\n\\begin {aligned}\n\\ln p(T| \\theta) &=\\sum_{n=1}^N \\ln{p(t^{(n)}|\\theta)} \\\\\n\\theta_{ML} &={\\arg\\max}_\\theta \\sum_{n=1}^N \\ln{p(t^{(n)}|\\theta)}\n\\end {aligned}\n$$\n\næˆ‘ä»¬å¸¸è§çš„å¤šç§æŸå¤±å‡½æ•°éƒ½å¯ä»¥é€šè¿‡è¿™ä¸ªæœ€å¤§ä¼¼ç„¶ç†è®ºæ¨å¯¼è·å¾—ã€‚å…¶åŒºåˆ«åœ¨äºå¯¹$p(\\theta)$çš„ä¸åŒè§£é‡Šæ–¹å¼ã€‚\n\n# 2. å›å½’é—®é¢˜\n\nåœ¨å›å½’é—®é¢˜ä¸­ï¼Œå¯ä»¥è®¤ä¸º$\\theta$æ˜¯ç”±å˜é‡$x^{(i)}$å’Œå›å½’æ¨¡å‹å‚æ•°$w$ç»„æˆçš„ã€‚å¹¶ä¸”ç›®æ ‡å€¼$t$æœä»**é«˜æ–¯åˆ†å¸ƒ**ã€‚\n$$\np(t|x,w)=N(t|y(x,w),\\beta^{-1})\n$$\n$\\beta$ä¸ºé«˜æ–¯å™ªå£°ã€‚\n\né‚£ä¹ˆå…¶ä¼¼ç„¶å‡½æ•°å’Œå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸ºï¼š\n$$\n\\begin {aligned}\np(T|X,w)      &=\\prod_{n=1}^N  N(t^{(n)}|y(x^{(n)},w),\\beta^{-1}) \\\\\n\\\\\n\\ln p(T|X,w)&=\\sum_{n=1}^N \\ln N(t^{(n)}|y(x^{(n)},w),\\beta^{-1}) \\\\\n                     &=\\frac N2 \\ln \\beta -\\frac N2  \\ln(2\\pi) - \\beta E_D(w) \\\\\n                     &=\\frac N2 \\ln \\beta -\\frac N2  \\ln(2\\pi) - N \\beta E_{MSE}(w) \\\\\n\\\\\nE_D(w)         &=\\frac 12 \\sum_{n=1}^N (t^{(n)} - y(x^{(n)} ,w))^2 \\\\\n\\\\\nE_{MSE}(w)         &=\\frac 1N \\sum_{n=1}^N (t^{(n)} - y(x^{(n)} ,w))^2\n\\end {aligned}\n$$\n$E_D$å°±æ˜¯è¢«ç§°ä¸º**sum-of-squares error function**çš„ä¸€ç§æŸå¤±å‡½æ•°ï¼Œ$E_{MSE}$å°±æ˜¯è¢«ç§°ä¸º**mean squared error function(MSE)**çš„ä¸€ç§æŸå¤±å‡½æ•° ã€‚ä½¿å¯¹æ•°ä¼¼ç„¶å‡½æ•°æœ€å¤§å€¼$w$å°±æ˜¯ä½¿$E_D$æˆ–$E_{MSE}$æœ€å°çš„$w$ ã€‚(åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒçš„æ˜¯æ¨¡å‹å‚æ•°$w$ï¼Œ$\\beta$ä¹Ÿå¯ä»¥æ±‚è§£ï¼Œæ­¤å¤„å°±ä¸æäº†ï¼Œæœ‰éœ€è¦çš„è¯å¯ä»¥å‚è€ƒPRML 3.1.1æˆ–5.2)\n\nä½¿ä¼¼ç„¶å‡½æ•°æˆ–è€…$E_D$å¯¹$w$çš„å¯¼æ•°ä¸º0ï¼Œæ±‚å¾—ï¼š\n$$\nw_{ML} = (X^TX)^{-1}X^TT\n$$\nè¿™ä¸ªå…¬å¼å°±æ˜¯**æœ€å°äºŒä¹˜æ³•**çš„å…¬å¼ã€‚\n\n\n# 3. äºŒåˆ†ç±»é—®é¢˜\n\nåœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå¯ä»¥è®¤ä¸º$\\theta$æ˜¯ç”±å˜é‡$x^{(i)}$å’Œæ¨¡å‹å‚æ•°$w$ç»„æˆçš„ã€‚å¹¶ä¸”ç›®æ ‡å€¼$t^{(i)}$æœä»**Bernoulli distributionï¼ˆä¼¯åŠªåŠ›åˆ†å¸ƒï¼Œ0-1åˆ†å¸ƒï¼‰**ã€‚$t^{(i)}$çš„å–å€¼ä¸º0æˆ–1ã€‚\n\nä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†äºŒåˆ†ç±»é—®é¢˜æ—¶ï¼Œè¾“å‡ºå±‚é€šå¸¸ä½¿ç”¨sigmoidå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚è€Œåœ¨ä¼ ç»Ÿçš„é€»è¾‘å›å½’æ¨¡å‹ä¸­ä¹Ÿæ˜¯ä½¿ç”¨çš„sigmoidå‡½æ•°ã€‚å³ï¼š\n$$\ny = \\sigma(a) \\equiv \\frac 1{1+exp(-a)}\n$$\nsigmoidå‡½æ•°çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§å°±æ˜¯å…¶è¾“å‡ºå€¼è¡¨ç¤ºçš„æ˜¯æ¦‚ç‡ï¼Œå³ï¼š$y(x,w)$è¡¨ç¤ºå–$t=1$çš„æ¦‚ç‡ï¼Œè€Œ$(1-y(x,w))$è¡¨ç¤ºå–$t=0$çš„æ¦‚ç‡ã€‚æ‰€ä»¥å¯¹äºä»»ä½•ä¸€ä¸ªæ ·æœ¬$\\{ x,t \\}$ æ»¡è¶³å‚æ•°ä¸º$y(x,w)$çš„ä¼¯åŠªåŠ›åˆ†å¸ƒ:\n$$\np(t|x,w)=y(x,w)^t \\{1-y(x,w)\\}^{(1-t)}\n$$\né‚£ä¹ˆå…¶ä¼¼ç„¶å‡½æ•°å’Œå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸ºï¼š\n$$\n\\begin {aligned}\np(T|X,w)     &= \\prod_{n=1}^N  y(x^{(n)},w)^{t^{(n)}} \\{1-y(x^{(n)},w)\\}^{(1-t^{(n)})}  \\\\\n\\\\\n\\ln p(T|X,w)&= \\sum_{n=1}^N \\{ t^{(n)} \\ln y(x^{(n)},w) + (1-t^{(n)}) \\ln [1-y(x^{(n)},w)]\\} \\\\\n\\\\\nE(w)             &= - \\ln p(T|X,w) \\\\\n                    &= - \\sum_{n=1}^N \\{ t^{(n)} \\ln y(x^{(n)},w) + (1-t^{(n)}) \\ln [1-y(x^{(n)},w)]\\}\n\\end {aligned}\n$$\næ­¤å¤„çš„$E(w)$å°±æ˜¯æˆ‘ä»¬å¸¸ç”¨åˆ°çš„**äº¤å‰ç†µæŸå¤±å‡½æ•°(cross-entropy error function)** ï¼Œæ˜¯å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„è´Ÿæ•°ã€‚\n\n# 4. å¤šåˆ†ç±»é—®é¢˜\n\né¦–å…ˆä»‹ç»ä¸€ä¸‹**Multinoulliåˆ†å¸ƒ**ã€‚åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼Œæ ·æœ¬çš„ç›®æ ‡å€¼æœ‰$K$ç§å¯èƒ½çš„äº’æ–¥å–å€¼ï¼Œæ‰€ä»¥æ¯ä¸ªç›®æ ‡å€¼$t^{(n)}â€‹$å¯ä»¥ä½¿ç”¨$Kâ€‹$ç»´å‘é‡çš„one-hotç¼–ç ã€‚å¦‚$K=6$ï¼Œä¸€ä¸ªæ ·æœ¬å±äºç¬¬3ä¸ªç±»åˆ«ï¼Œåˆ™$t_3=1, t_k=1,(1 \\le k \\le K ,k  \\ne 3)$ï¼Œå³ï¼š\n$$\n\\begin {aligned}\n& t =(0,0,1,0,0,0) \\\\\n& \\sum_{k=1}^K t_k = 1\n\\end {aligned}\n$$\né‚£ä¹ˆä»æ€»ä½“æŠ½æ ·æ—¶ï¼Œä¸€ä¸ªæ ·æœ¬å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ª$K$ç»´å‘é‡$\\mu$è¡¨ç¤ºï¼Œ$\\mu_k$è¡¨ç¤º$t_k = 1$çš„æ¦‚ç‡ï¼Œä¸”$\\sum_{k=1}^K\\mu_k =1$ ã€‚\n\né‚£ä¹ˆä¸€ä¸ªæ ·æœ¬$t$æœä»çš„åˆ†å¸ƒå°±æ˜¯ï¼š\n$$\n\\begin {aligned}\np(t) &= p(t | \\mu)  \\\\\n       &= \\prod_{k=1}^K p(t_k= 1 | \\mu_k) \\\\\n       &=\\prod_{k=1}^K \\mu_k^{ t_k}\n\\end {aligned}\n$$\nå…¶ä¸­ï¼š\n$$\n\\begin {aligned}\nt  &= (t_1,t_2 \\dots t_K)^T \\\\\n\\mu &= (\\mu_1,\\mu_2 \\dots \\mu_K)^T\n\\end {aligned}\n$$\nä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤šåˆ†ç±»é—®é¢˜æ—¶ï¼Œè¾“å‡ºå±‚é€šå¸¸ä½¿ç”¨softmaxå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚å³ï¼š\n$$\ny_k(x,w) = \\frac {\\exp(a_k(x,w))}{\\sum_j^K \\exp(a_j(x,w))} , (1 \\le k \\le K)\n$$\nsoftmaxå‡½æ•°çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§å°±æ˜¯å…¶è¾“å‡ºå€¼è¡¨ç¤ºçš„æ˜¯æ¦‚ç‡ï¼Œå³:$y_k(x,w)$ å’Œä¸Šæ–‡çš„ç¦»æ•£å‹åˆ†å¸ƒä¸­çš„$\\mu_k$ çš„æ„ä¹‰æ˜¯ä¸€æ ·ï¼Œéƒ½è¡¨ç¤º$t_k=1$çš„æ¦‚ç‡ã€‚æ‰€ä»¥å¯¹äºä»»ä½•ä¸€ä¸ªæ ·æœ¬$\\{ x,t \\}$ :\n$$\n\\begin {aligned}\np(t|x,w)   &=\\prod_{k=1}^K y_k^{ t_k} \\\\\n                 &=\\prod_{k=1}^K y_k(x,w)^{ t_k}\n\\end {aligned}\n$$\n\né‚£ä¹ˆå…¶ä¼¼ç„¶å‡½æ•°å’Œå¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸ºï¼š\n$$\n\\begin {aligned}\np(T|X,w)         & = \\prod_{n=1}^N \\prod_{k=1}^K y_k(x^{(n)},w)^{ t_k^{(n)}} \\\\\n\\\\\n\\ln p(T|X,w)   & = \\sum_{n=1}^N \\sum_{k=1}^K t_k^{(n)} \\ln y_k(x^{(n)},w)\\\\\n\\\\\nE(w)                & = - \\ln p(T|X,w) \\\\\n                       & = - \\sum_{n=1}^N \\sum_{k=1}^K t_k^{(n)} \\ln y_k(x^{(n)},w)\n\\end {aligned}\n$$\næ­¤å¤„çš„$E(w)$å°±æ˜¯æˆ‘ä»¬å¸¸ç”¨åˆ°çš„**äº¤å‰ç†µæŸå¤±å‡½æ•°(cross-entropy error function)** ï¼Œæ˜¯å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„è´Ÿæ•°ã€‚\n\näºŒåˆ†ç±»é—®é¢˜å¯ä»¥çœ‹åšæ˜¯ä¸€ç§ç‰¹æ®Šçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œæ‰€ä»¥åœ¨ç¥ç»ç½‘ç»œä¸­ä¹Ÿå¯ä»¥ä½¿ç”¨softmaxå‡½æ•°ä½œä¸ºäºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°ã€‚\n\n# 5. æ€»ç»“\n\nä»¥ä¸ŠæŸå¤±å‡½æ•°éƒ½æ˜¯åŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼çš„ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡æœ€å¸å¼•äººçš„åœ°æ–¹åœ¨äºï¼Œå®ƒè¢«è¯æ˜å½“æ ·æœ¬æ•°ç›®$N \\to \\infty$ æ—¶ï¼Œå°±æ”¶æ•›ç‡è€Œè¨€æ˜¯æœ€å¥½çš„æ¸è¿‘ä¼°è®¡ã€‚åœ¨åˆé€‚çš„æ¡ä»¶ä¸‹ï¼Œå½“è®­ç»ƒæ ·æœ¬æ•°ç›®è¶‹å‘äºæ— ç©·å¤§æ—¶ï¼Œå‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¼šæ”¶æ•›åˆ°å‚æ•°çš„çœŸå®å€¼ï¼Œè¿™ä¸ªæ¡ä»¶å°±æ˜¯ï¼š\n\n- çœŸå®åˆ†å¸ƒå¿…é¡»åœ¨æ¨¡å‹åˆ†å¸ƒæ—ä¸­ï¼Œå¦åˆ™æ— æ³•ä¼°è®¡æ­£å¼åˆ†å¸ƒã€‚å¦‚åœ¨å¤„ç†å›å½’é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬å‡è®¾çœŸå®åˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œæ±‚è§£è¿‡ç¨‹å°±æ˜¯ä¼°è®¡é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°çš„ï¼Œä½†æ˜¯å¦‚æœçœŸå®åˆ†å¸ƒä¸æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬æŒ‰ç…§é«˜æ–¯åˆ†å¸ƒæ¥æ±‚è§£ï¼Œå¾—åˆ°çš„ç»“æœè‚¯å®šå°±æ˜¯é”™è¯¯çš„ã€‚\n- çœŸå®åˆ†å¸ƒå¿…é¡»åˆšå¥½å¯¹åº”ä¸€ä¸ªæ¨¡å‹åˆ†å¸ƒçš„$\\theta$å€¼ï¼Œ\n\nç¥ç»ç½‘ç»œä¸­é’ˆå¯¹äºä¸åŒé—®é¢˜ä½¿ç”¨çš„è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°æ˜¯ä¸åŒçš„ï¼Œè¿›è€Œä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ï¼š\n\n|      |    è¾“å‡ºå±‚åˆ†å¸ƒ    | è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°  | è¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡ |                æŸå¤±å‡½æ•°                 |\n| :--: | :---------: | :------: | :------: | :---------------------------------: |\n|  å›å½’  |  Gaussian   | identity |    1     | sum-of-squares error function / MSE |\n| äºŒåˆ†ç±»  |  Bernoulli  | sigmoid  |    1     |    cross-entropy error function     |\n| å¤šåˆ†ç±»  | Multinoulli | softmax  |    K     |    cross-entropy error function     |\n\näºŒåˆ†ç±»é—®é¢˜å¯ä»¥çœ‹åšæ˜¯ä¸€ç§ç‰¹æ®Šçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œæ‰€ä»¥è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°ä¹Ÿå¯ä»¥ä½¿ç”¨softmaxï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨å¤šåˆ†ç±»çš„ cross-entropy error functionã€‚\n\nå…¶ä»–ï¼š\n\nâ€‹\tè¿˜æœ‰é’ˆå¯¹äºå…¶ä»–é—®é¢˜çš„è®¾è®¡ï¼Œå¦‚ï¼šmultiple target variablesã€‚å½“é‡åˆ°çš„æ—¶å€™å†è¡¥å……\n\n# å‚çœ‹èµ„æ–™\n\nDeep Leaning\n\nPattern Recognition and Machine Learning","tags":["Machine Learning"],"categories":["Neural Network"]},{"title":"ä¿¡æ¯è®ºä¸­çš„ä¸€äº›çŸ¥è¯†ç‚¹","url":"%2Fblog%2FPoints-in-Information-Theory.html","content":"\n# ä¿¡æ¯é‡(è‡ªä¿¡æ¯)\n\nä¿¡æ¯å¥ åŸºäººé¦™å†œ(Shannon)è®¤ä¸ºâ€œä¿¡æ¯æ˜¯ç”¨æ¥æ¶ˆé™¤éšæœºä¸ç¡®å®šæ€§çš„ä¸œè¥¿â€. ä¸€ä¸ªäº‹ä»¶è•´å«çš„ä¿¡æ¯é‡ä¸è¿™æ¡ä¿¡æ¯èƒ½å¤Ÿæ¶ˆé™¤çš„ä¸ç¡®å®šæ€§æ˜¯æ­£ç›¸å…³çš„.  ä¿¡æ¯é‡åº”è¯¥æ»¡è¶³å¦‚ä¸‹æ¡ä»¶:\n\n1. äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šä½, ä¿¡æ¯é‡è¶Šå¤§; \n2. äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šé«˜, ä¿¡æ¯é‡è¶Šä½; \n3. å¤šä¸ªå¯¹ç«‹äº‹ä»¶åŒæ—¶å‘ç”Ÿ, æ€»ä¿¡æ¯é‡æ˜¯å¤šä¸ªäº‹ä»¶ä¿¡æ¯é‡ç›¸åŠ . \n\n\nç”±æ­¤ç¡®å®šçš„ä¿¡æ¯é‡è¡¨ç¤ºä¸º:\n$$\nI(x)=-\\log_{2}p(x)\n$$\n\nå…¶ä¸­, $p(x)$è¡¨ç¤ºéšæœºäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡. å¯ä»¥çœ‹å‡º, è‡ªä¿¡æ¯çš„è®¡ç®—å’Œéšæœºå˜é‡æœ¬èº«æ•°å€¼æ²¡æœ‰å…³ç³», åªå’Œå…¶æ¦‚ç‡æœ‰å…³. \n\n# ç†µ(entropy)\n\nå¯¹äºä¸€ä¸ªå˜é‡$X \\sim p$($X$æœä»$p$åˆ†å¸ƒ), è¯¥å˜é‡çš„ç†µæ˜¯æè¿°å…¶ä¸ç¡®å®šæ€§çš„é‡, è¡¨ç¤ºè¯¥å˜é‡è•´å«çš„ä¿¡æ¯é‡çš„æœŸæœ›(å¹³å‡ä¿¡æ¯é‡).\n\neg:å¯¹äºä¸€ä¸ªæœ‰kä¸ªçŠ¶æ€çš„ç¦»æ•£éšæœºå˜é‡$X$, æœ‰\n$$\nH(X)=-\\sum_{k=1}^Kp(X=k) \\log p(X=k)\n$$\na.å½“$\\log$ä»¥2ä¸ºåº•çš„æ—¶å€™ç§°ä¹‹ä¸ºbits,ç»“æœå¯ä»¥è§†ä¸ºå¤šå°‘ä¸ªäºŒè¿›åˆ¶ä½å¯ä»¥è¡¨ç¤ºè¯¥å˜é‡\n\nb.å½“\\logä»¥eä¸ºåº•çš„æ—¶ä¾¯ç§°ä¹‹ä¸º nats\n\nç†µåªä¾èµ–äºéšæœºå˜é‡çš„åˆ†å¸ƒ, ä¸éšæœºå˜é‡å–å€¼æ— å…³, æ‰€ä»¥ä¹Ÿå¯ä»¥å°† $X$ çš„ç†µè®°ä½œ$ H(p)$. \n\n$0log0=0$.  \n\nç†µçš„åŸºæœ¬æ€§è´¨:\n\n- éè´Ÿæ€§: $ğ»(ğ‘‹) â‰¥ 0$, ç­‰å·è¡¨æ˜ç¡®å®šåœº(æ— éšæœºæ€§)çš„ç†µæœ€å°;\n- åŸºè´¨æ€§: $ğ»(ğ‘‹) â‰¤ \\log |X|$, å…¶ä¸­ç­‰å·æˆç«‹å½“ä¸”ä»…å½“$p(x)=\\frac 1{\\mid x \\mid}$, è¿™é‡Œ$\\mid X \\mid$è¡¨ç¤ºé›†åˆ$X$ä¸­çš„å…ƒç´ ä¸ªæ•°. è¯¥æ€§è´¨è¡¨æ˜ç­‰æ¦‚åœºå…·æœ‰çš„æœ€å¤§ç†µ;\n- ç†µæè¿°äº†éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§, ç†µè¶Šå¤§ï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§å°±è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šæ··ä¹±;\n- ç†µæè¿°äº†éšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡;\n\n# è”åˆç†µå’Œæ¡ä»¶ç†µ\n\nè®¾$X$ã€$Y$æ˜¯ä¸¤ä¸ªç¦»æ•£å‹éšæœºå˜é‡, å®ƒä»¬çš„è”åˆåˆ†å¸ƒä¸º$ğ‘(ğ‘¥, ğ‘¦)$, åˆ™$X$ ã€$Y$çš„è”åˆç†µå®šä¹‰ä¸º\n$$\nH(X,Y)=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y)\n$$\nè®¾$X$ã€$Y$æ˜¯ä¸¤ä¸ªç¦»æ•£å‹éšæœºå˜é‡, å®ƒä»¬çš„è”åˆåˆ†å¸ƒä¸º$ğ‘(ğ‘¥, ğ‘¦)$, åˆ™ç»™å®šğ‘‹æ—¶ğ‘Œçš„æ¡ä»¶ç†µå®šä¹‰ä¸º\n$$\n\\begin {align}\nH(Y|X) & =\\sum_{x \\in X} p(x)H(y|X=x)                        \\\\\n       & =-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y|x) \\\\\n       & =E[-\\log (y \\mid x)]                                \\\\\n\\end {align}\n$$\nè”åˆç†µå’Œæ¡ä»¶ç†µçš„ç›´è§‚å«ä¹‰è§ä¸‹å›¾:\n\n![](Points-in-Information-Theory/cond-joint-entropy.png)\n\né“¾å¼è§„åˆ™: \n$$\nH(X, Y) = H(Y|X) + ğ»(X)\n$$\n\n$$\n\\begin {align}\nH(X_1, X_2, X_3) &= H(X_1) + ğ»(X_2, X_3 \\mid X_1) \\\\\n                 &= H(X_1) + ğ»(X_2 \\mid X_1) + ğ»(X_3 \\mid X_1, X_2) \\\\\n\\end {align}\n$$\n\nå®šç†: \n$$\n\\begin {align}\n& H(X \\mid Y) \\le H(X) \\\\\n& H(X, Y) \\le H(X) + H(Y) \n\\end {align}\n$$\n\n# äº¤å‰ç†µ (cross entropy)\n\nè®¾éšæœºå˜é‡$X$çš„åˆ†å¸ƒå¯†åº¦ä¸º$ğ‘(ğ‘¥)$, $ğ‘(ğ‘¥)$æ˜¯é€šè¿‡ç»Ÿè®¡æ‰‹æ®µå¾—åˆ°çš„$X$çš„è¿‘ä¼¼åˆ†å¸ƒ, åˆ™éšæœºå˜é‡$X$çš„äº¤å‰ç†µå®šä¹‰ä¸º\n$$\nH(X,q)=-\\sum_{x \\in X}p(x) \\log q(x)\n$$\n\n note: æœ‰çš„æ–‡ç« ä¸­äº¤å‰ç†µè¡¨ç¤ºä¸º $H(p, q)$ . è¯¥è¡¨ç¤ºæ–¹æ³•ä¸è”åˆç†µç›¸ä¼¼, éœ€æŒ‰ç…§ä¸Šä¸‹æ–‡ç¡®å®š.\n\näº¤å‰ç†µå¯ä»¥çœ‹ä½œæ˜¯å½“æˆ‘ä»¬ç”¨æ¨¡å‹ $q$ç¼–ç æ¥è‡ªæ¨¡å‹$p$çš„å˜é‡æ—¶æ‰€éœ€çš„å¹³å‡bits(å¦‚æœ$\\log$ä»¥2ä¸ºåº•çš„è¯)\n\næ‰€ä»¥, æœ‰$H(p)=H(p,p)$,æ‰€ä»¥KLè·ç¦»å°±å¯ä»¥çœ‹åšæ˜¯ï¼šç”¨æ¨¡å‹qæ¥ç¼–ç æ¥è‡ªæ¨¡å‹pçš„å˜é‡æ‰€éœ€çš„é¢å¤–bitsï¼\n\nè‹¥å­˜åœ¨$X$çš„çœŸå®åˆ†å¸ƒä¸º$p(x)$, å®ƒçš„ä¸¤ä¸ªè¿‘ä¼¼åˆ†å¸ƒä¸º$ğ‘_1(ğ‘¥)$ , $ğ‘_2(ğ‘¥)$ ,  å¹¶ä¸”$ğ»(p, q_1)< ğ»(p, q_2)$ , åˆ™$ğ‘_1$æ˜¯æ›´å¥½çš„è¿‘ä¼¼åˆ†å¸ƒ. \n\n## åº”ç”¨\n\n- æŸå¤±å‡½æ•°\n- è¯­è¨€æ¨¡å‹è¯„ä»·æ–¹æ³•(å®é™…ä¸Šæ˜¯ç†µç‡)\n\n# ç†µç‡\n\nç†µç‡å¯ä»¥ç›´è§‚çš„ç†è§£ä¸ºä¸€ä¸ªé•¿åº¦ä¸º$n$çš„éšæœºå˜é‡åºåˆ—ï¼Œè¯¥åºåˆ—çš„ç†µéš$n$å¢é•¿çš„å¢é•¿ç‡.\n\nå®šä¹‰: å½“å¦‚ä¸‹æé™å­˜åœ¨æ—¶, éšæœºè¿‡ç¨‹$\\{X_i\\}$ çš„ç†µç‡å®šä¹‰ä¸º:\n$$\nH(\\chi) = \\lim_{n \\to \\infty} \\frac 1nH(x_1, x_2, \\dots, x_n)\n$$\nåŒæ—¶å®šä¹‰ä¸€ä¸ªä¸ç†µç‡ç›¸å…³çš„é‡\n$$\nH'(\\chi)= \\lim_{n \\to \\infty}H(X_n|X_1,X_2,....X_{n-1})\n$$\n\næ€§è´¨: **å¯¹äºå¹³ç¨³éšæœºè¿‡ç¨‹ï¼Œä»¥ä¸Šä¸¤è€…æé™å‡å­˜åœ¨ï¼Œä¸”æœ‰ $H(\\chi)= H'(\\chi)$**\nè¯¥æ€§è´¨éå¸¸å¸¸ç”¨, ä¸ºäº†è¯æ˜è¿™ä¸ªæ€§è´¨, å¼•å…¥å¦‚ä¸‹ä¸¤ä¸ªå®šç†:\n\nå®šç†ä¸€: å¯¹äºå¹³ç¨³éšæœºè¿‡ç¨‹, $H(X_n|X_1,X_2,....X_{n-1})$ éš $n$ é€’å‡, ä¸”å­˜åœ¨æé™ $H'(\\chi)$.\n\nè¯æ˜ å¦‚ä¸‹:\n$$\n\\begin {align}\nH(X_{n+1} \\mid X_1,X_2,....X_n) & \\le H(X_{n+1} \\mid X_2,....X_n) \\\\\n                                & = H(X_n|X_1,X_2,....X_{n-1})\n \\end {align}\n$$\nä¸ç­‰å¼ç”±æ¡ä»¶ä½œç”¨ä½¿ç†µå‡å°è¿™ä¸ªæ€§è´¨å¾—åˆ°, ç­‰å¼ç”±å¹³ç¨³éšæœºè¿‡ç¨‹çš„æ€§è´¨å¾—åˆ°.\næ‰€ä»¥, $H'(\\chi)$ æ˜¯éè´Ÿä¸”é€’å‡çš„, å³:  $H'(\\chi)$ å­˜åœ¨.\n\nå®šç†äºŒ: (Cesaro å‡å€¼): è‹¥ $a_n \\to a$, ä¸” $b_n = \\frac 1n \\sum_{i=1}^n a_i$, åˆ™ $b_n \\to a$\nè¯æ˜ç•¥\n\nä¸‹é¢è¯æ˜ç†µç‡çš„æ€§è´¨: å¯¹äºå¹³ç¨³éšæœºè¿‡ç¨‹ï¼Œä»¥ä¸Šä¸¤è€…æé™å‡å­˜åœ¨ï¼Œä¸”æœ‰ $H(\\chi)= H'(\\chi)$ .\n\næœ‰è”åˆç†µçš„é“¾å¼æ³•åˆ™çŸ¥:\n$$\n\\frac 1n H(x_1, x_2, \\dots, x_n) =  \\frac 1n \\sum_{i=1}^n H(X_n|X_1,X_2,....X_{n-1})\n$$\nå³: ç†µç‡ä¸ºæ¡ä»¶ç†µçš„æ—¶é—´å¹³å‡.  ç”±å®šç†ä¸€çŸ¥ æ¡ä»¶ä¸Šè¶‹äºæé™ $H'(\\chi)$.\nåˆ ç”±Cesaro å‡å€¼å¯çŸ¥:\n$$\n\\begin {align}\nH(\\chi) & = \\lim_{n \\to \\infty} \\frac 1nH(x_1, x_2, \\dots, x_n) \\\\\n        & = \\lim_{n \\to \\infty}H(X_n|X_1,X_2, \\dots, X_{n-1})   \\\\\n        & = H'(\\chi)                                            \\\\\n\\end {align}\n$$\n\n**å¹³ç¨³é©¬å°”å¯å¤«é“¾çš„ç†µç‡**\n\nè®¾ $ \\{X_i\\}$ä¸ºå¹³ç¨³é©¬å°”å¯å¤«é“¾ï¼Œä¸”å…¶å¹³ç¨³åˆ†å¸ƒä¸º $\\mu$ , è½¬ç§»çŸ©é˜µä¸º $P$  , åˆ™ç†µç‡ä¸º:\n\n$$\n\\begin {align}\nH(\\chi) &= H'(\\chi) \\\\\n        &= \\lim_{n \\to \\infty}H(X_n|X_1,X_2,\\dots, X_{n-1}) \\\\\n        &= \\lim_{n \\to \\infty} H(X_n \\mid X_{n-1}) \\\\\n        &= H(X_2 \\mid X_1) \\\\\n        &= -\\sum_{ij} p(X_1 = \\mu_i, X_2 = \\mu_j) \\log p(X_2 = \\mu_j \\mid X_1 = \\mu_i) \\\\ \n        &= -\\sum_{ij} p(X_1 = \\mu_i) p(X_2 = \\mu_j \\mid X_1 = \\mu_i) \\log p(X_2 = \\mu_j \\mid X_1 = \\mu_i) \\\\ \n        &= -\\sum_{ij} \\mu_i P_{ij}\\log{P_{ij}} \\\\\n\\end {align}\n$$\n\nnote: **ç†µç‡** å¯ä»¥ç†è§£ä¸º **éšæœºè¿‡ç¨‹çš„ \"ç†µ\" **. \n\nåº”ç”¨:\n\n- è¯­è¨€æ¨¡å‹è¯„ä»·æ–¹æ³•\n\n# ç›¸å¯¹ç†µ\n\nè¿™ä¸ªå€¼æ˜¯ç”¨æ¥è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´ç›¸å¼‚åº¦çš„, \n\nè®¾$ğ‘(ğ‘¥)$ æ˜¯éšæœºå˜é‡ğ‘‹çš„çœŸå®åˆ†å¸ƒå¯†åº¦, $ğ‘(ğ‘¥)$æ˜¯é€šè¿‡ç»Ÿè®¡æ‰‹æ®µå¾—åˆ°çš„ğ‘‹ çš„è¿‘ä¼¼åˆ†å¸ƒ, åˆ™äºŒè€…é—´ç›¸å¯¹ç†µå®šä¹‰ä¸º:\n$$\nD_{KL}(p||q)=-\\sum_{k=1}^Kp_k \\log \\frac{p_k}{q_k}\n$$\nå¦‚æœæ˜¯è¿ç»­çš„éšæœºå˜é‡, æŠŠ$\\sum$ç”¨ç§¯åˆ† ç¬¦å·æ›¿æ¢å°±å¥½äº†\n\nå¯¹ä¸Šå¼è¿›è¡Œè½¬åŒ–ï¼š\n$$\nD_{KL}(p||q)=\\sum_{k}{p_k} \\log{p_k} -\\sum_kp_k \\log q_k=-H(p)+H(p,q)\n$$\nå…¶ä¸­$H(p,q)$ä¸ºäº¤å‰ç†µ, $p$ ä¸ºçœŸæ˜¯åˆ†å¸ƒ, $q$ä¸ºè¿‘ä¼¼åˆ†å¸ƒ.\n\nç›¸å¯¹ç†µä¹Ÿç§°ä½œ\nâ€“ Kullback-Leibler å‘æ•£åº¦ã€KLå‘æ•£åº¦\nâ€“ Kullback-Leibler è·ç¦»ã€KLè·ç¦»\n\nKLè·ç¦»å°±å¯ä»¥çœ‹åšæ˜¯ï¼š\n\n- ç›¸å¯¹ç†µæè¿°åŒä¸€ä¸ªéšæœºå˜é‡çš„ä¸åŒåˆ†å¸ƒçš„å·®å¼‚\n- ç›¸å¯¹ç†µæè¿°äº†å› ä¸ºé”™ç”¨åˆ†å¸ƒå¯†åº¦è€Œå¢åŠ çš„ä¿¡æ¯é‡\n- ç”¨æ¨¡å‹$q$æ¥ç¼–ç æ¥è‡ªæ¨¡å‹$p$çš„å˜é‡æ‰€éœ€çš„é¢å¤–bitsï¼\n- å› ä¸ºæ˜¯â€œé¢å¤–çš„â€, æ‰€ä»¥ KLçš„è·ç¦»çš„å€¼ä¸€å®šå¤§äº$0$, $D_{KL}=0$å½“ä¸”ä»…å½“$p=q$. \n\n\nåº”ç”¨äºæ¨èç³»ç»Ÿçš„ä¸€ä¸ªä¾‹å­\n\nåœ¨ä½¿ç”¨LDA(Latent Dirichlet Allocation)è®¡ç®—ç‰©å“çš„å†…å®¹ç›¸ä¼¼åº¦æ—¶, æˆ‘ä»¬å¯ä»¥å…ˆè®¡ç®—å‡ºç‰©å“åœ¨è¯é¢˜ä¸Šçš„åˆ†å¸ƒ, ç„¶ååˆ©ç”¨ä¸¤ä¸ªç‰©å“çš„è¯é¢˜åˆ†å¸ƒè®¡ç®—ç‰©å“çš„ç›¸ä¼¼åº¦. æ¯”å¦‚, å¦‚æœä¸¤ä¸ªç‰©å“çš„è¯é¢˜åˆ†å¸ƒç›¸ä¼¼, åˆ™è®¤ä¸ºä¸¤ä¸ªç‰©å“å…·æœ‰è¾ƒé«˜çš„ç›¸ä¼¼åº¦, åä¹‹åˆ™è®¤ä¸ºä¸¤ä¸ªç‰©å“çš„ç›¸ä¼¼åº¦è¾ƒä½. \n\n*note:* äº¤å‰ç†µå’Œç›¸å¯¹ç†µå…¶å®æ˜¯ç­‰ä»·çš„, é‚£ä¹ˆåœ¨å®é™…é—®é¢˜ä¸­è¯¥æ€ä¹ˆé€‰æ‹©å‘¢? \näº¤å‰ç†µå¤§é‡åº”ç”¨åœ¨sigmoidå‡½æ•°å’Œsoftmaxå‡½æ•°ä¸­ï¼Œæœ€å…¸å‹çš„ç®—æ³•åº”è¯¥å°±æ˜¯ç¥ç»ç½‘ç»œå’Œé€»è¾‘å›å½’å§ï¼Œè€Œç›¸å¯¹ç†µå¤§é‡åº”ç”¨åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä¾‹å¦‚GANã€EMã€è´å¶æ–¯å­¦ä¹ å’Œå˜åˆ†æ¨å¯¼ä¸­ã€‚ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹å‡º:\n\n- å¦‚æœæƒ³é€šè¿‡ç®—æ³•å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œæ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡ï¼Œé‚£ä¹ˆé€šå¸¸éƒ½æ˜¯ä½¿ç”¨ç›¸å¯¹ç†µï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ˜ç¡®çš„çŸ¥é“ç”Ÿæˆçš„åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„å·®è·ï¼Œæœ€å¥½çš„KLæ•£åº¦å€¼åº”è¯¥æ˜¯0ï¼›\n- è€Œåœ¨åˆ¤åˆ«æ¨¡å‹ä¸­ï¼Œä»…ä»…åªéœ€è¦è¯„ä¼°æŸå¤±å‡½æ•°çš„ä¸‹é™å€¼å³å¯ï¼Œäº¤å‰ç†µå¯ä»¥æ»¡è¶³è¦æ±‚ï¼Œå…¶è®¡ç®—é‡æ¯”KLæ•£åº¦å°ã€‚\n\n\n\n# äº’ä¿¡æ¯(Mutual Information)\n\näº’ä¿¡æ¯å¯ä»¥è¯„ä»·ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œè¿™ä¸»è¦å½’å› äºå…¶å¯¹ç§°æ€§ï¼Œå‡è®¾äº’ä¿¡æ¯ä¸å…·å¤‡å¯¹ç§°æ€§ï¼Œé‚£ä¹ˆå°±ä¸èƒ½ä½œä¸ºè·ç¦»åº¦é‡ï¼Œä¾‹å¦‚ç›¸å¯¹ç†µï¼Œç”±äºä¸æ»¡è¶³å¯¹ç§°æ€§ï¼Œæ•…é€šå¸¸è¯´ç›¸å¯¹ç†µæ˜¯è¯„ä»·åˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ï¼Œè€Œä¸ä¼šè¯´è·ç¦»ã€‚\n\näº’ä¿¡æ¯çš„å®šä¹‰ä¸ºï¼š**ä¸€ä¸ªéšæœºå˜é‡ç”±äºå·²çŸ¥å¦ä¸€ä¸ªéšæœºå˜é‡è€Œå‡å°‘çš„ä¸ç¡®å®šæ€§.**\n$$\n\\begin {align}\nI(X; Y) & = H(X) - H(X \\mid Y)=H(Y) - H(Y \\mid X) \\\\\n        & = H(X) + H(Y) - H(X, Y) \\\\\n        & = H(X, Y) - H(X \\mid Y) - H(Y \\mid X) \\\\\n\\end {align}\n$$\nå†æ¥çœ‹ä¸€ä¸‹è¿™å¼ å›¾ç†è§£ä¸€ä¸‹:\n\n![](Points-in-Information-Theory/cond-joint-entropy.png)\n\nå¯¹äºä¸‰ä¸ªå˜é‡çš„ è”åˆç†µ, æ¡ä»¶ç†µ, äº’ä¿¡æ¯ çš„ç›´è§‚å«ä¹‰è§ä¸‹å›¾:\n\n![](Points-in-Information-Theory/cond-joint-entropy-info.png)\n\näº’ä¿¡æ¯ç”¨äºè¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡åˆ†å¸ƒä¹‹é—´çš„**è·ç¦», ç›¸å…³æ€§, ç‹¬ç«‹æ€§**ã€‚\n\næˆ‘ä»¬çŸ¥é“å¦‚æœ$p(x,y)=p(x)p(y)$, åˆ™Xå’ŒYäº’ç›¸ç‹¬ç«‹(ä¸ç›¸å…³), åŸºäºè¿™ä¸ªå‰ææˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨$p(x,y)$ ä¸ $p(x)p(y)$ çš„å…³ç³» æ¥å®šä¹‰ ä¸¤ä¸ªéšæœºå˜é‡$x$å’Œ$y$ çš„ç›¸å…³æ€§(ç›¸äº’ç‹¬ç«‹çš„ç¨‹åº¦). \n\näº’ä¿¡æ¯ä½¿ç”¨ç›¸å¯¹ç†µåº¦é‡$ p(x,y)$ä¸$p(x)p(y)$ä¹‹é—´çš„å…³ç³»ï¼š\n$$\nI(X; Y) = D_{KL}(p(X,Y)||p(X)p(Y))= - \\sum_{x \\in X}\\sum_{y \\in Y}p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)}\n$$\nå¦‚: Xå’ŒYäº’ç›¸ç‹¬ç«‹, $p(x,y)=p(x)p(y)$, å¯¹æ•°é¡¹ä¸º0  åˆ™ $I(X, Y) = 0$. åä¹‹ ä¸¤ä¸ªå˜é‡ç›¸å…³æ€§è¶Šå¼º, åˆ™$I(X, Y)$ è¶Šå¤§.\n\näº’ä¿¡æ¯æ˜¯å¤§äºç­‰äº0çš„ï¼Œå½“ä¸”ä»…å½“Xä¸Yç›¸äº’ç‹¬ç«‹æ—¶å€™ç­‰å·æˆç«‹ã€‚\n\n## PMI\n\nå¦ä¸€ä¸ªä¸MIæœ‰å…³çš„çš„å®šä¹‰æ˜¯**pointwise mutual information(PMI),**\n$$\nPMI(x,y)=\\log \\frac{p(x,y)}{p(x)p(y)}=\\log \\frac{p(x|y)}{p(x)}=\\log \\frac{p(y|x)}{p(y)}\n$$\nå¯ä»¥å¾—çŸ¥ MIå€¼å…¶å®å°±æ˜¯PMIå€¼çš„**æœŸæœ›**, è¯¥æ•°å€¼è¡¡é‡äº†${p(x,y)}$ å’Œ ${p(x)p(y)}$ çš„å·®å¼‚æ€§, å³:  the discrepancy between these events occuring together compared to what would be expected by chance.\n\n \n\n## æœ€å¤§äº’ä¿¡æ¯ç³»æ•°(maximal information coefficient)\n\nä¸Šé¢ä»‹ç»çš„äº’ä¿¡æ¯éƒ½æ˜¯é’ˆå¯¹ç¦»æ•£éšæœºå˜é‡è®¡ç®—çš„, å¯¹äºè¿ç»­éšæœºå˜é‡äº’ä¿¡æ¯å®šä¹‰ä¸º:\n$$\nI(X, Y) = D_{KL}(p(X,Y)||p(X)p(Y))= - \\int_x \\int_y p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)} dx dy\n$$\n$pï¼ˆxï¼Œyï¼‰$æ˜¯è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒå‡½æ•°. \n\nåœ¨ç”Ÿäº§ç¯å¢ƒä¸‹, æˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°è¿™æ ·çš„æƒ…å†µ, æ•°æ®ç‰¹å¾æ˜¯è¿ç»­å‹ç‰¹å¾, ä½†æ˜¯æˆ‘ä»¬æ˜¯åŸºäºæœ‰é™ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æçš„. ä¸ºäº†è·å¾—è¿™äº›è¿ç»­å‹ç‰¹å¾(éšæœºå˜é‡)çš„æ¦‚ç‡å¯†åº¦å‡½æ•°é€šå¸¸æ¯”è¾ƒéº»çƒ¦(å¦‚ æ ¸å¯†åº¦ä¼°è®¡), å¯¹äºä¸¤ä¸ªå˜é‡çš„è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒçš„è®¡ç®—å°±æ›´åŠ çš„å¤æ‚äº†. å³: è¿ç»­éšæœºå˜é‡ çš„è”åˆæ¦‚ç‡è®¡ç®—ç›¸å¯¹æ¥è¯´æ¯”è¾ƒéº»çƒ¦. æ‰€ä»¥æˆ‘ä»¬å°±è¦æ‰¾ä¸€ä¸ªåŠæ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜.\n\nMIC å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„. MIC çš„æ€æƒ³æ˜¯å°†ä¸¤ä¸ªå˜é‡ç¦»æ•£åŒ–. æ–¹æ³•æ˜¯ å°†å½“å‰äºŒç»´ç©ºé—´åœ¨ x,y æ–¹å‘åˆ†åˆ«åˆ’åˆ†ä¸ºä¸€å®šçš„åŒºé—´æ•°ï¼Œç„¶åæŸ¥çœ‹å½“å‰çš„æ•°æ®ç‚¹(æ ·æœ¬)åœ¨å„ä¸ªæ–¹æ ¼ä¸­è½å…¥çš„æƒ…å†µï¼Œå¹¶ä¸”ä½¿ç”¨æ•£ç‚¹å›¾æ¥è¡¨ç¤ºï¼Œè¿™æ ·å°±è§£å†³äº†åœ¨äº’ä¿¡æ¯ä¸­çš„è”åˆæ¦‚ç‡éš¾æ±‚çš„é—®é¢˜ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹:\n\n1. ç»™å®šç½‘æ ¼åŒ–çš„åˆ—æ•° $c$ å’Œè¡Œæ•° $r$, å¯¹XYæ„æˆçš„äºŒç»´åæ ‡ç©ºé—´è¿›è¡Œ$c$åˆ—$r$è¡Œç½‘æ ¼åŒ–(å³å¯¹è¿ç»­ç©ºé—´ç¦»æ•£åŒ–), æ­¤æ—¶å¯èƒ½æœ‰è‹¥å¹²ç§ç½‘æ ¼åŒ–æ–¹æ¡ˆ. æ±‚å‡ºæ¯ç§ç½‘æ ¼åŒ–æ–¹æ¡ˆå¯¹åº”çš„äº’ä¿¡æ¯å€¼, å¹¶æ‰¾å‡ºå…¶ä¸­çš„æœ€å¤§å€¼ $ \\max_{G \\in \\mathcal G (c, r)} I(x_G ; y_G)$\n   å‡è®¾$c=2$, $r=2$. åˆ™å¯èƒ½æœ‰ä»¥ä¸‹çº¢ã€è“ä¸¤ç§ç½‘æ ¼åŒ–æ–¹æ¡ˆï¼ˆå®é™…ä¸Šä¼šæ›´å¤šï¼‰ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªç½‘æ ¼åŒ–æ–¹æ¡ˆå¯¹åº”çš„äº’ä¿¡æ¯å€¼ï¼Œæ‰¾å‡ºä½¿äº’ä¿¡æ¯å€¼æœ€å¤§çš„ç½‘æ ¼åŒ–æ–¹æ¡ˆã€‚\n\n   ![](Points-in-Information-Theory/mic01.png)\n\n2. å¯¹äº’ä¿¡æ¯å€¼è¿›è¡Œå½’ä¸€åŒ–: $\\frac {\\max_{G \\in \\mathcal G(c, r)} I(x_G ; y_G)} {\\log \\min(c, r)}$ .\n\n3. é€‰å–æ‰€æœ‰ç¦»æ•£åŒ–æ–¹æ¡ˆä¸­äº’ä¿¡æ¯çš„æœ€å¤§å€¼ä½œä¸ºMICå€¼: $c$, $r$ æœ‰å¤šç§é€‰å–æ–¹æ¡ˆ, åœ¨æ‰€æœ‰çš„å¤‡é€‰æ–¹æ¡ˆä¸­é€‰å–ä¸€ä¸ªä½¿å½’ä¸€åŒ–çš„äº’ä¿¡æ¯æœ€å¤§çš„å€¼ å³:\n   $$\n   MIC = \\max_{c \\times r \\lt B} \\frac {\\max_{G \\in \\mathcal G (c, r)} I(x_G ; y_G)} {\\log \\min(c, r)}\n   $$\n   â€‹\n\nå…¶ä¸­: \n\n- $c$, $r$ æ˜¯åœ¨ x,y æ–¹å‘ä¸Šçš„åˆ’åˆ†æ ¼å­çš„ä¸ªæ•°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ç½‘æ ¼åˆ†å¸ƒ;\n- $B$ æ˜¯å˜é‡ï¼Œåœ¨åŸä½œè€…çš„è®ºæ–‡å½“ä¸­æåˆ° B çš„å¤§å°è®¾ç½®æ˜¯æ•°æ®é‡çš„ 0.6 æ¬¡æ–¹. å…·ä½“ä¸ºç”šä¹ˆç”¨è¿™ä¸ªå€¼, çŒœæµ‹åº”è¯¥æ˜¯ç»éªŒå§;\n- $\\mathcal G (c, r)$ æ˜¯åœ¨ç»™å®š$c$, $r$ å, æ‰€æœ‰çš„ç½‘æ ¼åŒ–æ–¹æ¡ˆçš„é›†åˆ, $G \\in \\mathcal G(c, r)$;\n- $x_G$, $ y_G$ åˆ†åˆ«è¡¨ç¤ºåœ¨ç½‘æ ¼åŒ–æ–¹æ¡ˆ $G$ ä¸­ å˜é‡ $x$ å’Œ $y$ å¯¹åº”çš„ç¦»æ•£åºåˆ—.\n\nMICæ€»ç»“\n\n**ç›®çš„**: ç”¨äºè¡¡é‡ä¸¤ä¸ªå˜é‡Xå’ŒYçš„çº¿æ€§æˆ–éçº¿æ€§çš„å¼ºåº¦ã€‚\n\nç‰¹ç‚¹: \n\n- **æ™®é€‚æ€§**ã€‚ä¸ä»…å¯ä»¥å‘ç°å˜é‡é—´çš„çº¿æ€§å‡½æ•°å…³ç³»ï¼Œè¿˜èƒ½å‘ç°**éçº¿æ€§**å…³ç³»(å¦‚æŒ‡æ•°çš„ï¼Œå‘¨æœŸçš„)ï¼›\n- **å‡è¡¡æ€§**ã€‚MICåº¦é‡ä¸ä»…å¯ä»¥ç”¨æ¥çºµå‘æ¯”è¾ƒåŒä¸€ç›¸å…³å…³ç³»çš„å¼ºåº¦ï¼Œè¿˜å¯ä»¥ç”¨æ¥æ¨ªå‘æ¯”è¾ƒä¸åŒå…³ç³»çš„å¼ºåº¦ã€‚æ¯”å¦‚: å¯¹äºç›¸åŒå™ªå£°æ°´å¹³çš„ä¸¤ä¸ªå‡½æ•°å…³ç³»(å¦‚ä¸€ä¸ªæŒ‡æ•°çš„ï¼Œå¦ä¸€ä¸ªæ˜¯å‘¨æœŸçš„)ï¼ŒMICåº¦é‡å…·æœ‰è¿‘ä¼¼çš„å€¼ã€‚\n\n**ç¼ºç‚¹:** MICçš„ç»Ÿè®¡èƒ½åŠ›é­åˆ°äº†ä¸€äº›è´¨ç–‘ï¼Œå½“é›¶å‡è®¾ä¸æˆç«‹æ—¶ï¼ŒMICçš„ç»Ÿè®¡å°±ä¼šå—åˆ°å½±å“ã€‚åœ¨æœ‰çš„æ•°æ®é›†ä¸Šä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ï¼Œä½†æœ‰çš„æ•°æ®é›†ä¸Šå°±å­˜åœ¨è¿™ä¸ªé—®é¢˜. \n\né’ˆå¯¹MICçš„ä¸¤ä¸ªç‰¹ç‚¹, åœ¨ã€ŠMachine Learning - A Probabilistic Perspectiveã€‹ä¸€ä¹¦ä¸­ä½¿ç”¨äº†å¦‚ä¸‹å›¾è§£é‡Š\n\n![IMG_265](Points-in-Information-Theory/Machine-Learning-A-Probabilistic-Perspective.jpg)\n\nè¯¥å›¾ä½¿ç”¨çš„æ•°æ®æºæ˜¯å…·æœ‰357ä¸ªå˜é‡(ç‰¹å¾)çš„æ•°æ®é›†, å·¦å›¾ä¸­çš„æ¯ä¸ªç‚¹æ˜¯è¿™äº›ç‰¹å¾çš„ä¸€ä¸ªä¸¤ä¸ªç»„åˆ(å…±63566ä¸ª), æ¨ªåæ ‡è¡¨ç¤ºä¸¤ä¸ªç‰¹å¾ çš„MIC, çºµè½´æ˜¯ç›¸å…³ç³»æ•°. å³è¾¹é€‰å–äº†å…­å¯¹ç‰¹å¾ç»„åˆ, å¹¶åˆ»ç”»å‡ºäº†å…¶å…³ç³»å›¾. \n\n- ä»å³å›¾å¯ä»¥çœ‹å‡ºCè¡¨ç¤ºçš„ä¸¤ä¸ªç‰¹å¾çš„åˆ†å¸ƒæ˜¯æ²¡æœ‰è§„å¾‹çš„, ä¸å…·æœ‰ç›¸å…³æ€§, å…¶ç›¸å…³ç³»æ•°æ¥è¿‘0, MICæ¥è¿‘0. \n- Hæˆ–è€…D è¡¨ç¤ºçš„éšæœºå˜é‡å…·æœ‰è¿‘ä¼¼çš„çº¿æ€§å…³ç³», å…¶ç›¸å…³ç³»æ•°å’ŒMICéƒ½å¾ˆé«˜.\n- EFGè¡¨ç¤ºçš„éšæœºå˜é‡åŸºæœ¬æ²¡æœ‰çº¿æ€§å…³ç³», æ‰€ä»¥ç›¸å…³ç³»æ•°æ¥è¿‘äº0, ä½†æ˜¯ä»å³å›¾å¯ä»¥çœ‹å‡ºå…¶éƒ½å…·æœ‰éçº¿æ€§å…³ç³». æ‰€æœ‰MICå€¼æ¯”è¾ƒå¤§. \n\nå¼€æºå·¥å…·: [minepy - Maximal Information-based Nonparametric Exploration](https://minepy.readthedocs.io/en/latest/) \nå…¶python api åœ°å€:http://minepy.sourceforge.net/docs/1.0.0/python.html\n\n\n\n# ä¿¡æ¯å¢ç›Š\n\nä¿¡æ¯å¢ç›Šæ˜¯å†³ç­–æ ‘ID3ç®—æ³•åœ¨è¿›è¡Œç‰¹å¾åˆ‡å‰²æ—¶ä½¿ç”¨çš„åˆ’åˆ†å‡†åˆ™ï¼Œå…¶ç‰©ç†æ„ä¹‰å’Œäº’ä¿¡æ¯å®Œå…¨ç›¸åŒï¼Œå¹¶ä¸”å…¬å¼ä¹Ÿæ˜¯å®Œå…¨ç›¸åŒã€‚å…¶å…¬å¼å¦‚ä¸‹ï¼š\n\n$$\ng(D, A) = H(D) - H(D \\mid A)\n$$\n\nå…¶ä¸­Dè¡¨ç¤ºæ•°æ®é›†ï¼ŒAè¡¨ç¤ºç‰¹å¾ï¼Œä¿¡æ¯å¢ç›Šè¡¨ç¤ºå¾—åˆ°Açš„ä¿¡æ¯è€Œä½¿å¾—ç±»Xçš„ä¸ç¡®å®šåº¦ä¸‹é™çš„ç¨‹åº¦ï¼Œåœ¨ID3ä¸­ï¼Œéœ€è¦é€‰æ‹©ä¸€ä¸ªAä½¿å¾—ä¿¡æ¯å¢ç›Šæœ€å¤§ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—åˆ†ç±»ç³»ç»Ÿè¿›è¡Œå¿«é€Ÿå†³ç­–ã€‚ \n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼šåœ¨æ•°å€¼ä¸Šï¼Œä¿¡æ¯å¢ç›Šå’Œäº’ä¿¡æ¯å®Œå…¨ç›¸åŒï¼Œä½†æ„ä¹‰ä¸ä¸€æ ·ï¼Œéœ€è¦åŒºåˆ†ï¼Œå½“æˆ‘ä»¬è¯´äº’ä¿¡æ¯æ—¶å€™ï¼Œä¸¤ä¸ªéšæœºå˜é‡çš„åœ°ä½æ˜¯ç›¸åŒçš„ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯çº¯æ•°å­¦å·¥å…·ï¼Œä¸è€ƒè™‘ç‰©ç†æ„ä¹‰ï¼Œå½“æˆ‘ä»¬è¯´ä¿¡æ¯å¢ç›Šæ—¶å€™ï¼Œæ˜¯æŠŠä¸€ä¸ªå˜é‡çœ‹æˆæ˜¯å‡å°‘å¦ä¸€ä¸ªå˜é‡ä¸ç¡®å®šåº¦çš„æ‰‹æ®µã€‚\n\n# ä¿¡æ¯å¢ç›Šç‡\n\nä¿¡æ¯å¢ç›Šç‡æ˜¯å†³ç­–æ ‘C4.5ç®—æ³•å¼•å…¥çš„åˆ’åˆ†ç‰¹å¾å‡†åˆ™ï¼Œå…¶ä¸»è¦æ˜¯å…‹æœä¿¡æ¯å¢ç›Šå­˜åœ¨çš„åœ¨æŸç§ç‰¹å¾å–æ— æ„ä¹‰å€¼çš„æ—¶å€™å¯¼è‡´çš„å†³ç­–æ ‘åˆ’åˆ†ç‰¹å¾å¤±è¯¯çš„é—®é¢˜ã€‚ä¾‹å¦‚å‡è®¾æœ‰ä¸€åˆ—ç‰¹å¾æ˜¯èº«ä»½è¯IDï¼Œæ¯ä¸ªäººçš„éƒ½ä¸ä¸€æ ·ï¼Œå…¶ä¿¡æ¯å¢ç›Šè‚¯å®šæ˜¯æœ€å¤§çš„ï¼Œä½†æ˜¯å¯¹äºä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»ç³»ç»Ÿæ¥è¯´ï¼Œè¿™ä¸ªç‰¹å¾æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œæ­¤æ—¶å¦‚æœé‡‡ç”¨ID3ç®—æ³•å°±ä¼šå‡ºç°å¤±è¯¯ï¼Œè€ŒC4.5æ­£å¥½å…‹æœäº†è¯¥é—®é¢˜ã€‚å…¶å…¬å¼å¦‚ä¸‹ï¼š\n$$\ng_r(D, A) = \\frac {g(D, A)} {H(A)}\n$$\n\n\n# åŸºå°¼ç³»æ•°\n\nåŸºå°¼ç³»æ•°æ˜¯å†³ç­–æ ‘CARTç®—æ³•å¼•å…¥çš„åˆ’åˆ†ç‰¹å¾å‡†åˆ™ï¼Œå…¶æå‡ºçš„ç›®çš„ä¸æ˜¯ä¸ºäº†å…‹æœä¸Šé¢ç®—æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œè€Œä¸»è¦è€ƒè™‘çš„æ˜¯è®¡ç®—å¿«é€Ÿæ€§ã€é«˜æ•ˆæ€§ï¼Œè¿™ç§æ€§è´¨ä½¿å¾—CARTäºŒå‰æ ‘çš„ç”Ÿæˆéå¸¸é«˜æ•ˆã€‚å…¶å…¬å¼å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\nGini(p) & = \\sum_{i=1}^m p_i (1-p_i) \\\\\n        & = 1 -   \\sum_{i=1}^m p_i^2 \\\\\n        & = 1 -  \\sum_{i=1}^m (\\frac {| C_k |}{|D|})^2\n\\end {align}\n$$\nå¯ä»¥çœ‹å‡ºï¼ŒåŸºå°¼ç³»æ•°è¶Šå°ï¼Œè¡¨ç¤ºé€‰æ‹©è¯¥ç‰¹å¾åç†µä¸‹é™æœ€å¿«ï¼Œå¯¹åˆ†ç±»æ¨¡å‹æ•ˆæœæ›´å¥½ï¼Œå…¶å’Œä¿¡æ¯å¢ç›Šå’Œä¿¡æ¯å¢ç›Šç‡çš„é€‰æ‹©æŒ‡æ ‡æ˜¯ç›¸åçš„ã€‚åŸºå°¼ç³»æ•°ä¸»è¦æ˜¯åº¦é‡æ•°æ®åˆ’åˆ†å¯¹è®­ç»ƒæ•°æ®é›†Dçš„ä¸çº¯åº¦å¤§å°ï¼ŒåŸºå°¼ç³»æ•°è¶Šå°ï¼Œè¡¨æ˜æ ·æœ¬çš„çº¯åº¦è¶Šé«˜ã€‚ \n\nè¿™é‡Œè¿˜å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªå…¬å¼æ˜¾å¾—éå¸¸çªå…€ï¼Œæ„Ÿè§‰çªç„¶å°±å‡ºæ¥äº†ï¼Œæ²¡æœ‰é‚£ç§ä»å‰äººç®—æ³•ä¸­æ”¹è¿›è€Œæ¥çš„æ„Ÿè§‰ï¼Ÿå…¶å®ä¸ºå•¥è¯´åŸºå°¼ç³»æ•°è®¡ç®—é€Ÿåº¦å¿«å‘¢ï¼Œå› ä¸ºåŸºå°¼ç³»æ•°å®é™…ä¸Šæ˜¯ä¿¡æ¯ç†µçš„ä¸€é˜¶è¿›ä¼¼ï¼Œä½œç”¨ç­‰ä»·äºä¿¡æ¯ç†µï¼Œåªä¸è¿‡æ˜¯ç®€åŒ–ç‰ˆæœ¬ã€‚æ ¹æ®æ³°å‹’çº§æ•°å…¬å¼ï¼Œå°†$f(x) = - \\ln (x)$ åœ¨ $x=1$ å¤„å±•å¼€ï¼Œå¿½ç•¥é«˜é˜¶æ— ç©·å°ï¼Œå…¶å¯ä»¥ç­‰ä»·ä¸º $f(x) = 1 - x$ , æ‰€ä»¥å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ä¸Šè¿°å®šä¹‰ã€‚\n\nå‚è€ƒèµ„æ–™ï¼š\n\n[ä¸çŸ¥ä¸ºä¸çŸ¥â€“ä¿¡æ¯è®ºå’Œæœ€å¤§ç†µåŸåˆ™](https://blog.csdn.net/dog250/article/details/78944526)\n\n[æœºå™¨å­¦ä¹ å„ç§ç†µï¼šä»å…¥é—¨åˆ°å…¨é¢æŒæ¡](https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##)\n\n[ä¿¡æ¯è®º(Information theory)çš„ä¸€äº›point](http://blog.csdn.net/dark_scope/article/details/8459576) \n\nMachine Learning - A Probabilistic Perspective\n","tags":["Information Theory"],"categories":["Information Theory"]},{"title":"Hidden Markov Models","url":"%2Fblog%2FHMM.html","content":"\nåœ¨ä¹‹å‰å­¦ä¹ è¿‡å¤šæ¬¡éšé©¬æ¨¡å‹ï¼Œä½†æ˜¯é•¿æ—¶é—´ä¸ç”¨å†åŠ ä¸Šä¹Ÿæ²¡å›å¤´å¤ä¹ ï¼Œå›å¤´çœ‹çš„æ—¶å€™æ„Ÿè§‰ä»€ä¹ˆéƒ½å¿˜äº†ï¼Œæ‰€ä»¥æœ¬æ¬¡é‡æ–°æ‹¾èµ·æ¥ã€‚è®°å¾—ä¹‹å‰çœ‹è¿‡ä¸€ç¯‡[è‹±æ–‡æ–‡ç« ](http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html)è®²çš„ä¸é”™ï¼ˆç°åœ¨æ— æ³•è®¿é—®äº†ï¼‰ï¼Œéšåšäº†ä¸€ä¸ªå¤‡ä»½ã€‚ç°åœ¨ä»¥è¿™ç¯‡æ–‡ç« ä¸ºåŸºç¡€é‡æ–°å­¦ä¹ ä¸€ä¸‹ã€‚ç›®å‰ç½‘ç»œä¸Šè¯¥[æ–‡ç« ](http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html)çš„ç¿»è¯‘ç‰ˆæœ¬æœ‰å¾ˆå¤šï¼Œæˆ‘ä¹Ÿä¼šå‚è€ƒä¸€äº›ã€‚\n\n# 1 æ¦‚è¿°\n\nä¸ªäººè§‰å¾—å­¦ä¹ ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œé¦–å…ˆäº†è§£è¿™ä¸ªæ¨¡å‹èƒ½å¹²ä»€ä¹ˆï¼Œç„¶ååœ¨æ·±å…¥äº†è§£æ¨¡å‹ç»†èŠ‚æ‰èƒ½äº‹åŠåŠŸå€ã€‚éšé©¬æ¨¡å‹æ˜¯ç”¨æ¥å¹²ä»€ä¹ˆçš„ï¼Ÿä¸¾ä¸€ä¸ªé¢„æµ‹å¤©æ°”çš„ä¾‹å­ï¼š\n\nåœ¨å‡ ç™¾å¹´å‰ï¼Œè‹±å›½äººä¸ºäº†é¢„æµ‹ä»Šå¤©çš„å¤©æ°”(sun, cloud, rain)ï¼Œå¯ä»¥é€šè¿‡è§‚å¯Ÿæµ·è—»çš„æ½®æ¹¿ç¨‹åº¦ï¼ˆdry, dryish, damp, soggyï¼‰ï¼šå¦‚æœæµ·è—»æ˜¯å¹²ç‡¥çš„(dry)ï¼Œåˆ™ä»Šå¤©å°†æ˜¯ä¸€æ™´å¤©(sun)ï¼›å¦‚æœæµ·è—»æ˜¯æ½®æ¹¿çš„(soggy)ï¼Œåˆ™ä»Šå¤©å°†æ˜¯ä¸€é›¨å¤©(rain)ã€‚ä¹Ÿå°±æ˜¯è¯´æµ·è—»çš„æ½®æ¹¿ç¨‹åº¦ä¸å¤©æ°”æ˜¯ç›¸å…³çš„ã€‚\n\nå‡è®¾å°æ˜åˆšä»ä¸­å›½æ¥åˆ°è‹±å›½ï¼Œæƒ³çŸ¥é“ä¸Šå‘¨ä»å‘¨ä¸€åˆ°å‘¨æ—¥çš„å¤©æ°”ï¼Œç”±äºä¸€äº›åŸå› ä»–åªèƒ½æ‹¿åˆ°ä¸Šå‘¨æµ·è—»çš„æ½®æ¹¿ç¨‹åº¦çš„è®°å½•ã€‚é‚£ä¹ˆä»–å¯ä»¥é€šè¿‡ä¸Šé¢çš„è§„å¾‹æ¨æµ‹å‡ºä¸Šå‘¨çš„å¤©æ°”ã€‚(ä¾‹å­ä¸æ°å½“ï¼Œå°†å°±ç€ç”¨å§)ã€‚\n\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå°æ˜è§‚å¯Ÿåˆ°çš„æ˜¯æµ·è‰çš„çŠ¶æ€ï¼Œä½†æ˜¯è¦é¢„æµ‹çš„å¤©æ°”æ˜¯éšè—çš„ã€‚è€Œéšé©¬æ¨¡å‹å°±æ˜¯ä½¿ç”¨æ˜¾æ€§çš„å¯è§‚å¯Ÿåˆ°çš„çŠ¶æ€åºåˆ—ï¼ˆä¸€å‘¨æµ·è—»æ½®æ¹¿ç¨‹åº¦çš„è®°å½•ï¼‰ï¼Œå¯¹éšè—çš„æœªçŸ¥çš„çŠ¶æ€åºåˆ—ï¼ˆä¸€å‘¨çš„å¤©æ°”ï¼‰è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹ã€‚\n\n# 2 é©¬å°”ç§‘è¿‡ç¨‹(Markov Process)\n\näº¤é€šä¿¡å·ç¯çš„å˜åŒ–æ˜¯æœ‰è§„å¾‹å¯å¯»çš„ï¼šçº¢ç¯-> ç»¿ç¯->é»„ç¯ å¾ªç¯è½¬æ¢ã€‚å¦‚æœå½“å‰æ˜¯çº¢ç¯ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªä¸€å®šå°±æ˜¯ç»¿ç¯ã€‚å½“å‰çŠ¶æ€åªä¾èµ–äºå‰ä¸€ä¸ªçŠ¶æ€ï¼ŒçŠ¶æ€ä¹‹é—´çš„è½¬åŒ–æ˜¯ç¡®å®šçš„ã€‚è¿™ç§ç¡®å®šæ€§çš„çŠ¶æ€è½¬åŒ–è§„å¾‹æ˜¯éå¸¸å®¹æ˜“å»ºæ¨¡æ¨¡æ‹Ÿçš„ã€‚\n\nå¤©æ°”çš„çŠ¶æ€åœ¨ä¸‰ç§å¤©æ°”(sun, cloud, rain)ä¹‹é—´è½¬æ¢ï¼Œä½†æ˜¯æ™´å¤©(sun)ä¹‹åå¯èƒ½æ˜¯å¤šäº‘(cloud)ä¹Ÿå¯èƒ½æ˜¯é›¨å¤©(rain)ã€‚çŠ¶æ€è½¬åŒ–æ˜¯ä¸ç¡®å®šçš„ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¿‡å»100å¤©çš„å¤©æ°”è§‚å¯Ÿåºåˆ—è®°å½•äº†è¿‡å»100å¤©çš„å¤©æ°”ï¼Œé‚£ä¹ˆæ˜å¤©çš„å¤©æ°”æ˜¯ä»€ä¹ˆå‘€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé¦–å…ˆéœ€è¦ä½¿ç”¨100å¤©çš„è§‚å¯Ÿåºåˆ—å»ºç«‹æ¨¡å‹ï¼Œç„¶åæ ¹æ®æ¨¡å‹å¯¹æ˜å¤©çš„å¤©æ°”è¿›è¡Œé¢„æµ‹ã€‚\n\nå¯¹éç¡®å®šæ€§çš„çŠ¶æ€è½¬åŒ–åºåˆ—å»ºæ¨¡å°±æ¶‰åŠåˆ°äº†éšæœºè¿‡ç¨‹çš„å»ºæ¨¡äº†ã€‚å¯¹ä¸€ä¸ªé•¿åº¦ä¸º$N$çš„éšæœºè¿‡ç¨‹å»ºæ¨¡å¦‚ä¸‹ï¼š å³è¿™ä¸ªåºåˆ—ä¸­æ¯ä¸€ä¸ªæ—¶é—´ç‚¹çš„çŠ¶æ€çš„è”åˆæ¦‚ç‡\n$$\nP(X_N,X_{N-1} \\dots X_1)\n$$\nä½¿ç”¨product ruleè¿›è¡Œæ”¹å†™ï¼š\n$$\nP(X_N,X_{N-1} \\dots X_1)=\\prod_{n=1}^NP(X_n|X_{n-1} \\dots X_1)\n$$\nå¯ä»¥çœ‹åˆ°å…¬å¼å³è¾¹æ˜¯ç›¸å½“å¤æ‚çš„ï¼Œ$P(X_n|X_{n-1} \\dots X_1)$è¯´æ˜ç³»åˆ—ä¸­æ—¶é—´ç‚¹$n$æ—¶åˆ»çš„çŠ¶æ€æ˜¯ç”±è¯¥æ—¶åˆ»ä¹‹å‰çš„æ‰€æœ‰çŠ¶æ€å†³å®šçš„ã€‚å¦‚æœ$N=1000$ ï¼Œé‚£ä¹ˆä¸ºäº†æ±‚å‡ºåºåˆ—ä¸­æœ€åä¸€ä¸ªæ—¶åˆ»æ‰€å¤„çŠ¶æ€çš„æ¦‚ç‡å°±éœ€è¦è€ƒè™‘å…¶ä¹‹å‰çš„999ä¸ªæ—¶åˆ»ã€‚\n\næ‰€ä»¥ä¸ºäº†å¯¹éç¡®å®šæ€§çš„çŠ¶æ€è½¬åŒ–åºåˆ—å»ºæ¨¡æ¨¡æ‹Ÿï¼Œéœ€è¦è¿›è¡Œä¸€äº›å‡è®¾ä½¿é—®é¢˜ç®€åŒ–ã€‚æ­¤å¤„éœ€è¦å¼•å…¥é©¬å°”ç§‘å¤«æ€§å’Œé©¬å°”ç§‘å¤«è¿‡ç¨‹ã€‚\n\n> é©¬å°”ç§‘å¤«æ€§: åœ¨ä¸€ä¸ªéšæœºè¿‡ç¨‹$\\{X_t, t\\in T\\}$ä¸­ï¼Œå¦‚æœéšæœºè¿‡ç¨‹åœ¨$t_0$æ—¶åˆ»æ‰€å¤„çš„çŠ¶æ€å·²çŸ¥æ—¶ï¼Œå®ƒåœ¨$t=0$æ—¶åˆ»ä»¥åçš„çŠ¶æ€ä¸å…¶åœ¨$t=0$æ—¶åˆ»ä¹‹å‰çš„çŠ¶æ€æ— å…³ã€‚è¿™ç§æ€§è´¨ç§°ä¸ºé©¬å°”ç§‘å¤«æ€§ã€‚\tå³ï¼šè¿‡ç¨‹ä¸­â€œå°†æ¥â€çš„æƒ…å†µä¸â€œè¿‡å»â€çš„æƒ…å†µæ˜¯æ— å…³çš„ã€‚\n>\n> é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼šå…·æœ‰é©¬å°”å¯å¤«æ€§çš„éšæœºè¿‡ç¨‹ç§°ä¸ºé©¬å°”å¯å¤«è¿‡ç¨‹ã€‚åœ¨ä¸€ä¸ªéšæœºè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªçŠ¶æ€ä»…å–å†³äºå…¶ä¹‹å‰çš„kä¸ªçŠ¶æ€ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºké˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ã€‚$k=1$æ—¶ä¸ºä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ã€‚\n>\n> ä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼šè®¾éšæœºè¿‡ç¨‹$\\{X_t, t\\in T\\}$çš„çŠ¶æ€ç©ºé—´ä¸ºSï¼Œå¦‚æœ\n> $$\n> P (X_n=x_{n}\\mid X_{n-1}=x_{n-1},\\dots ,X_0=x_{0})=P(X_{t_n}=x_{n}\\mid X_{t_{n-1}}=x_{n-1})\n> $$\n> å…¶ä¸­$n>2$ï¼Œ$x_i \\in S$ï¼Œåˆ™æˆè¯¥éšæœºè§„ç§°ä¸ºä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ã€‚å³å½“å‰çŠ¶æ€åªå–å†³äºå‰é¢çš„ä¸€ä¸ªçŠ¶æ€ã€‚\n\n![](HMM/first_order_markov_process.png)\n\nåœ¨ä¸Šé¢çš„å¤©æ°”å˜åŒ–é—®é¢˜ä¸­ï¼Œå°†ä¸‰ç§å¤©æ°”(sun, cloud, rain)çŠ¶æ€ç»„æˆçŠ¶æ€ç©ºé—´$S$ï¼Œå°†ä¸€å‘¨çš„å¤©æ°”å˜åŒ–çš„è¿‡ç¨‹æŠ½è±¡ä¸ºä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼ˆä»Šå¤©çš„å¤©æ°”åªä¸æ˜¨å¤©æœ‰å…³ï¼Œä¸å‰å¤©æ— å…³ï¼‰ï¼Œä»è€Œç®€åŒ–é—®é¢˜å¤„ç†ã€‚å¯¹è¿™æ ·ä¸€ä¸ªä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹å»ºæ¨¡å¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\nP(X_N,X_{N-1} \\dots X_1)&=\\prod_{n=1}^NP(X_n|X_{n-1} \\dots X_1) \\\\\n                                           &=P(x_1)\\prod_{n=2}^NP(X_n|X_{n-1})\n\\end {aligned}\n$$\nä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ä¸­ï¼Œå½“å‰çŠ¶æ€åªå–å†³äºå‰é¢çš„ä¸€ä¸ªçŠ¶æ€ï¼Œä¹Ÿå°±æ˜¯è¯´è¯¥è¿‡ç¨‹åªè€ƒè™‘çŠ¶æ€ä¹‹é—´çš„ä¸¤ä¸¤å…³ç³»ã€‚å¦‚æœçŠ¶æ€ç©ºé—´ä¸­æœ‰Mä¸ªçŠ¶æ€ï¼Œé‚£ä¹ˆéœ€è¦è€ƒè™‘çš„å…³ç³»å°±æœ‰$M \\times M$ ç§ã€‚ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µ(state transition matrix) Aè¡¨ç¤ºçŠ¶æ€ä¹‹é—´çš„ä¸¤ä¸¤å…³ç³»ï¼ŒçŸ©é˜µä¸­çš„å…ƒç´ $a_{ij}=P(S_j \\mid S_i)$ï¼Œå…¶ä¸­$S_i$è¡¨ç¤ºçŠ¶æ€ç©ºé—´$S$ä¸­çš„ç¬¬$i$ä¸ªçŠ¶æ€ï¼Œ$S_j$è¡¨ç¤ºçŠ¶æ€ç©ºé—´$S$ä¸­çš„ç¬¬$j$ä¸ªçŠ¶æ€ï¼Œ$0 \\le i \\le |S|, 0 \\le j \\le |S|$ã€‚åœ¨å¤©æ°”å˜åŒ–è¿™ä¹ˆä¸€ä¸ªé©¬å°”ç§‘å¤«è¿‡ç¨‹ä¸­ï¼ŒçŠ¶æ€è½¬ç§»æ¦‚ç‡æ˜¯è¿™æ ·ä¸€ä¸ªçŸ©é˜µï¼š\n\n$$\n\\begin{bmatrix}\n            & sun   &cloud  & rain    \\\\\n sun     &0.50   & 0.375 & 0.125 \\\\\n cloud  & 0.25  & 0.125 & 0.625 \\\\\n rain     & 0.25  & 0.375 & 0.375\n  \\end{bmatrix}\n$$\n\nç¬¬äºŒè¡Œçš„ä¸‰ä¸ªæ•°è¡¨ç¤ºï¼Œæ˜¨å¤©æ˜¯å¤šäº‘(cloud)çš„æƒ…å†µä¸‹ï¼Œä»Šå¤©æ˜¯æ™´å¤©(sun)ã€å¤šäº‘(cloud)ã€ä¸‹é›¨(rain)çš„æ¦‚ç‡åˆ†åˆ«ä¸º 0.25 ã€ 0.125 ã€ 0.625ã€‚\n\nåœ¨ä¸€ä¸ªåºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªçŠ¶æ€æ˜¯æ²¡æœ‰å‰ä¸€ä¸ªçŠ¶æ€çš„ï¼Œå¹¶ä¸”çŠ¶æ€ç©ºé—´ä¸­çš„æ‰€æœ‰çŠ¶æ€éƒ½æœ‰å¯èƒ½æ˜¯ç¬¬ä¸€ä¸ªçŠ¶æ€ï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªå‘é‡æ¥æè¿°çŠ¶æ€ç©ºé—´$S$ä¸­çš„å€¼æ˜¯ç¬¬ä¸€ä¸ªçš„æ¦‚ç‡ï¼Œå³åˆå§‹çŠ¶æ€å‘é‡($\\pi$)ã€‚åœ¨å¤©æ°”å˜åŒ–è¿™ä¹ˆä¸€ä¸ªé©¬å°”ç§‘å¤«è¿‡ç¨‹ä¸­ï¼Œç¬¬ä¸€å¤©æ˜¯æ™´å¤©(sun)ã€å¤šäº‘(cloud)ã€ä¸‹é›¨(rain)çš„æ¦‚ç‡åˆ†åˆ«ä¸º 0.35 ã€ 0.05 ã€ 0.6ã€‚å³:\n$$\n\\pi = (0.35 ã€ 0.05 ã€ 0.6)^T\n$$\næŒ‰ç…§ç”Ÿæˆæ¨¡å‹çš„è§‚ç‚¹ç†è§£ä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼š\n\n- æŒ‰ç…§åˆå§‹çŠ¶æ€å‘é‡($\\pi$)ä»çŠ¶æ€ç©ºé—´$S$ä¸­é€‰æ‹©ä¸€ä¸ªçŠ¶æ€$S_i$ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç¬¬ä¸€ä¸ªçŠ¶æ€$x_1$;\n- æŒ‰ç…§è½¬ç§»çŠ¶æ€çŸ©é˜µ$A$ä»çŠ¶æ€ç©ºé—´$S$ä¸­é€‰æ‹©ä¸€ä¸ªçŠ¶æ€$S_j$ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç¬¬äºŒä¸ªçŠ¶æ€$x_2$, ç”Ÿæˆçš„æ¦‚ç‡ä¸º$a_{ij}$ ;\n- æŒ‰ç…§è¯¥è¿‡ç¨‹ä¸€æ¬¡ç”Ÿæˆ $x_3 ,x_4 \\dots x_n$ ã€‚\n\nä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ä¸­æœ‰ä¸‰ä¸ªåŸºæœ¬æ¦‚å¿µï¼š\n\n1 **çŠ¶æ€ç©ºé—´**ï¼šåœ¨éšæœºè¿‡ç¨‹ä¸­å¯é€‰çš„çŠ¶æ€çš„é›†åˆã€‚å¦‚ä¸‰ç§å¤©æ°”(sun, cloud, rain)çŠ¶æ€ã€‚\n2 **åˆå§‹çŠ¶æ€å‘é‡($\\pi$)**ï¼š æè¿°çŠ¶æ€ç©ºé—´$S$ä¸­çš„å€¼æ˜¯ç¬¬ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚\n3 **çŠ¶æ€è½¬ç§»çŸ©é˜µ(A)**ï¼šæè¿°æ¯ä¸ªçŠ¶æ€ä¹‹é—´ç›¸äº’è½¬æ¢çš„æ¦‚ç‡ã€‚\n\n\n# 3 éšé©¬å°”å¯å¤«æ¨¡å‹(Hidden Markov Models)\n\né©¬å°”ç§‘å¤«è¿‡ç¨‹æ˜¯å¯¹ä¸€ä¸ªè§‚å¯Ÿåˆ°çš„åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œåœ¨æ–‡ç« å¼€å§‹çš„ä¾‹å­ä¸­ï¼Œâ€œå°æ˜åˆšä»ä¸­å›½æ¥åˆ°è‹±å›½ï¼Œæƒ³çŸ¥é“ä¸Šå‘¨ä»å‘¨ä¸€åˆ°å‘¨æ—¥çš„å¤©æ°”ï¼Œç”±äºä¸€äº›åŸå› ä»–åªèƒ½æ‹¿åˆ°ä¸Šå‘¨æµ·è—»çš„æ½®æ¹¿ç¨‹åº¦çš„è®°å½•ã€‚â€å°æ˜çŸ¥é“çš„æ˜¯æµ·è—»æ¹¿åº¦ä¸€å‘¨çš„çŠ¶æ€å˜åŒ–åºåˆ—ï¼Œä»–ä¸çŸ¥é“ä¸Šå‘¨ä»å‘¨ä¸€åˆ°å‘¨æ—¥çš„å¤©æ°”çŠ¶æ€ï¼Œæµ·è—»çš„çŠ¶æ€å’Œå¤©æ°”çš„çŠ¶æ€æ˜¯å­˜åœ¨ç´§å¯†å…³ç³»çš„ã€‚è¿™é‡Œä¾‹å­ä¸­å­˜åœ¨ä¸¤ä¸ªçŠ¶æ€åºåˆ—ï¼šè§‚å¯Ÿåˆ°çš„çŠ¶æ€åºåˆ—(æµ·è—»çš„æ½®æ¹¿çŠ¶æ€)ï¼Œéšè—çš„çŠ¶æ€åºåˆ—(ä¸€å‘¨çš„å¤©æ°”çŠ¶æ€)ã€‚\n\nè¿™ç§å­˜åœ¨ä¸€ä¸ªéšè—çŠ¶æ€åºåˆ—ä¸€ä¸ªè§‚å¯ŸçŠ¶æ€åºåˆ—ä¸”éšè—çŠ¶æ€åºåˆ—å’Œè§‚å¯ŸçŠ¶æ€åºåˆ—å­˜åœ¨å…³ç³»çš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹è¿›è¡Œå»ºæ¨¡ã€‚åœ¨éšé©¬æ¨¡å‹ä¸­ï¼Œå‡å®šéšè—çŠ¶æ€åºåˆ—æ˜¯ä¸€ä¸ªä¸€é˜¶é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼Œè§‚å¯ŸçŠ¶æ€åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½å’Œéšè—çŠ¶æ€åºåˆ—ä¸­çš„èŠ‚ç‚¹ä¸€ä¸€å¯¹åº”ï¼Œè¿™ä¸ªå¯¹åº”å…³ç³»å°±ååº”äº†è§‚å¯ŸçŠ¶æ€å’Œéšè—çŠ¶æ€çš„å¯¹åº”å…³ç³»ã€‚å¦‚ä¸‹é›¨å¤©(éšè—çŠ¶æ€)ä¼šä½¿æµ·è—»æ½®æ¹¿(è§‚å¯ŸçŠ¶æ€)ã€‚éšé©¬æ¨¡å‹ä½¿ç”¨çŠ¶æ€å‘å°„çŸ©é˜µï¼ˆemission probability matrixï¼‰$\\phi$æ¥è¡¨ç¤ºéšè—çŠ¶æ€å’Œè§‚å¯ŸçŠ¶æ€çš„å¯¹åº”å…³ç³»\n\n![](HMM/hidden_markov_model.png)\n\nå¯è§‚å¯ŸçŠ¶æ€çš„é›†åˆä¸º$O$ï¼Œå’Œéšè—çŠ¶æ€çš„çš„é›†åˆä¸º$S$ã€‚$O_i$è¡¨ç¤ºå¯è§‚å¯ŸçŠ¶æ€é›†åˆä¸­çš„ç¬¬$i$ä¸ªçŠ¶æ€ï¼Œ$S_j$è¡¨ç¤ºéšè—çŠ¶æ€é›†åˆä¸­çš„ç¬¬$j$ä¸ªçŠ¶æ€ã€‚çŠ¶æ€å‘å°„çŸ©é˜µ$\\phi$ä¸­çš„å…ƒç´ :\n\n$$\n\\phi_{ji}=P(O_i|S_j)\n$$\n\næ–‡ç« å¼€å¤´çš„å¤©æ°”é—®é¢˜çš„çŠ¶æ€å‘ç€çŸ©é˜µå¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š\n$$\n\\begin{bmatrix}\n           & dry   &dryish& damp & soggy \\\\\n sun    &0.60  & 0.20  & 0.15    &0.05     \\\\\n cloud & 0.25 & 0.25  & 0.25    &0.25     \\\\\n rain    & 0.05 & 0.10  & 0.35    &0.50\n  \\end{bmatrix}\n$$\nç¬¬ä¸€è¡Œè¡¨ç¤ºï¼Œå¦‚æœä»Šå¤©æ˜¯æ™´å¤©(sun)ï¼Œé‚£ä¹ˆä»Šå¤©çš„æµ·è—»å¤„äºå››ä¸­çŠ¶æ€(dryã€dryishã€dampã€soggy)çš„æ¦‚ç‡åˆ†åˆ«ä¸º0.60ã€ 0.20ã€ 0.15ã€0.05ã€‚\n\nnote: çŠ¶æ€å‘ç€çŸ©é˜µä¸­æ¯ä¸€è¡Œçš„å’Œå¿…é¡»æ˜¯1ã€‚\n\nç›¸å¯¹äºé©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼Œéšé©¬æ¨¡å‹å¤šäº†ä¸€ä¸ªè§‚å¯ŸçŠ¶æ€åºåˆ—ï¼Œå’Œä¸€ä¸ªå‘å°„çŠ¶æ€çŸ©é˜µã€‚éšé©¬æ¨¡å‹åŒ…å«å¦‚ä¸‹äº”ä¸ªæ¦‚å¿µï¼š\n\n1. **éšè—çŠ¶æ€ç©ºé—´**ï¼šå¦‚ä¸‰ç§å¤©æ°”(sun, cloud, rain)çŠ¶æ€ã€‚\n2. **è§‚å¯ŸçŠ¶æ€ç©ºé—´**ï¼šå¦‚æµ·è—»çš„å››ä¸­çŠ¶æ€(dryã€dryishã€dampã€soggy)\n3. **åˆå§‹çŠ¶æ€å‘é‡($\\piâ€‹$)**ï¼š æè¿°çŠ¶æ€ç©ºé—´$Sâ€‹$ä¸­çš„å€¼æ˜¯ç¬¬ä¸€ä¸ªçŠ¶æ€çš„æ¦‚ç‡ã€‚\n4. **çŠ¶æ€è½¬ç§»çŸ©é˜µ(A)**ï¼šæè¿°æ¯ä¸ªéšè—çŠ¶æ€ä¹‹é—´ç›¸äº’è½¬æ¢çš„æ¦‚ç‡ã€‚\n5. **å‘å°„çŠ¶æ€çŸ©é˜µ($\\phi$)**ï¼šç»™å®šä¸€ä¸ªéšè—çŠ¶æ€æ—¶ï¼Œä¸€ä¸ªè§‚å¯ŸçŠ¶æ€å‡ºç°çš„æ¦‚ç‡ã€‚\n\nä¸€ä¸ªéšé©¬æ¨¡å‹æ˜¯ä¸€ä¸ªæ ‡å‡†çš„é©¬å°”ç§‘å¤«è¿‡ç¨‹åŠ ä¸Šä¸€ä¸ªè§‚å¯ŸçŠ¶æ€é›†åˆå’Œä»éšè—çŠ¶æ€åˆ°è§‚å¯ŸçŠ¶æ€çš„å…³ç³»çŸ©é˜µã€‚\n\néšé©¬æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸ºåˆå§‹çŠ¶æ€å‘é‡($\\pi$)ã€çŠ¶æ€è½¬ç§»çŸ©é˜µ(A)ã€å‘å°„çŠ¶æ€çŸ©é˜µ($\\phi$)çš„ä¸‰å…ƒç»„$\\{\\pi, A, \\phi \\}$ ã€‚\n\n# 4 éšé©¬æ¨¡å‹çš„åº”ç”¨\n\néšé©¬æ¨¡å‹å¯ä»¥ç”¨äºè§£å†³ä¸‰ç±»é—®é¢˜ï¼Œé’ˆå¯¹ä¸åŒçš„é—®é¢˜ä½¿ç”¨ä¸åŒçš„æ±‚è§£ç®—æ³•ï¼š\n\n1. leaningï¼šå½“çŸ¥é“ä¸€ä¸ªè§‚å¯Ÿåºåˆ—æ—¶ï¼Œä½¿ç”¨é€‚å½“çš„ç®—æ³•è®­ç»ƒä¸€ä¸ªHMMï¼Œæ±‚å¾—å‚æ•°$\\{\\pi, A, \\phi \\}$ ã€‚ä½¿ç”¨ forward-backward algorithmæ±‚è§£ã€‚\n2. evaluation: å·²çŸ¥ä¸€ä¸ªçš„HMM($\\{\\pi, A, B\\}$ å·²çŸ¥)ï¼Œå’Œä¸€ä¸ªè§‚å¯Ÿåºåˆ—æ—¶ï¼Œæ±‚è§‚å¯Ÿåºåˆ—å‡ºç°çš„æ¦‚ç‡ã€‚ä½¿ç”¨forward algorithm æ±‚è§£ã€‚\n3. decoding: å·²çŸ¥ä¸€ä¸ªè§‚å¯Ÿåºåˆ—å’Œå…¶å¯¹åº”çš„HMMæ—¶ï¼Œæ±‚ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„éšè—çŠ¶æ€åºåˆ—ã€‚ä½¿ç”¨Viterbi algorithmæ±‚è§£ã€‚\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[æœºå™¨å­¦ä¹  Hidden Markov Models](http://blog.csdn.net/matrix_space/article/details/45582017)\n\nPattern Recognition and Machine Learning","tags":["Machine Learning"],"categories":["Machine Learning"]},{"title":"æ¿€æ´»å‡½æ•°-ç¥ç»ç½‘ç»œ","url":"%2Fblog%2FActivation_Function.html","content":"\n# 1 æ¿€æ´»å‡½æ•°çš„ä½œç”¨\n\n- æ¿€æ´»å‡½æ•°æ˜¯ç”¨æ¥åŠ å…¥éçº¿æ€§å› ç´ çš„ï¼Œè§£å†³çº¿æ€§æ¨¡å‹æ‰€ä¸èƒ½è§£å†³çš„é—®é¢˜ã€‚\n  ç¥ç»ç½‘ç»œä¹‹æ‰€ä»¥åŒºåˆ«äºçº¿æ€§æ¨¡å‹çš„ä¸€ç‚¹å°±æ˜¯åº”ä¸ºå®ƒæœ‰æ¿€æ´»å‡½æ•°ï¼Œå¦‚ä¸€ä¸ªä¸‰å±‚ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š\n\n  $$\n  \\begin {aligned}\n  y_K(x, w)& = \\sigma (\\sum_{j=1}^M w_{kj}^{(2)} h(\\sum_{i=1}^D {w_{ji}^{(1)} x_i} + w_{j0}^{(1)}) + w_{k0}^{(2)}) \\\\\n  &= \\sigma (W^{(2)} h(W^{(1)}X))\n  \\end {aligned}\n  $$\n  å…¶ä¸­$h$å’Œ$\\sigma$åˆ†åˆ«æ˜¯éšè—å±‚å’Œè¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰è¿™ä¸¤ä¸ªæ¿€æ´»å‡½æ•°ï¼š\n  $$\n  \\begin {aligned}\n  y_K(x, w)&= W^{(2)}W^{(1)}X \\\\\n  &=WX\n  \\end {aligned}\n  $$\n  è¿™ä¸å°±æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹å—ã€‚æ‰€ä»¥ç›¸å¯¹äºçº¿æ€§æ¨¡å‹ï¼Œæ¿€æ´»å‡½æ•°ä½¿ç¥ç»ç½‘ç»œå…·æœ‰çš„æ›´å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚\n\n- åœ¨ä¸€ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼šMost hidden units are distinguished from each other only by the choice of the form of the activation\n  function\n\n# 2 å¸¸è§æ¿€æ´»å‡½æ•°\n\n\n\n## 2.1 Sigmoid\n\n$$\n\\begin {aligned}\n\\sigma(x)& = \\frac1{1+e^x} \\\\\n\\\\\n\\sigma'(x)& = \\sigma(x) (1-\\sigma(x) )\n\\end {aligned}\n$$\n![](Activation_Function/sigmoid_func.png)\n\nsigmoid å‡½æ•°å°†è¾“å…¥çš„å®æ•°å‹ç¼©åˆ°èŒƒå›´[0, 1]ä¹‹é—´ã€‚\n\nä¼˜ç‚¹ï¼š\n\n1. Sigmoidå‡½æ•°å°†è¾“å…¥æ˜ å°„åœ¨(0,1)ä¹‹é—´ï¼Œå•è°ƒè¿ç»­ï¼Œå¯ä»¥ç”¨ä½œè¾“å‡ºå±‚ã€‚\n2. æ±‚å¯¼å®¹æ˜“ã€‚\n\nç¼ºç‚¹ï¼š\n\n1. **å…·æœ‰é¥±å’Œæ€§(saturate)å¯¹å‚æ•°$W$çš„åˆå§‹å€¼æ•æ„Ÿï¼Œ åå‘ä¼ æ’­è¿‡ç¨‹æ˜“æ¢¯åº¦æ¶ˆå¤±**\n   - åå‘ä¼ æ’­è¿‡ç¨‹ï¼šæ¢¯åº¦æ¶ˆå¤±\n     sigmoid å‡½æ•°å°†è¾“å…¥çš„å®æ•°å‹ç¼©åˆ°èŒƒå›´[0, 1]ä¹‹é—´ï¼Œä½†æ˜¯åœ¨å‡½æ•°çš„ä¸¤ç«¯çš„è¾ƒå¤§èŒƒå›´ä¸­ï¼Œsigmoid å‡½æ•°çš„å¯¼æ•°æ¥è¿‘0ã€‚åœ¨backpropagationè¿‡ç¨‹ä¸­éœ€è¦è¿­ä»£æ›´æ–°å‚æ•°$w_{ji}$ï¼š\n\n     $$\n     w_{ji} = w_{ji} - \\alpha \\frac{\\partial}{\\partial w_{ji}}E\n     $$\n\n     å…¶ä¸­ï¼š\n     $$\n     \\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial a_j} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i  = h'(a_j) \\sum_{k=1}^K w_{kj} \\delta_k z_i\n     $$\n\n     å‡½æ•°$h'$æ˜¯æ¿€æ´»å‡½æ•°$h$çš„æ¢¯åº¦ï¼Œå¦‚æœ$h$æ˜¯sigmoidå‡½æ•°ï¼Œåœ¨å‡½æ•°çš„ä¸¤ç«¯çš„è¾ƒå¤§èŒƒå›´ä¸­ï¼Œ$h'$æ¥è¿‘0ï¼Œä»è€Œå¯¼è‡´$\\frac{\\partial E}{\\partial w_{ji}}=0$ (kill gradients)ï¼Œè¿›è€Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­$w_{ji}$å¾—ä¸åˆ°æ›´æ–°ã€‚\n\n   - é¥±å’Œæ€§ä½¿ æ¨¡å‹å¯¹å‚æ•°$W$çš„åˆå§‹å€¼æ•æ„Ÿ\n     sigmoid å‡½æ•°å°†è¾“å…¥çš„å®æ•°å‹ç¼©åˆ°èŒƒå›´[0, 1]ä¹‹é—´ï¼Œä½†æ˜¯åœ¨å‡½æ•°çš„ä¸¤ç«¯çš„è¾ƒå¤§èŒƒå›´ä¸­ï¼Œsigmoidå‡½æ•°çš„å€¼æ˜¯0æˆ–è€…1ã€‚ä¸€ä¸ªç¥ç»å…ƒçš„å‰å‘ä¼ æ’­ä¸ºï¼š\n     $$\n     f(a)=f(\\sum_i {w_i \\times x_i })\n     $$\n     åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¹‹å‰éœ€è¦å¯¹å‚æ•°$W$è¿›è¡Œåˆå§‹åŒ–ï¼Œå¦‚æœå‚æ•°$W$è¢«åˆå§‹åŒ–äº†ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåˆ™$a$è¾ƒå¤§ï¼Œå°†ä½¿è¯¥ç¥ç»å…ƒçš„è¾“å‡ºå€¼é•¿æ—¶é—´æ˜¯0æˆ–è€…1ã€‚\n     åŒæ—¶ï¼Œåœ¨åå‘ä¼ æ’­ä¸­æœ‰ä¸$a$è¾ƒå¤§ï¼Œä½¿å¾—$h'(a)=0$ï¼Œä¹Ÿå°±äº§ç”Ÿäº†æ¢¯åº¦æ¶ˆå¤±ç°è±¡ã€‚\n\n2. *Sigmoid outputs are not zero-centered*. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x>0$ elementwise in $f=wTx+b$), then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.\n\n\n## 2.2 Tanh\n\n$$\n\\begin {aligned}\n    \\tanh(x)&=\\frac{\\sinh(x)}{\\cosh(x)}=\\frac{e^x-e^{-x}}{e^x+e^{-x}} \\\\\n    \\tanh(x) &=2\\sigma(2x)-1 \\\\\n    \\tanh'(x)&= 1-\\tanh^2(x) \n    \\end {aligned}\n$$\n\n![](Activation_Function/tanh_func.png)\n\n  tanhå°†å®æ•°å€¼å‹ç¼©åˆ°[-1, 1]ä¹‹é—´ã€‚å’Œsigmoidä¸€æ ·ï¼Œtanhå…·æœ‰é¥±å’Œæ€§å¯¹å‚æ•°$W$çš„åˆå§‹å€¼æ•æ„Ÿï¼Œ åå‘ä¼ æ’­è¿‡ç¨‹æ˜“æ¢¯åº¦æ¶ˆå¤±çš„ç¼ºç‚¹ã€‚\n\n  ä½†æ˜¯tanhæ˜¯ä»¥0ä½ä¸­å¿ƒï¼Œå› æ­¤ï¼Œå®é™…åº”ç”¨ä¸­ï¼Œtanh ä¼šæ¯” sigmoid æ›´å¥½ã€‚\n\n## 2.3 ReLU\n\n$$\nf(x)=\\max(0, x)\n$$\n![](Activation_Function/ReLU_func.png)\n\nåœ¨è®¾è®¡éšè—å±‚æ—¶ï¼ŒReLUæ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„é€‰æ‹©ã€‚\n\nä¼˜ç‚¹ï¼š\n\n- ReLU å¾—åˆ°çš„SGDçš„æ”¶æ•›é€Ÿåº¦ä¼šæ¯” sigmoid/tanh å¿«å¾ˆå¤š\n- ç›¸æ¯”äº sigmoid/tanhï¼Œè®¡ç®—ç®€å•ã€‚\n\nç¼ºç‚¹ï¼š\n\n- è®­ç»ƒè¿‡ç¨‹ä¸­å®¹æ˜“å­˜åœ¨ç¥ç»å…ƒæ­»äº¡ç°è±¡ï¼šåœ¨æŸä¸€è½®åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¦‚æœä¸€ä¸ªå‚æ•°$w_{ji}$çš„æ¢¯åº¦$\\frac{\\partial E}{\\partial w_{ji}}$éå¸¸å¤§ï¼Œå¯¼è‡´æ›´æ–°å$w_{ji} = w_{ji} - \\alpha \\frac{\\partial}{\\partial w_{ji}}E$ ï¼Œ$w$å…·æœ‰ä¸€ä¸ªéå¸¸å¤§çš„å€¼(æ­£å€¼æˆ–è€…è´Ÿå€¼)ã€‚åˆ™æ¿€æ´»å› å­ $a_j = \\sum_i w_{ji}z_i$ å¯èƒ½ä¼šå‡ºè´Ÿå€¼ã€‚æ­¤ä¸‹ä¸€è½®çš„æ­£å‘ä¼ æ’­è¯¥ç¥ç»å…ƒçš„è¾“å‡ºå€¼æ˜¯0ï¼Œåå‘ä¼ æ’­è¯¥ç¥ç»å…ƒæ‰€æœ‰å‚æ•°$w$çš„æ¢¯åº¦ä¹Ÿæ˜¯0ã€‚ä»è€Œå¯¼è‡´è¯¥ç¥ç»å…ƒâ€œdeadâ€ã€‚\n  å®é™…æ“ä½œä¸­ï¼Œå¦‚æœä½ çš„learning rate å¾ˆå¤§ï¼Œé‚£ä¹ˆå¾ˆæœ‰å¯èƒ½ä½ ç½‘ç»œä¸­çš„40%çš„ç¥ç»å…ƒéƒ½â€deadâ€äº†ã€‚ å½“ç„¶ï¼Œå¦‚æœä½ è®¾ç½®äº†ä¸€ä¸ªåˆé€‚çš„è¾ƒå°çš„learning rateï¼Œè¿™ä¸ªé—®é¢˜å‘ç”Ÿçš„æƒ…å†µå…¶å®ä¹Ÿä¸ä¼šå¤ªé¢‘ç¹ã€‚\n\n## 2.4 Leaky ReLU\n\n  æœ‰ä¸€äº›åŸºäºReLUçš„æ³›åŒ–çš„æ¿€æ´»å‡½æ•°ï¼Œè¿™äº›æ¿€æ´»å‡½æ•°çš„æ•ˆæœå¤§éƒ¨åˆ†å’ŒReLUå·®ä¸å¤šï¼Œæœ‰çš„ä¼šåœ¨å…·ä½“çš„æŸç±»ä»»åŠ¡ä¸­è¡¨ç°å¥½äºReLUã€‚è¿™äº›æ¿€æ´»å‡½æ•°éƒ½å…·æœ‰å¦‚ä¸‹å½¢å¼ï¼š\n$$\n  f(x)=\\max(0, x) + \\alpha \\min(0, x)\n$$\n\n  å½“$\\alpha = -1$æ—¶ï¼Œ$f(x)=|x|$ã€‚ç§°ä¸ºAbsolute value rectificationã€‚å¥½åƒä¸å¸¸ç”¨ã€‚\n\n  > It is used for object recognition from images, where it makes sense to seek features that are invariant under a polarity reversal of the input illumination.\n\nå°†$\\alpha$å€¼ä½œä¸ºå‚æ•° å‚ä¸ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®­ç»ƒæ—¶ç§°ä¸ºparametric ReLU æˆ–è€… PReLU\n\n---\n\nå½“$\\alpha$å–ä¸€ä¸ªå¾ˆå°çš„å€¼æ—¶ï¼Œå¦‚$\\alpha = 0.01$ï¼Œç§°ä¸ºleaky ReLU:\n$$\nf(x)=\\max(0, x) +0.01 \\min(0, x)\n$$\n![](Activation_Function/leaky_ReLU_func.png)\n\n\n\n  Leaky ReLUå°±æ˜¯ç”¨æ¥è§£å†³ReLUçš„\"dead\" unitsé—®é¢˜çš„ã€‚è¿™é‡Œçš„$\\alpha$æ˜¯ä¸€ä¸ªå¾ˆå°çš„å¸¸æ•°ã€‚è¿™æ ·ï¼Œå³ä¿®æ­£äº†æ•°æ®åˆ†å¸ƒï¼Œåˆä¿ç•™äº†ä¸€äº›è´Ÿè½´çš„å€¼ï¼Œä½¿å¾—è´Ÿè½´ä¿¡æ¯ä¸ä¼šå…¨éƒ¨ä¸¢å¤±ã€‚\n\n## 2.5 Maxout\n\nmaxoutæ˜¯å¯¹ReLUå’Œleaky ReLUçš„æ³›åŒ–ï¼Œå…¶å…¬å¼ä¸ºï¼š\n$$\nf(x) = \\max(w_1^Tx+b_1,w_2^Tx+b_2)\n$$\nmaxoutå…·æœ‰ReLUçš„æ‰€æœ‰ä¼˜ç‚¹ï¼šReLU å¾—åˆ°çš„SGDçš„æ”¶æ•›é€Ÿåº¦ä¼šæ¯” sigmoid/tanh å¿«å¾ˆå¤šã€‚ç›¸æ¯”äº sigmoid/tanhï¼Œè®¡ç®—ç®€å•ã€‚\n\nmaxoutä¸å…·æœ‰ReLUçš„æ‰€ç¼ºç‚¹ï¼ˆdying ReLUï¼‰ã€‚\n\nä½†æ˜¯å®ƒå­˜åœ¨ä¸¤ç»„å‚æ•°ï¼Œæ‰€ä»¥å…¶å‚æ•°å¢åŠ ä¸ºåŸæ¥çš„ä¸¤å€ã€‚\n\n\n\n# 3 æ¿€æ´»å‡½æ•°çš„é€‰æ‹©\n\nç¥ç»ç½‘ç»œéšè—å±‚çš„æ¿€æ´»å‡½æ•°çš„é€‰æ‹©æ˜¯æ¯”è¾ƒå›°éš¾çš„ï¼Œè¿™ä¸ªéœ€è¦æ ¹ç»å…·ä½“çš„é—®é¢˜è¿›è¡Œå®éªŒã€‚\n\n- ReLUé€šå¸¸æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„é€‰æ‹©ï¼Œä½¿ç”¨ReLUæ—¶è¦æ³¨æ„å­¦ä¹ ç‡(learning rate)çš„è®¾ç½®ï¼Œå¦‚æœå¤ªå¤§å®¹æ˜“é€ æˆèŠ‚ç‚¹æ­»äº¡ï¼ˆ â€œdeadâ€ unitsï¼‰ã€‚æ‰€ä»¥åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­éœ€è¦ç›‘æ§ â€œdeadâ€ unitsåœ¨æ€»çš„èŠ‚ç‚¹ä¸­æ‰€å çš„æ¯”ä¾‹\n- å¦‚æœä½¿ç”¨ReLUæ—¶â€œdeadâ€ unitsçš„æ¯”ä¾‹è¾ƒé«˜ï¼Œå¯ä»¥å°è¯•Leaky ReLU å’Œ Maxout\n- sigmoid or tanhï¼šåœ¨ReLUå‡ºç°ä¹‹å‰ç¥ç»ç½‘ç»œä¸»è¦ä½¿ç”¨sigmoidå’Œtanhä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œç”±äºä¸Šé¢è¯´æ˜çš„åŸå› ï¼Œç°åœ¨å·²ç»å¾ˆå°‘ç”¨äº†ã€‚\n\n\n\nå…¶ä»–å¸¸è§æ¿€æ´»å‡½æ•°[è¯·å‚è§](https://en.wikipedia.org/wiki/Activation_function)\n\n\n\n# å‚è€ƒèµ„æ–™\n\nhttp://cs231n.github.io/neural-networks-1/\n\nDeep Leaning\n\nPattern Recognition and Machine Learning\n\n[ç¥ç»ç½‘ç»œä¹‹æ¿€æ´»å‡½æ•°(Activation Function)](http://blog.csdn.net/memray/article/details/51442059)\n\n[Activation function wikipedia](https://en.wikipedia.org/wiki/Activation_function)","tags":["Machine Learning"],"categories":["Neural Network"]},{"title":"ç¥ç»ç½‘ç»œåŸºç¡€","url":"%2Fblog%2FBasic_of_Neural_Networks.html","content":"\n\n\nè™½ç„¶å­¦ä¹ ç¥ç»ç½‘ç»œå¾ˆé•¿æ—¶é—´äº†ï¼Œä½†æ˜¯ä¹Ÿç»å¸¸é‡åˆ°é—®é¢˜çš„æ—¶å€™äº§ç”Ÿæ··ä¹±ï¼Œæœ¬æ¬¡è®¡ç®—åšä¸€ä¸ªç¥ç»ç½‘ç»œçš„ä¸“é¢˜ï¼Œå°†æˆ‘çœ‹åˆ°çš„å…³äºç¥ç»ç½‘ç»œçš„ä¸œè¥¿æŒ‰ç…§æˆ‘è‡ªå·±çš„å­¦ä¹ å’Œç†è§£ä¹ æƒ¯æ•´ç†ä¸‹æ¥ã€‚\n\næœ¬æ–‡æ˜¯è¿™ä¸ªä¸“é¢˜çš„ç¬¬ä¸€ç« ï¼Œä»‹ç»åŸºæœ¬çš„ç¥ç»ç½‘ç»œä¸­çŸ¥è¯†ã€‚å¹¶åœ¨æœ€åæ•´ç†äº†ä¸€ä¸‹ç¥ç»ç½‘ç»œçš„è®¾è®¡ä¸­çš„å‡ ä¸ªé—®é¢˜ï¼Œä»¥åä¼šé€ä¸€å­¦ä¹ ã€‚\n\n# 1 å¯¹ä¸€ä¸ªç¥ç»å…ƒ(neuron)å»ºæ¨¡\n\näººå·¥ç¥ç»ç½‘ç»œä¸€å¼€å§‹æ˜¯ç”¨æ¥å¯¹ç”Ÿç‰©ç¥ç»ç³»ç»Ÿè¿›è¡Œå»ºæ¨¡çš„ï¼Œåæ¥å‘ç°å®ƒåœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­èƒ½å¤Ÿè·å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Œå†ç„¶åçš„äº‹æƒ…å¤§å®¶å°±éƒ½çŸ¥é“äº†ã€‚\n\näººå·¥ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¯¹ç”Ÿç‰©ç¥ç»å…ƒçš„æ•°å­¦å»ºæ¨¡ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸¤è€…ä¹‹é—´æ˜¯å¯ä»¥ç±»æ¯”çš„ã€‚å…¶ç±»æ¯”å…³ç³»å¦‚ä¸‹ï¼š\n\n| ç”Ÿç‰©ç¥ç»å…ƒ     | çªè§¦(synapse) | æ ‘çª(dendrity)          | ç»†èƒä½“(cell body)                     | è½´çª(axon)                                |\n| --------- | ----------- | --------------------- | ---------------------------------- | --------------------------------------- |\n| **äººå·¥ç¥ç»å…ƒ** | æƒé‡ $w_i$    | ä¹˜æ³•æ“ä½œ $w_i \\times x_i$ | æ¿€æ´»å› å­  $a=\\sum_i {w_i \\times x_i }$ | æ¿€æ´»å‡½æ•° $f(a)=f(\\sum_i {w_i \\times x_i })$ |\n\nåœ¨ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­ï¼Œä¸€ä¸ªç¥ç»å…ƒé€šè¿‡æ ‘çª(dendrity)æœ«æ¢¢çš„çªè§¦(synapse)æ¥å—å¦ä¸€ä¸ªç¥ç»å…ƒä¼ åˆ°è¿‡æ¥çš„ç¥ç»å†²åŠ¨(impulse)ï¼Œçªè§¦ä¸Šçš„ä¸€äº›åŒ–å­¦ç‰©è´¨å¯¹æ¥å—åˆ°çš„ç¥ç»å†²åŠ¨äº§ç”Ÿå…´å¥‹æˆ–è€…æŠ‘åˆ¶ï¼Œæ ‘çªå°†é€šè¿‡çªè§¦å¤„ç†çš„ä¿¡å·ä¼ é€’åˆ°ç»†èƒä½“ã€‚è€Œä¸€ä¸ªç¥ç»å…ƒç»†èƒé€šè¿‡æ ‘çªå’Œè‹¥å¹²ä¸ªå…¶å®ƒç»†èƒç›¸è¿æ¥ï¼Œåœ¨ç»†èƒä½“ä¸Šå°†æ‰€æœ‰çš„æ ‘çªæ¥æ”¶åˆ°çš„ç¥ç»å†²åŠ¨è¿›è¡Œæ±‡æ€»ï¼Œæ ¹æ®æ±‡æ€»çš„ç¥ç»å†²åŠ¨å†³å®šè¯¥ç¥ç»å…ƒæ˜¯å¦è¢«æ¿€æ´»ï¼Œå¹¶æŠŠæ¿€æ´»çŠ¶æ€ä¿¡æ¯é€šè¿‡è½´çªç»™å…¶å®ƒç¥ç»å…ƒã€‚\n\n![ç”Ÿç‰©ç¥ç»å…ƒæ¨¡å‹](Basic_of_Neural_Networks/A_cartoon_drawing_of_a_biological_neuron.png)\n\nåœ¨äººå·¥ç¥ç»ç½‘ç»œä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹ä¸Šæœ€å…ˆå’Œä¸Šä¸€å±‚èŠ‚ç‚¹ä½œç”¨çš„æ˜¯æƒé‡å› å­ $w_i$(ç±»æ¯”äºçªè§¦), å…¶ä½œç”¨æ˜¯æ§åˆ¶å‰ä¸€ä¸ªèŠ‚ç‚¹ä¼ é€’è¿‡æ¥çš„ä¿¡æ¯å¯¹äºå½“å‰èŠ‚ç‚¹çš„å½±å“(å…´å¥‹æˆ–æŠ‘åˆ¶) ã€‚ç„¶åé€šè¿‡ç±»ä¼¼äºæ ‘çªçš„ç»“æ„å°†ä¿¡æ¯$w_i \\times x_i$ä¼ å¯¼åˆ°ç»†èƒä½“ï¼Œåœ¨ç»†èƒä½“é‡å°†æ‰€æœ‰å½±å“ç›¸åŠ å¾—åˆ°æ¿€æ´»å› å­$a$ã€‚ç„¶åæ ¹æ®æ¿€æ´»å› å­çš„å¤§å°æŒ‰ç…§æ¿€æ´»å‡½æ•°$f(a)$æ¥å†³å®šè¿™ä¸ªèŠ‚ç‚¹æ˜¯å¦è¢«æ¿€æ´»å¹¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚\n\n![æ•°å­¦ç¥ç»å…ƒæ¨¡å‹](Basic_of_Neural_Networks/a_mathematical_neuron_model.png)\n\né€šè¿‡ä»¥ä¸Šåˆ†æå¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªç¥ç»å…ƒå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªä»¥$w_0 \\dots w_n$ä¸ºå‚æ•°ï¼Œ$x_0\\dots x_n$ä¸ºè¾“å…¥çš„å‡½æ•°ï¼š\n$$\nh(x0\\dots x_n;w0 \\dots w_n)=f(\\sum_{i=0}^n {w_i x_i})\n$$\nå…¶ä¸­$f$æ˜¯ç”¨æˆ·çš„å®šä¹‰çš„æ¿€æ´»å‡½æ•°ã€‚\n\n# 2 äººå·¥ç¥ç»ç½‘ç»œçš„ç»“æ„\n\näººå·¥ç¥ç»ç½‘ç»œæ˜¯å°†è‹¥å¹²è¿™è¿™æ ·çš„ç¥ç»å…ƒè¿›è¡Œè¿æ¥æ¥å¯¹ç”Ÿç‰©ç¥ç»ç³»ç»Ÿè¿›è¡Œå»ºæ¨¡çš„ï¼Œå…¶ä¸­åŒ…æ‹¬ è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å…¥å±‚ã€‚æ¯ä¸€å±‚ç”±è‹¥å¹²èŠ‚ç‚¹(ç¥ç»å…ƒ)ç»„æˆã€‚\n\nè¾“å…¥å±‚ (input layer)ï¼šè®­ç»ƒæ•°æ®çš„å„ç»´åº¦å±æ€§ç‰¹å¾ï¼Œæ¯ä¸€ä¸ªè¾“å…¥å±‚èŠ‚ç‚¹å¯¹åº”ä¸€ä¸ªå±æ€§ã€‚è¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡ç”±æ•°æ®å†³å®šã€‚è¾“å…¥å±‚å¿…é¡»æœ‰ã€‚\n\néšè—å±‚(hidden layer)ï¼šç”±è‹¥å¹²ä¸Šæ–‡æ‰€è¿°çš„å®Œæ•´ç¥ç»å…ƒç»„æˆçš„ã€‚éšè—å±‚çš„å±‚æ•°å¯ä»¥ç”±ç”¨æˆ·å®šä¹‰ï¼Œæ¯ä¸€å±‚çš„ç¥ç»å…ƒæ•°é‡ä¹Ÿå¯ä»¥å®šä¹‰ï¼Œéšè—å±‚å¯ä»¥æœ‰ï¼Œä¹Ÿå¯ä»¥æ²¡æœ‰ã€‚\n\nè¾“å‡ºå±‚(output layer)ï¼šå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºç­‰çš„èŠ‚ç‚¹æ•°é‡ä¸ºç±»åˆ«æ•°é‡ã€‚è¾“å‡ºå±‚å¿…é¡»æœ‰ã€‚\n\n![multi_layer_forward_networks](Basic_of_Neural_Networks/muliti_layer_forward_networks.png)\n\n# 3 å‰å‘ä¼ æ’­ç®—æ³•(forward propagation)\n\nåœ¨Deep Learningä¸­6.5èŠ‚æ˜¯è¿™æ ·è¡¨è¿°å‰å‘ä¼ æ’­ç®—æ³•çš„ï¼š\n\n> When we use a feedforward neural network to accept an input $x$ and produce an output $\\hat y$, information flows forward through the network. The inputs $x$ provide the initial information that then propagates up to the hidden units at each layer and finally produces $\\hat y$. This is called forward propagation. \n\nè¾“å…¥ä¿¡æ¯$x$é€å±‚é€šè¿‡ç¥ç»ç½‘ç»œä¸­çš„å„å±‚ï¼Œåœ¨æ¯ä¸€å±‚ä¸­ç»è¿‡è®¡ç®—ï¼Œæœ€ç»ˆåœ¨è¾“å‡ºå±‚å¾—åˆ°å€¼$\\hat y$çš„è¿‡ç¨‹ã€‚ å³å‰å‘ä¼ æ’­ç®—æ³•æ˜¯å°†**è¾“å…¥ä¿¡æ¯**é€å±‚è¿›è¡Œ**è®¡ç®—**ï¼Œå‘å‰è¿›è¡Œä¼ æ’­çš„è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­è¾“å…¥xæ˜¯ç¡®å®šçš„ï¼Œåœ¨è®¾è®¡å¥½çš„ç¥ç»ç½‘ç»œä¸­æ¯ä¸€å±‚çš„è®¡ç®—æ–¹å¼ä¹Ÿæ˜¯ç¡®å®šçš„(å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¯ä¸ªç¥ç»å…ƒçš„æƒé‡$w_i$ æ˜¯ç¡®å®šå€¼)ã€‚\n\n# 4 åå‘ä¼ æ’­ç®—æ³•(back propagation)\n\nå…ˆä»çº¿æ€§å›å½’æ¨¡å‹çš„è®­ç»ƒè¯´èµ·ï¼šå¯¹äºæ ·æœ¬é›† $(x^{(i)},t^{(i)}),i\\in [1,N]$, æ‹¬å·ä¸­ä¸Šæ ‡æ˜¯æ ·æœ¬åºå·\n\nçº¿æ€§å›å½’æ¨¡å‹ä¸ºï¼š $\\hat y_w^{(i)} = w^Tx^{(i)} = \\sum_j{w_j x_j}^{(i)}$ ï¼ˆ$x$ å’Œ $\\hat y$ æ‹¬å·ä¸­ä¸Šæ ‡æ˜¯æ ·æœ¬åºå·ï¼Œ$x$çš„ä¸‹è¡¨ä¸ºå±æ€§åºå·ï¼‰\n\næŸå¤±å‡½æ•°ä¸ºï¼š $ E_w = \\frac12\\sum_{i=1}^N (\\hat y_w^{(i)} - t^{(i)}))^2 $ \n\nç›®æ ‡å‡½æ•°å°±æ˜¯(å®é™…è¿˜éœ€è¦åŠ æ­£åˆ™åŒ–)ï¼š $\\min_w E_w$\n\nä¹Ÿå°±æ˜¯è¯´é€šè¿‡å¤šæ¬¡è°ƒæ•´å‚æ•°$w$çš„å€¼ä½¿å¾—æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚è€Œè¿™ä¸ªè°ƒæ•´å‚æ•°çš„è¿‡ç¨‹å°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•ï¼Œè¿­ä»£å…¬å¼ä¸ºï¼š\n$$\nw_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j}E_w\n$$\nå…¶ä¸­: $\\alpha$ æ˜¯å­¦ä¹ ç‡,\n$$\n\\begin {aligned} \\\\\n\\frac{\\partial}{\\partial w_j}E_w &= \\frac{\\partial}{\\partial w_j} \\frac12\\sum_{i=1}^N (\\hat y_w^{(i)} - t^{(i)})^2 \\\\\n& =\\sum_{i=1}^N (\\hat y_w^{(i)} - t^{(i)})\\frac{\\partial}{\\partial w_j}(\\hat y_w^{(i)} - t^{(i)}) \\\\\n&=\\sum_{i=1}^N (\\hat y_w^{(i)} - t^{(i)})\\frac{\\partial}{\\partial w_j}(\\sum_j{w_j x_j}^{(i)} - t^{(i)}) \\\\\n&=\\sum_{i=1}^N (\\hat y_w^{(i)} - t^{(i)})x_j^{(i)}\n\\end {aligned}\n$$\n*note: ä»¥ä¸Šå…¬å¼æ˜¯æŒ‰ç…§æ¢¯åº¦ä¸‹é™æ³•æ¨å¯¼çš„, å¦‚æœæ˜¯éšæœºæ¢¯åº¦ä¸‹é™æ³• $N=1$, å¦‚æœæ˜¯mini batchæ¢¯åº¦ä¸‹é™æ³•$N=batch size$*\n\n---\n\nå›åˆ°æ­£é¢˜ï¼\n\nå‰å‘ä¼ æ’­ç®—æ³•å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°$\\hat y(x^{(i)},W)$ ã€‚å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬$(x^{(i)},t^{(i)})$ï¼Œå‰å‘ä¼ æ’­ç®—æ³•ä»¥$x^{(i)}$ä¸ºè¾“å…¥äº§ç”Ÿä¸€ä¸ªé¢„æµ‹å€¼$\\hat y_W^{(i)} =\\hat y(x^{(i)},W)$ã€‚\n\næ‰€ä»¥å…¶æŸå¤±å‡½æ•°ä¸ºï¼š\n$$\nE_W^{(i)} = \\frac12 (\\hat y_W^{(i)} - t^{(i)}))^2\n$$\nä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„$w \\in W$çš„è¿­ä»£å…¬å¼ä¸º\n$$\nw_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j}E_w\n$$\nåˆ°ç°åœ¨ä¸ºæ­¢ä¸€åˆ‡è¿›è¡Œçš„éƒ½å¾ˆé¡ºåˆ©ï¼Œä½†æ˜¯é—®é¢˜æ¥äº†, $ \\frac{\\partial}{\\partial w_j}E_w$ æ€ä¹ˆæ±‚ï¼Ÿ\n\n![three_layers_networks](Basic_of_Neural_Networks/three_layers_networks.png)\n\nä¸Šå›¾æ˜¯PRMLä¸­çš„Figure5.1ï¼Œå…¶ä¸­$y_K$çš„è®¡ç®—å…¬å¼ä¸ºï¼š\n$$\ny_K(x, w) = \\sigma (\\sum_{j=1}^M w_{kj}^{(2)} h(\\sum_{i=1}^D {w_{ji}^{(1)} x_i} + w_{j0}^{(1)}) + w_{k0}^{(2)})\n$$\nè¯¥å…¬å¼ä¸­ æ‹¬å·ä¸­çš„ä¸Šæ ‡ä¸ºç¥ç»ç½‘ç»œçš„å±‚å·ï¼Œè¾“å…¥å±‚xæ˜¯ç¬¬0å±‚ï¼Œéšè—å±‚zæ˜¯ç¬¬1å±‚ï¼Œè¾“å‡ºå±‚yæ˜¯ç¬¬2å±‚ã€‚\n\nå¦‚æœè¦æ±‚$w_{ji}^{(1)}$ ä¸­çš„$w_{11}^{(1)}$ ï¼Œæ ¹æ®æ±‚å¯¼æ³•åˆ™éœ€è¦4æ¬¡æ±‚å¯¼ã€‚å¦‚æœç¥ç»ç½‘ç»œæœ‰100å±‚å‘¢ï¼Ÿ\n\n**ç¥ç»ç½‘ç»œç”±äºä½¿ç”¨äº†å¤šå±‚çš„åµŒå¥—ç»“æ„ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­æ·±å±‚çš„å‚æ•°$w$çš„æ±‚å¯¼è¿‡ç¨‹å¤æ‚ã€‚**\n\nåå‘ä¼ æ’­ç®—æ³•å°±æ˜¯ç”¨äºè§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Œåœ¨Deep Learningä¸­6.5èŠ‚æ˜¯è¿™æ ·è¡¨è¿°åå‘ä¼ æ’­ç®—æ³•çš„ï¼š\n\n> During training, forward propagation can continue onward until it produces a scalar cost $J(\\theta)$. The back-propagation algorithm, often simply called backprop, allows the information from the cost to then flow backwards through the network, in order to compute the gradient.\n\nä¹Ÿå°±æ˜¯è¯´åå‘ä¼ æ’­æ˜¯å°†æŸå¤±å‡½æ•°çš„ä¿¡æ¯ç”±è¾“å‡ºå±‚å‘è¾“å…¥å±‚æ–¹å‘é€å±‚ä¼ é€’ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°wçš„åå¯¼æ•°ã€‚\n\nä¸‹é¢æŒ‰ç…§backpropagationæ¥è®¡ç®—ä¸‹å›¾ä¸­çš„$ \\frac{\\partial E}{\\partial w_{ji}}$ ï¼Œä»¥ä¸‹æ‰€æœ‰æ¨åˆ°éƒ½æ˜¯æŒ‰ç…§éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œæ‰€ä»¥åªéœ€è¦æ±‚ä¸ç¬¬nä¸ªæ ·æœ¬ç›¸å…³çš„æ¢¯åº¦ï¼Œæ•…çœç•¥nï¼š\n\n![backpropagation](Basic_of_Neural_Networks/backpropagation.png)\n\n> ä¸Šå›¾ä¸­æœ‰åšåˆ°å³ä¾æ¬¡ä¸ºè¾“å…¥å±‚(ä»¥$i$ä¸º, å…±$I$ä¸ª)ã€éšè—å±‚(ä»¥$j$ä¸ºä¸‹æ ‡, å…±$J$ä¸ªèŠ‚ç‚¹)ã€è¾“å‡ºå±‚(ä»¥$k$ä¸ºä¸‹æ ‡, å…±$K$ä¸ªèŠ‚ç‚¹)ã€‚\n>\n> å¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼šå­—æ¯$a$è¡¨ç¤ºæ¿€æ´»å› å­ï¼Œå¦‚ï¼š$a_j = \\sum_{i=0}^I w_{ji}z_i$ ,$a_k = \\sum_{j=0}^J w_{kj}z_j$\n>\n> éšè—å±‚çš„æ¿€æ´»å‡½æ•°ä¸º$h$ ,  è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ä¸ºidentity, \n>\n> å­—æ¯$z$ è¡¨ç¤ºèŠ‚ç‚¹è¾“å‡ºå€¼ï¼Œå¦‚: $z_j = h(a_j)$ ,$z_k = a_k$ \n\né¦–å…ˆä½¿ç”¨é“¾å¼æ³•åˆ™ï¼š\n\n$$\n\\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial a_j} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i\n$$\n\nå…¶ä¸­\n\n$$\n\\frac{\\partial a_j}{\\partial w_{ji}} =z_i\n$$\n\n$$\n\\delta_j \\equiv \\frac{\\partial E}{\\partial a_j}\n$$\n\n**$\\delta_j$è¡¨ç¤º$j$è¿™ä¸ªèŠ‚ç‚¹å¯¹æœ€ç»ˆçš„è¯¯å·®éœ€è¦è´Ÿå¤šå°‘è´£ä»»ã€‚**è¾“å‡ºå±‚èŠ‚ç‚¹$k$ çš„è¯¯å·®æ˜¯(è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ä¸ºidentity, $\\hat y_k = z_k = a_k$ ):\n$$\n\\delta_k =\\frac {\\partial}{\\partial a_k}\\frac12(z_k - t_k)^2 = z_k - t_k\n$$\nåå‘ä¼ æ’­ç®—æ³•å¯ä»¥ç†è§£ä¸ºï¼Œè¯¯å·®$\\delta$ä»è¾“å±‚å‘è¾“å…¥æˆæ–¹å‘ä¼ æ’­çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä¹Ÿå«åš **Error Backpropagation** ã€‚\n\nä¸‹é¢åªéœ€è¦æ¨å¯¼$\\delta_j$ , åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œ\n$$\n\\begin{aligned}\n\\delta_j & \\equiv \\frac{\\partial E}{\\partial a_j} \\\\\n&= \\sum_{k=1}^K \\frac {\\partial E}{\\partial a_k} \\frac{\\partial a_k}{\\partial a_j} \\\\\n& = h'(a_j) \\sum_{k=1}^K w_{kj} \\delta_k\n\\end{aligned}\n$$\n\nå…¶ä¸­\n$$\n\\frac {\\partial E}{\\partial a_k} = \\delta_k\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial a_k}{\\partial a_j} & = \\frac{\\partial }{\\partial a_j}(\\sum_{j=0}^Jz_j w_{kj}) \\\\\n&=\\frac{\\partial }{\\partial a_j}(\\sum_{j=0}^J h(a_j) w_{kj}) \\\\\n&= h'(a_j) \\sum_{k=1}^K w_{kj}\n\\end{aligned}\n$$\n**åå‘ä¼ æ’­çš„ç›®æ ‡æ˜¯ä¸ºäº†è®¡ç®—åå¯¼æ•°$\\frac{\\partial E}{\\partial w_{ji}}$ ï¼Œæ–¹æ³•æ˜¯è¯¯å·®$\\delta$ æ²¿ç€ç¥ç»ç½‘ç»œå‘åä¼ æ’­ã€‚**\n\nPRLM ä¸­5.3.1 èŠ‚å°†åå‘ä¼ æ’­çš„æµç¨‹æ€»ç»“å¦‚ä¸‹\n\n> 1. Apply an input vector $x_n$ to the network and forward propagate through\n>    the network using $a_j=\\sum_iw_{ji}z_i$ and $z_j = h(a_j)$ to find the activations of all the hidden and output units.\n> 2. Evaluate the $\\delta_k$ for all the output units using $\\delta_k =\\frac {\\partial}{\\partial a_k}\\frac12(z_k - t_k)^2 = z_k - t_k$\n> 3. Backpropagate the $\\delta$ using $\\delta_j =  h'(a_j) \\sum_{k=1}^K w_{kj} \\delta_k $to obtain $\\delta_j$ for each hidden unit in the network.\n> 4. Use $\\frac{\\partial E}{\\partial w_{ji}} = \\delta_j z_i$ to evaluate the required derivatives\n\n\n\n# 5 ç¥ç»ç½‘ç»œé—®é¢˜å¤§çº²\n\næœ€åç»™è‡ªå·±ç«‹ä¸€ä¸ªflagï¼Œä»¥ä¸‹æ˜¯æˆ‘æ•´ç†çš„å‡ ä¸ªç¥ç»ç½‘ç»œçš„è®¾è®¡ä¸­çš„å‡ ä¸ªé—®é¢˜ï¼Œä»¥åä¼šé€ä¸€å­¦ä¹ ã€‚\n\n1. ç½‘ç»œç»“æ„è®¾è®¡ï¼šéšè—å±‚æ•°é‡ã€éšè—å±‚ç¥ç»å…ƒæ•°é‡\n2. æ¿€æ´»å‡½æ•°é€‰æ‹©ï¼šReLuï¼Œtanhï¼Œsigmoid â€¦â€¦\n3. Error function è®¾è®¡ï¼šsum-of-squaresã€Cross Entropy â€¦â€¦\n4. æ­£åˆ™åŒ–é—®é¢˜ï¼šL2ï¼Œdropout â€¦â€¦\n5. ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ï¼šSGDã€GD â€¦â€¦\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[CS231n Convolutional Neural Networks for Visual Recognition lecture notes](http://cs231n.github.io/neural-networks-1/)\n\nDeep Leaning\n\nPattern Recognition and Machine Learning","tags":["Machine Learning"],"categories":["Neural Network"]},{"title":"Pythonå‡½æ•°çš„å‚æ•°","url":"%2Fblog%2FParameters_of_Function_in_Python.html","content":"\nå‡½æ•°å®šä¹‰ä¸­ä»å½¢å¼ä¸Šçœ‹å­˜åœ¨å››ç§å½¢å¼çš„å‚æ•°ï¼š\n\nä½ç½®å‚æ•°ï¼š```def tupleSqeArgs(arg1[,arg2])```\n\né»˜è®¤å‚æ•°ï¼š```def tupleSqeArgs([arg1,[arg2,]] arg3='defaultB')```\n\nå…ƒç»„å‚æ•°ï¼š```def tupleVarArgs([arg1, [arg2='defaultB',]] *nkw)```\n\nå­—å…¸å‚æ•°ï¼š```def dictVarArgs([arg1, [arg2='defaultB', [*nkw,]]] **kw)```\n\n[]å†…çš„å†…å®¹æ˜¯å¯é€‰çš„ã€‚\n\næŒ‰ç…§*pythonæ ¸å¿ƒç¼–ç¨‹*ä¸­çš„å®šä¹‰ï¼Œå‚æ•°çš„åˆ†ç±»æ˜¯è¿™æ ·çš„\n\n > - å½¢å¼å‚æ•°\n >    - ä½ç½®å‚æ•°\n >    - é»˜è®¤å‚æ•°\n > - å¯å˜é•¿åº¦çš„å‚æ•°\n >    - éå…³é”®å­—å¯å˜é•¿å‚æ•°ï¼ˆå…ƒç»„ï¼‰\n >    - å…³é”®å­—å˜é‡å‚æ•°ï¼ˆå­—å…¸ï¼‰\n\næ„Ÿè§‰è¯»èµ·æ¥æ˜¯ä¸æ˜¯å¾ˆç»•å£ä¸”ä¸å®¹æ˜“è®°å¿†ï¼ˆåæ­£æˆ‘è®°ä¸ç€åé¢ä¸¤ä¸ªï¼‰ï¼Œæœ¬æ–‡ç»“åˆè‡ªå·±çš„ä¸€äº›å®éªŒï¼ŒæŒ‰ç…§è‡ªå·±çš„è®°å¿†å’Œå­¦ä¹ ä¹ æƒ¯ï¼Œå°†pythonå‡½æ•°çš„å‚æ•°é‡æ–°åˆ†ä¸ºå¦‚ä¸‹ä¸‰ç±»ï¼š\n\n1. ä½ç½®å‚æ•°\n2. å…ƒç»„å‚æ•°\n3. å…³é”®è¯å‚æ•°\n\nNoteï¼šPythonçš„å‡½æ•°å‚æ•°éœ€è¦ä»**å‡½æ•°å®šä¹‰**å’Œ**å‡½æ•°è°ƒç”¨**ä¸¤ä¸ªæ–¹å‘ç†è§£ã€‚\n\n## 1 ä½ç½®å‚æ•°\n\nè°ƒç”¨æ—¶éœ€è¦æŒ‰ç…§å‡½æ•°å®šä¹‰ä¸­å‚æ•°çš„å‡ºç°é¡ºåºè¿›è¡Œè°ƒç”¨ã€‚\n\n**é»˜è®¤å‚æ•°**ï¼šä¸€ç§ç‰¹æ®Šçš„ä½ç½®å‚æ•°ï¼šé»˜è®¤å‚æ•°å¿…é¡»å‡ºç°åœ¨ä½ç½®å‚æ•°ä¹‹åï¼Œå‡½æ•°è°ƒç”¨æ—¶å¯ä»¥ä¸å¯¹é»˜è®¤å‚æ•°ä¼ å€¼ã€‚\n\n```python\ndef tupleSqeArgs(arg1,arg2, arg3='defaultB'): \n    print 'formal arg 1:', arg1 \n    print 'formal arg 2:', arg2\n    print 'formal arg 3:', arg3\n```\n\nè°ƒç”¨æ–¹å¼å¦‚ä¸‹ï¼š\n\n```python\ntupleSqeArgs(1,2)\ntupleSqeArgs(1,2,3)\ntupleSqeArgs(arg1=1,arg2=2) #å¯ä»¥ç†è§£ä¸ºå…³é”®è¯å‚æ•°\ntupleSqeArgs(arg1=1,arg2=2,arg3=3) #å¯ä»¥ç†è§£ä¸ºå…³é”®è¯å‚æ•°\n```\n\n## 2 å…ƒç»„å‚æ•°ï¼ˆTupleï¼‰\n\nå…ƒç»„ä¿å­˜äº†æ‰€æœ‰ä¼ é€’ç»™å‡½æ•°çš„\"é¢å¤–\"çš„å‚æ•°(åŒ¹é…äº†æ‰€æœ‰ä½ç½®å’Œå…·åå‚æ•°åå‰©ä½™çš„)ã€‚å‡½æ•°å®šä¹‰ä¸­ï¼Œæ‰€æœ‰çš„å…ƒç¥–å‚æ•°å¿…é¡»åœ¨ä½ç½®å‚æ•°ä¹‹åï¼Œä¸”åªèƒ½æœ‰ä¸€ä¸ªã€‚\n\n```python\ndef tupleVarArgs(arg1, arg2='defaultB', *nkw): \n    print 'formal arg 1:', arg1 \n    print 'formal arg 2:', arg2\n    for eachXtrArg in nkw: \n        print 'another arg:', eachXtrArg,type(eachXtrArg)\n```\n\nè°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n\n```python\ntupleVarArgs('abc')  #len(nkw) == 0\ntupleVarArgs(23, 4.56)  #len(nkw) == 0\ntupleVarArgs('abc', 123, 'xyz', 456.789)   #len(nkw) == 2,nkw==('xyz', 456.789)\nt = ('xyz', 456.789)\ntupleVarArgs('abc', 123, *t)   #len(nkw) == 2,nkw==('xyz', 456.789)\n```\n\n## 3 å…³é”®è¯å‚æ•°ï¼ˆDictionaryï¼‰\n\nåœ¨æˆ‘ä»¬æœ‰ä¸å®šæ•°ç›®çš„æˆ–è€…é¢å¤–é›†åˆçš„å…³é”®å­—çš„æƒ…å†µä¸­ï¼Œ å‚æ•°è¢«æ”¾å…¥ä¸€ä¸ªå­—å…¸ä¸­ï¼Œå­—å…¸ä¸­é”®ä¸ºå‚æ•°åï¼Œå€¼ä¸ºç›¸åº”çš„å‚æ•°å€¼ã€‚\n\n```python\ndef dictVarArgs(arg1, arg2='defaultB', **kw):\n    print 'formal arg1:', arg1 \n    print 'formal arg2:', arg2 \n    for eachXtrArg in kw.keys():\n        print 'Xtra arg %s: %s' %  (eachXtrArg, str(kw[eachXtrArg])) \n```\n\nè°ƒç”¨æ–¹å¼ä¸ºï¼š\n\n```python\ndictVarArgs(1220, 740.0, c='grail') \ndictVarArgs(arg2='tales', c=123, d='poe', arg1='mystery')\n\nd = {'arg1': 'mystery',\n'arg2': 'tales',\n'c': 123,\n'd': 'poe'}\ndictVarArgs(**d)\n\nd = {'c': 123,'d': 'poe'}\ndictVarArgs('mystery','tales', **d)\n```\n\n\n\n# 4 ä½ç½®å‚æ•°&å…ƒç»„å‚æ•°&å…³é”®è¯å‚æ•°å…±åŒå‡ºç°æ—¶\n\n```python\ndef dictVarArgs(arg1, arg2='defaultB',*nkw, **kw):\n    print 'formal arg1:', arg1\n    print 'formal arg2:', arg2\n    for eachXtrArg in nkw: \n        print 'another arg:', eachXtrArg,type(eachXtrArg)    \n    for eachXtrArg in kw.keys():\n        print 'Xtra arg %s: %s' %  (eachXtrArg, str(kw[eachXtrArg]))\n\n```\n\nåˆæ³•çš„è°ƒç”¨æ–¹æ³•å¦‚ä¸‹:\n\n```python\n dictVarArgs(123, 567,'tales', 'mys',c=123, d='poe') \n # å…ƒç»„å‚æ•°ï¼ˆ'tales', 'mys'ï¼‰å¿…é¡»å‡ºç°åœ¨å…·åå‚æ•°ï¼ˆå…³é”®è¯å‚æ•°ï¼‰ä¹‹å‰ï¼Œåœ¨ä½ç½®å‚æ•°ä¹‹å\n```\n\néæ³•è°ƒç”¨å¦‚ä¸‹ï¼š\n\n```python\ndictVarArgs(123, arg2=567,'tales', 'mys',c=123, d='poe') \n# è¿™ç§è°ƒç”¨æ–¹æ³•arg2=567ï¼Œè®¤ä¸ºarg2æ—¶å…³é”®è¯å‚æ•°ã€‚å…ƒç»„å‚æ•°å‡ºç°åœ¨äº†å…³é”®è¯å‚æ•°ä¹‹åï¼Œè°ƒç”¨å¤±è´¥ã€‚\n```\n\n","tags":["Python"],"categories":["Python"]},{"title":"Pythonç¼–ç é—®é¢˜","url":"%2Fblog%2FPython_Encode.html","content":"\nä»ä¸€å¼€å§‹æ¥è§¦Pythonå°±è¢«å®ƒçš„ç¼–ç é—®é¢˜å›°æ‰°ï¼Œé‡åˆ°é—®é¢˜å°±ä»ç½‘ä¸Šæ‰¾ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œèƒ½è§£å†³é—®é¢˜å°±ç”¨ï¼Œä½†æ˜¯ä¹Ÿä¸æ˜ç™½å…¶ä¸­çš„åŸç†ï¼Œç”¨å®Œå°±å¿˜äº†ï¼Œä¸‹æ¬¡é‡åˆ°é—®é¢˜çš„æ—¶å€™å†å»æ‰¾ã€‚æœ€è¿‘æˆ‘è¯æ—¶é—´é˜…è¯»äº†ä¸€äº›åšå®¢å’Œä»£ç ï¼Œå°†ç›¸å…³çš„é—®é¢˜æ•´ç†ä¸‹æ¥ã€‚\n\n# 1 ç¼–ç (Encode)ä¸è§£ç (Decode)\n\n- ä¸–ç•Œä¸Šåªæœ‰ä¸¤ç§ç¼–ç ï¼šUnicodeå’Œother(ASCII,GBK,BIG5,UTF-8)\n- otherä¸­çš„æ‰€æœ‰ç¼–ç éƒ½å¯ä»¥ä¸unicodeç¼–ç ç›¸äº’è½¬åŒ–ã€‚Unicodeæ˜¯å„ç§ç¼–ç ä¹‹é—´è½¬æ¢çš„åª’ä»‹ï¼Œotherä¸­çš„ç¼–ç ä¸èƒ½ç›´æ¥ç›¸äº’è½¬åŒ–\n- ASCIIç æ˜¯å•å­—èŠ‚ç¼–ç ï¼ŒGBK,BIG5,UTF-8ç­‰æ˜¯å¤šå­—èŠ‚ç¼–ç ã€‚è¿™ä¸€ç‚¹å¾ˆé‡è¦\n- æ“ä½œç³»ç»Ÿå’Œå¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€éƒ½ç›´æ¥æ”¯æŒUnicodeã€‚åœ¨è®¡ç®—æœºå†…å­˜ä¸­å¤„ç†å­—ç¬¦æ—¶(å¦‚ä½¿ç”¨Notepad++ç¼–è¾‘ä¸€ä¸ªtxtæ–‡ä»¶)ï¼Œç»Ÿä¸€ä½¿ç”¨Unicodeç¼–ç ï¼Œå½“éœ€è¦ä¿å­˜åˆ°ç¡¬ç›˜æˆ–è€…éœ€è¦ä¼ è¾“çš„æ—¶å€™ï¼Œå°±è½¬æ¢ä¸ºotherä¸­ç¼–ç (å¦‚UTF-8)å¯¹åº”çš„å­—èŠ‚æµã€‚\n- Encode: å°† Unicodeå­—ç¬¦æµ è½¬æ¢ä¸º otherä¸­ç¼–ç (å¦‚UTF-8)å¯¹åº”çš„å­—èŠ‚æµ çš„è¿‡ç¨‹\n- Decode: å°† otherä¸­ç¼–ç (å¦‚UTF-8)å¯¹åº”çš„å­—èŠ‚æµ è½¬æ¢ä¸º Unicodeå­—ç¬¦æµ çš„è¿‡ç¨‹\n\n\n![](Python_Encode/char_codes.png)\n\n# 2 å¸¸è§ç¼–ç é—®é¢˜\n\næˆ‘å¸¸é‡åˆ°çš„ä¸Pythonç›¸å…³çš„ç¼–ç é—®é¢˜æœ‰ä¸¤ä¸ªã€‚ä¸ºäº†èƒ½å¤Ÿè¯¦ç»†çš„è¯´æ˜é—®é¢˜ï¼Œæˆ‘Pythonæºæ–‡ä»¶çš„ç¼–è¾‘åˆ°æ‰§è¡Œåˆ†ä¸ºäº†ä¸‰ä¸ªé˜¶æ®µï¼š1. æºæ–‡ä»¶çš„ç¼–è¾‘ï¼›2. è§£é‡Šå™¨è¯»å–æºæ–‡ä»¶; 3. è§£é‡Šå™¨æ‰§è¡Œæºæ–‡ä»¶ã€‚\n\n![](Python_Encode/3_encoding_settings.png)\n\n\n\n## 2.1 æºæ–‡ä»¶ç¼–è¾‘\n\næˆ‘ä»¬ä½¿ç”¨çš„ç¼–è¾‘å™¨éƒ½æ˜¯æœ‰æ–‡ä»¶ç¼–ç è®¾ç½®çš„ï¼Œå¦‚Windowsç³»ç»Ÿè‡ªå¸¦çš„è®°äº‹æœ¬åœ¨å›½å†…é»˜è®¤æ˜¯GBKï¼ŒVimé€šè¿‡é…ç½®æ–‡ä»¶è®¾ç½®ä¸ºutf-8ã€‚ç£ç›˜ä¸Šçš„æ–‡ä»¶éƒ½æ˜¯ä»¥äºŒè¿›åˆ¶æ ¼å¼å­˜æ”¾çš„ï¼Œå…¶ä¸­æ–‡æœ¬æ–‡ä»¶éƒ½æ˜¯ä»¥æŸç§ç‰¹å®šç¼–ç (å¦‚GBK)çš„å­—èŠ‚å½¢å¼å­˜æ”¾çš„ã€‚å½“ä½¿ç”¨ç¼–è¾‘å™¨æ‰“å¼€æ–‡æœ¬æ–‡ä»¶Src.pyæ—¶ï¼Œç¼–è¾‘å™¨é¦–å…ˆåœ¨ç£ç›˜ä¸Šè¯»å–æ–‡ä»¶çš„äºŒè¿›åˆ¶å­—èŠ‚æµï¼Œç„¶åç¼–è¾‘å™¨ä¼šæŒ‰ç…§å…¶è®¾ç½®çš„ç¼–ç æ–¹å¼å°†å­—èŠ‚æµè§£ç (Decode)æˆä¸ºæˆ‘ä»¬èƒ½å¤Ÿè®¤è¯†çš„Unicodeå­—ç¬¦ä¸²ã€‚æ³¨æ„ï¼Œå½“æ‰“å¼€æŸä¸ªç‰¹å®šæ–‡ä»¶æ—¶ï¼Œç¼–è¾‘å™¨è®¾ç½®çš„ç¼–ç æ–¹å¼è¦å’Œæ–‡ä»¶å®é™…ä½¿ç”¨çš„ç¼–ç ä¸€æ ·ï¼Œå³ï¼Œå¦‚æœæ–‡ä»¶ä½¿ç”¨çš„æ˜¯GBKç¼–ç ï¼Œæ–‡æœ¬ç¼–è¾‘å™¨è®¾ç½®çš„ç¼–ç æ–¹å¼ä¸ºGBKæ‰èƒ½æ­£ç¡®æ‰“å¼€è¿™ä¸ªæ–‡ä»¶ï¼ˆå½“æˆ‘ä»¬ä½¿ç”¨è®°äº‹æœ¬ã€Notepad++æ—¶ï¼Œè¿™äº›è½¯ä»¶å¯ä»¥è‡ªåŠ¨æ£€æŸ¥æ–‡ä»¶çš„ç¼–ç æ–¹å¼ï¼Œç„¶åä»¥è¿™ä¸ªç¼–ç æ‰“å¼€æ–‡ä»¶ï¼‰ã€‚\n\nå½“æ–‡æœ¬ç¼–è¾‘å®Œæˆä»¥åï¼Œéœ€è¦ä¿å­˜æ–‡ä»¶ï¼Œç¼–è¾‘å™¨é¦–å…ˆå°†å…¶æ“ä½œçš„Unicodeå­—ç¬¦ä¸²è½¬æ¢ä¸ºäºŒè¿›åˆ¶çš„å­—èŠ‚æµ(Encode)ï¼Œç„¶åå†™å…¥ç£ç›˜ã€‚\n\n## 2.2 Pythonè§£é‡Šå™¨è¯»å–æºæ–‡ä»¶\n\nå½“è°ƒç”¨python Src.pyæ‰§è¡ŒåŸæ–‡ä»¶çš„æ—¶å€™ï¼ŒPythonè§£é‡Šå™¨é¦–å…ˆéœ€è¦è¯»å–è¿™ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œä½†æ˜¯å®ƒä¸çŸ¥é“è¿™ä¸ªSrc.pyæ˜¯ä»¥ä»€ä¹ˆç¼–ç æ–¹å¼å­˜å‚¨çš„ã€‚å¦‚æœä¸å‘Šè¯‰å®ƒï¼Œä»–å°±ä¼šä»¥ASCIIç çš„æ¥è§£ç (Decode)è¿™ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæ‰€ä»¥å¦‚æœSrc.pyæ–‡ä»¶ä¸­åªæœ‰ASCIIç å­—ç¬¦ï¼Œä¸€èˆ¬æ˜¯èƒ½å¤Ÿæ­£ç¡®è§£ç çš„ã€‚ä½†æ˜¯å¦‚æœè¿™ä¸ªæ–‡ä»¶ä¸­åŒ…å«æ±‰å­—ï¼Œè¿™ä¸ªè§£ç è¿‡ç¨‹å°±ä¼šå¤±è´¥ï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰pythonè§£é‡Šå™¨ï¼Œè¿™ä¸ªæ–‡ä»¶ä½¿ç”¨çš„æ˜¯ä½•ç§ç¼–ç ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦åœ¨æºæ–‡ä»¶ä¸­å†™å…¥ å¦‚ä¸‹å­—ç¬¦ä¸²çš„åŸå› ã€‚\n\n```python\n# coding:utf-8\n```\n\nè¿™è¡Œä»£ç å‘Šè¯‰è§£é‡Šå™¨ï¼Œè¯¥æ–‡ä»¶æ˜¯ä»¥utf-8ç¼–ç çš„ã€‚note: æ­¤å¤„è®¾ç½®çš„ç¼–ç è¦ä¸æºæ–‡ä»¶å®é™…ä½¿ç”¨çš„ç¼–ç æ–¹å¼ä¸€è‡´ã€‚\n\nå…³äºæºæ–‡ä»¶ä¸­å£°æ˜å­—ç¬¦ä¸²çš„ä¸¤ç§æ–¹å¼å¦‚ä¸‹ï¼š\n\n```python\n# coding:utf-8\nu = u'å¤ªé˜³'\nprint 'type(u):', type(u)  # type(u): <type 'unicode'>\nprint 'repr(u):', repr(u)  # repr(u): u'\\u592a\\u9633'\nstr_byte = 'å¤ªé˜³'\nprint 'type(str_byte):', type(str_byte)  # type(str_byte): <type 'str'>\nstr_u = str_byte.decode('utf-8')\nprint 'type(str_u):', type(str_u)  # type(str_u): <type 'unicode'>\nprint 'repr(str_u):', repr(str_u)  # repr(str_u): u'\\u592a\\u9633'\nstr_u = str_byte.decode('gbk')  # è§£ç å¤±è´¥ UnicodeDecodeError\n```\n\nåœ¨æºæ–‡ä»¶è¯»å–é˜¶æ®µï¼Œè§£é‡Šå™¨éƒ½ä¼šä»¥utf-8è§£ç  u'å¤ªé˜³' å’Œ 'å¤ªé˜³'\n\nåœ¨ç¨‹åºæ‰§è¡Œé˜¶æ®µï¼Œ \n\n- u'å¤ªé˜³'  ä¼šè½¬åŒ–ä¸ºunicodeå¯¹è±¡ï¼Œ uå¼€å¤´çš„å­—ç¬¦ä¸²ä¼š å£°æ˜è¿™ä¸ªå­—ç¬¦ä¸²æ˜¯ unicodeå¯¹è±¡ï¼›\n- 'å¤ªé˜³'  ä¼šè½¬åŒ–ä¸º utf-8ç¼–ç çš„å­—èŠ‚ä¸²ï¼ˆstrå¯¹è±¡ï¼‰ï¼Œæ­¤ç§å®šä¹‰æ–¹æ³•å®é™…ä¸Šå£°æ˜å‘½äº†ä¸€ä¸ªstrå¯¹è±¡ã€‚ä¸ºä»€ä¹ˆæ˜¯utf-8ï¼Œæ˜¯  â€œ# coding:utf-8â€  å£°æ˜çš„ï¼Œæ‰€ä»¥å¯ä»¥ç”¨utf-8è§£ç è¯¥å­—èŠ‚ä¸²ï¼Œè€Œç”¨gbkè§£ç åˆ™å¤±è´¥äº†ã€‚\n\n## 2.3 Pythonç¨‹åºçš„æ‰§è¡Œ\n\næˆ‘å¸¸ä½¿ç”¨çš„æ˜¯python2ï¼Œä½†æ˜¯ç”±äºptyhon2äº§ç”Ÿçš„æ—¶é—´æ¯”è¾ƒæ—©ï¼Œæ‰€ä»¥è®¾è®¡ä¸Šå‡ºç°äº†å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚ä»¥ä¸‹å†…å®¹åªé’ˆå¯¹Python 2.x ï¼ŒPython 3.x æ²¡æœ‰ä»¥ä¸‹é—®é¢˜ã€‚\n\n### 2.3.1 å­—ç¬¦ä¸²ä¸å­—èŠ‚ä¸²\n\nä¸Šé¢å·²ç»è®²è¿‡ï¼Œåœ¨è®¡ç®—æœºå†…å­˜ä¸­ï¼Œå­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦éƒ½æ˜¯Unicodeç¼–ç çš„ã€‚ä½†æ˜¯åœ¨pythonåˆšå‡ºç°æ—¶ï¼Œè¿˜æ²¡æœ‰Unicodeç¼–ç ï¼Œå®ƒå¤„ç†çš„éƒ½æ˜¯ASCIIç è¿™æ ·çš„å•å­—èŠ‚å­—ç¬¦ï¼Œæ‰€ä»¥å®ƒå°†å•å­—çš„åºåˆ—å®šä¹‰ä¸ºå­—ç¬¦ä¸²ï¼Œå³ ç±»strã€‚ä»¥ç°åœ¨çš„çœ¼å…‰æ¥çœ‹ï¼Œç±»strçš„å¯¹è±¡å®é™…ä¸Šæ˜¯å­—èŠ‚ä¸²ï¼Œè€Œä¸æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„å­—ç¬¦ä¸²ã€‚è¿˜å¥½ï¼Œåæ¥Pythonåˆæ·»åŠ äº†unicodeç±»ï¼Œè¿™æ‰æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„å­—ç¬¦ä¸²ã€‚\n\nç±»strå¯¹è±¡æ˜¯é™¤unicodeä»¥å¤–çš„æ‰€æœ‰ç¼–ç (å¦‚GBK,UTF-8)çš„å­—ç¬¦ä¸²å¯¹åº”çš„å­—èŠ‚ä¸²\n\nnoteï¼šåœ¨pythonä¸­ç±»strå’Œç±»unicodeéƒ½æ˜¯basestringçš„å­ç±»ã€‚\n\nåŒæ—¶ï¼Œç”±äºä»¥ä¸Šæ‰€è¿°åŸå› ï¼Œpython2çš„é»˜è®¤ç¼–ç æ˜¯ASCIIç ï¼Œå¯¹äºä¸€ä¸ªç»™å®šçš„strå¯¹è±¡ï¼Œå®ƒå°±é»˜è®¤è¿™ä¸ªå¯¹è±¡æ¯ä¸ªå­—èŠ‚éƒ½ä»£è¡¨ä¸€ä¸ªASCIIç å­—ç¬¦ã€‚\n\næ‰€ä»¥å¯¹äºéASCIIç å­—ç¬¦ä¸²ï¼Œå¦‚æœæƒ³å¾—åˆ°å­—ç¬¦ä¸²çš„é•¿åº¦ï¼Œéœ€è¦åœ¨unicodeå¯¹è±¡ä¸Šä½¿ç”¨lenå‡½æ•°ï¼Œåœ¨strå¯¹è±¡ä¸Šä½¿ç”¨lenå‡½æ•°ä¼šå¾—åˆ°å…¶ç¼–ç çš„å­—èŠ‚é•¿åº¦ï¼Œè€Œä¸æ˜¯å­—ç¬¦é•¿åº¦ã€‚\n\n```python\n# coding:utf-8\n\nu = u'å¤ªé˜³'\nprint 'type(u):', type(u)  # len(str_byte): 6\nprint 'repr(u):', repr(u)  # repr(u): u'\\u592a\\u9633'\nprint 'len(u):', len(u)  # len(u): 2\nstr_byte = 'å¤ªé˜³'\nprint 'type(str_byte):', type(str_byte)  # type(str_byte): <type 'str'>\nprint 'repr(str_byte):', repr(str_byte)  # repr(str_byte): '\\xe5\\xa4\\xaa\\xe9\\x98\\xb3'\nprint 'len(str_byte):', len(str_byte)  # len(str_byte): 6\n```\n\n### 2.3.2 å­—èŠ‚ä¸²ä¹‹é—´çš„è½¬æ¢\n\nåœ¨pythonç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­æœ‰ä¸€ä¸ªstrå¯¹è±¡(å­—èŠ‚ä¸²) str_utf8 ï¼Œå®ƒæ˜¯ä»¥UTF-8ç¼–ç çš„å­—èŠ‚ä¸²ï¼Œå¦‚æœæˆ‘æƒ³æŠŠå®ƒè½¬æ¢ä¸ºä»¥GBKç¼–ç çš„å­—èŠ‚ä¸²str_gbkåº”è¯¥æ€ä¹ˆåŠå‘¢ã€‚æ ¹æ®ä¸Šé¢æè¿°é¦–å…ˆéœ€è¦å°†str_utf8è§£ç (decode)ä¸ºunicodeç¼–ç çš„å­—ç¬¦ä¸²uï¼Œç„¶åå°†å…¶ç¼–ç (encode)ä¸ºGBKç¼–ç çš„å­—èŠ‚ä¸²ã€‚\n\n> UTF-8=>Unicode=>GBK\n\n```python\n# coding:utf-8\n\nu = u'å¤ªé˜³'\nstr_utf8 = u.encode('utf-8')  \nprint repr(str_utf8)    # '\\xe5\\xa4\\xaa\\xe9\\x98\\xb3'\nstr_unicode = str_utf8.decode('utf-8')\nstr_gbk1 = str_unicode.encode('gbk')\nprint repr(str_gbk1)   # '\\xcc\\xab\\xd1\\xf4'\n\nstr_gbk2 =  str_utf8.decode('utf-8').encode('gbk')  #ä¹Ÿå¯ä»¥è¿™æ ·\nprint repr(str_gbk2)  # '\\xcc\\xab\\xd1\\xf4'\n```\n\nä½†æ˜¯æœ‰æ—¶å€™å¯ä»¥ç›´æ¥åšåˆ°ä»UTF-8åˆ°GBKçš„è½¬åŒ–ï¼š\n\n```python\n# coding:utf-8\n\nimport sys\nprint sys.getdefaultencoding() # ascii\nreload(sys)\nsys.setdefaultencoding('utf-8') # å°†pythonè§£é‡Šå™¨çš„é»˜è®¤è§£ç è®¾ç½®ä¸ºutf-8\n\nu = u'å¤ªé˜³'\nstr_utf8 = u.encode('utf-8')  \nprint repr(str_utf8)    # '\\xe5\\xa4\\xaa\\xe9\\x98\\xb3'\nstr_gbk3 =  str_utf8.encode('gbk')  #ä¹Ÿå¯ä»¥è¿™æ ·\nprint repr(str_gbk3)  # '\\xcc\\xab\\xd1\\xf4'\n\n```\n\nä»ä»£ç ä¸Šçœ‹ï¼Œutf-8çš„å­—èŠ‚ä¸²ç›´æ¥è½¬åŒ–ä¸ºäº†gbkç¼–ç çš„å­—èŠ‚ä¸²ã€‚å®é™…ä¸Šçš„è½¬åŒ–è¿‡ç¨‹è¿˜æ˜¯\n\n> UTF-8=>Unicode=>GBK \n\nå¯¹äºè¿™ç§ä»£ç ï¼Œpythonè§£é‡Šå™¨çš„åšæ³•å®é™…ä¸Šæ˜¯ \n\n```\nstr_gbk3 =  str_utf8.decode(defaultencoding).encode('gbk') \n```\n\nè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å®šä¹‰defaultencodingçš„çš„åŸå› ã€‚\n\næ­¤å¤„éœ€è¦æ³¨æ„ï¼špythonä¸­é»˜è®¤çš„defaultencoding æ˜¯asciiã€‚\n\n### 2.3.4 pythonè¯»å†™æ–‡ä»¶\n\nè¯»å†™çš„æ–‡ä»¶å†…å®¹å¯ä»¥å½“åšä¸€ä¸ªpythonç¨‹åºå˜é‡æ¥å¤„ç†ã€‚\n\nå†…ç½®çš„open()æ–¹æ³•æ‰“å¼€æ–‡ä»¶æ—¶ï¼Œread()è¯»å–çš„æ˜¯å­—èŠ‚æµ(str)ï¼Œè¯»å–åéœ€è¦ä½¿ç”¨æ­£ç¡®çš„ç¼–ç æ ¼å¼(ç”±å®é™…è¯»å–çš„æ–‡ä»¶å†³å®š)è¿›è¡Œè§£ç (decode)ã€‚write()å†™å…¥æ—¶ï¼Œå¦‚æœå‚æ•°æ˜¯unicodeï¼Œåˆ™éœ€è¦ä½¿ç”¨ä½ å¸Œæœ›å†™å…¥çš„ç¼–ç è¿›è¡Œç¼–ç (encode)ï¼Œå¦‚æœæ˜¯å…¶ä»–ç¼–ç æ ¼å¼çš„å­—èŠ‚ä¸²(str)ï¼Œåˆ™éœ€è¦å…ˆç”¨è¯¥strçš„ç¼–ç è¿›è¡Œdecodeï¼Œè½¬æˆunicodeåå†ä½¿ç”¨å†™å…¥çš„ç¼–ç è¿›è¡Œencodeã€‚å¦‚æœç›´æ¥å°†unicodeä½œä¸ºå‚æ•°ä¼ å…¥write()æ–¹æ³•ï¼ŒPythonå°†å…ˆä½¿ç”¨æºä»£ç æ–‡ä»¶å£°æ˜çš„å­—ç¬¦ç¼–ç è¿›è¡Œç¼–ç ç„¶åå†™å…¥ã€‚\n\n```python\n# coding: UTF-8 \nf = open('test.txt')  # å·²çŸ¥æ˜¯GBKç¼–ç \ns = f.read()\nf.close()\nprint type(s)         # <type 'str'>\nu = s.decode('GBK')   # è§£ç æˆunicode \nf = open('test2.txt', 'w')   # å·²çŸ¥æ˜¯UTF-8ç¼–ç \ns = u.encode('UTF-8')   # ç¼–ç ä¸ºUTF-8\nf.write(s)\nf.close()\n```\n\n\n\n# 3 å­—ç¬¦ç¼–ç ç®€ä»‹\n\nåœ¨ç¬¬ä¸€èŠ‚å·²ç»å¯¹å­—ç¬¦ç¼–ç è¿›è¡Œäº†ä¸€ä¸ªç®€å•çš„æ¦‚æ‹¬ï¼Œæœ¬èŠ‚è¿›è¡Œç¨å¾®è¯¦ç»†çš„è¯´æ˜ã€‚æœ¬èŠ‚å†…å®¹ä¸»è¦å‚è€ƒè‡ª[Pythonå­—ç¬¦ç¼–ç è¯¦è§£](http://www.cnblogs.com/huxi/archive/2010/12/05/1897271.html)\n\nå­—ç¬¦ç¼–ç çš„ä½œç”¨å°±æ˜¯å°†å­—ç¬¦ä¸è®¡ç®—æœºçš„æ•°å€¼ç›¸å¯¹åº”ã€‚å¦‚å­—ç¬¦\"A\"å¯¹åº”65(\\x41)ï¼Œå­—ç¬¦\"1\"å¯¹åº”49(\\x31)ã€‚å­—ç¬¦çš„æ•°é‡å’Œç¼–ç çš„é•¿åº¦æ˜¯ç›¸å…³çš„ï¼Œå¦‚ä¸­å›½äººä½¿ç”¨çš„æ±‰å­—æœ‰å‡ ä¸‡ä¸ªï¼Œè€Œç¾å›½äººä½¿ç”¨çš„è‹±æ–‡åŠ ä¸Šç‰¹æ®Šç¬¦å·ä¹Ÿå°±é‚£ä¹ˆå‡ åä¸ªã€‚æ­£å¼ç”±äºä¸åŒç¯å¢ƒä¸­å­—ç¬¦å’Œå­—ç¬¦æ•°é‡çš„ä¸åŒæ‰äº§ç”Ÿäº†å„ç§ä»¤äººå¤´ç–¼çš„ç¼–ç ã€‚\n\n## 3.1 ASCII\n\nASCII(American Standard Code for Information Interchange)ï¼Œæ˜¯ä¸€ç§å•å­—èŠ‚çš„ç¼–ç ã€‚è®¡ç®—æœºä¸–ç•Œé‡Œä¸€å¼€å§‹åªæœ‰è‹±æ–‡ï¼Œè€Œå•å­—èŠ‚å¯ä»¥è¡¨ç¤º256ä¸ªä¸åŒçš„å­—ç¬¦ï¼Œå¯ä»¥è¡¨ç¤ºæ‰€æœ‰çš„è‹±æ–‡å­—ç¬¦å’Œè®¸å¤šçš„æ§åˆ¶ ç¬¦å·ã€‚ä¸è¿‡ASCIIåªç”¨åˆ°äº†å…¶ä¸­çš„ä¸€åŠï¼ˆ\\x80ä»¥ä¸‹ï¼‰ï¼Œå…±128ä¸ªå­—ç¬¦ï¼Œè¿™ä¹Ÿæ˜¯MBCSå¾—ä»¥å®ç°çš„åŸºç¡€ã€‚\n\n## 3.2 MBCS\n\nç„¶è€Œè®¡ç®—æœºä¸–ç•Œé‡Œå¾ˆå¿«å°±æœ‰äº†å…¶ä»–è¯­è¨€ï¼Œå•å­—èŠ‚çš„ASCIIå·²æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚åæ¥æ¯ä¸ªè¯­è¨€å°±åˆ¶å®šäº†ä¸€å¥—è‡ªå·±çš„ç¼–ç ï¼Œç”±äºå•å­—èŠ‚èƒ½è¡¨ç¤ºçš„å­—ç¬¦å¤ªå°‘ï¼Œè€Œä¸”åŒæ—¶ä¹Ÿéœ€è¦ä¸ASCIIç¼–ç ä¿æŒå…¼å®¹ï¼Œæ‰€ä»¥è¿™äº›ç¼–ç çº·çº·ä½¿ç”¨äº†å¤šå­—èŠ‚æ¥è¡¨ç¤ºå­—ç¬¦ï¼Œå¦‚GBKã€ã€GB2312ã€BIG5ç­‰ï¼Œä»–ä»¬çš„è§„åˆ™æ˜¯ï¼Œå¦‚æœç¬¬ä¸€ä¸ªå­—èŠ‚æ˜¯\\x80ä»¥ä¸‹ï¼Œåˆ™ä»ç„¶è¡¨ç¤ºASCIIå­—ç¬¦ï¼›è€Œå¦‚æœæ˜¯\\x80ä»¥ä¸Šï¼Œåˆ™è·Ÿä¸‹ä¸€ä¸ªå­—èŠ‚ä¸€èµ·ï¼ˆå…±ä¸¤ä¸ªå­—èŠ‚ï¼‰è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ã€‚\n\nMBCS(Multi-Byte Character Set)æ˜¯è¿™äº›ç¼–ç çš„ç»Ÿç§°ã€‚ç›®å‰ä¸ºæ­¢å¤§å®¶éƒ½æ˜¯ç”¨åŒå­—èŠ‚ï¼Œæ‰€ä»¥æœ‰æ—¶å€™ä¹Ÿå«åšDBCS(Double-Byte Character Set)ã€‚å¿…é¡»æ˜ç¡®çš„æ˜¯ï¼ŒMBCSå¹¶ä¸æ˜¯æŸä¸€ç§ç‰¹å®šçš„ç¼–ç ï¼ŒWindowsé‡Œæ ¹æ®ä½ è®¾å®šçš„åŒºåŸŸä¸åŒï¼ŒMBCSæŒ‡ä»£ä¸åŒçš„ç¼–ç ï¼Œè€ŒLinuxé‡Œæ— æ³•ä½¿ç”¨ MBCSä½œä¸ºç¼–ç ã€‚åœ¨Windowsä¸­ä½ çœ‹ä¸åˆ°MBCSè¿™å‡ ä¸ªå­—ç¬¦ï¼Œå› ä¸ºå¾®è½¯ä¸ºäº†æ›´åŠ æ´‹æ°”ï¼Œä½¿ç”¨äº†ANSIæ¥å“å”¬äººï¼Œè®°äº‹æœ¬çš„å¦å­˜ä¸ºå¯¹è¯æ¡†é‡Œç¼–ç ANSIå°±æ˜¯MBCSã€‚åŒæ—¶ï¼Œåœ¨ç®€ä½“ä¸­æ–‡Windowsé»˜è®¤çš„åŒºåŸŸè®¾å®šé‡Œï¼ŒæŒ‡ä»£GBKã€‚\n\n## 3.3 Unicode\n\nåæ¥ï¼Œæœ‰äººå¼€å§‹è§‰å¾—å¤ªå¤šç¼–ç å¯¼è‡´ä¸–ç•Œå˜å¾—è¿‡äºå¤æ‚äº†ï¼Œäºæ˜¯å¤§å®¶ååœ¨ä¸€èµ·æ‹è„‘è¢‹æƒ³å‡ºæ¥ä¸€ä¸ªæ–¹æ³•ï¼šæ‰€æœ‰è¯­è¨€çš„å­—ç¬¦éƒ½ç”¨åŒä¸€ç§å­—ç¬¦é›†æ¥è¡¨ç¤ºï¼Œè¿™å°±æ˜¯Unicodeã€‚\n\næœ€åˆçš„Unicodeæ ‡å‡†UCS-2ä½¿ç”¨ä¸¤ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ï¼Œæ‰€ä»¥ä½ å¸¸å¸¸å¯ä»¥å¬åˆ°Unicodeä½¿ç”¨ä¸¤ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦çš„è¯´æ³•ã€‚ä½†è¿‡äº†ä¸ä¹…æœ‰äººè§‰å¾—256$\\times$256å¤ªå°‘äº†ï¼Œè¿˜æ˜¯ä¸å¤Ÿç”¨ï¼Œäºæ˜¯å‡ºç°äº†UCS-4æ ‡å‡†ï¼Œå®ƒä½¿ç”¨4ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ï¼Œä¸è¿‡æˆ‘ä»¬ç”¨çš„æœ€å¤šçš„ä»ç„¶æ˜¯UCS-2ã€‚\n\n**æ­¤å¤„éœ€è¦æ³¨æ„ï¼ŒASIIå’ŒMBCSå³ä¿å­˜äº†å­—ç¬¦å’Œç ä½çš„å¯¹åº”å…³ç³»ï¼Œä¼ è¾“å’Œå­˜å‚¨çš„æ—¶å€™ä¹Ÿæ˜¯ä½¿ç”¨äº†å­—ç¬¦å¯¹åº”çš„ç ä½ã€‚ä¸ASIIå’ŒMBCSä¸åŒï¼ŒUCS(Unicode Character Set)è¿˜ä»…ä»…æ˜¯å­—ç¬¦å¯¹åº”ç ä½çš„ä¸€å¼ è¡¨è€Œå·²ï¼Œæ¯”å¦‚\"æ±‰\"è¿™ä¸ªå­—çš„ç ä½æ˜¯6C49ã€‚å­—ç¬¦å…·ä½“å¦‚ä½•ä¼ è¾“å’Œå‚¨å­˜åˆ™æ˜¯ç”±UTF(UCS Transformation Format)æ¥è´Ÿè´£ã€‚**\n\n**ç°ä»£æ“ä½œç³»ç»Ÿå’Œå¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€éƒ½ç›´æ¥æ”¯æŒUnicodeã€‚åœ¨è®¡ç®—æœºå†…å­˜ä¸­ï¼Œç»Ÿä¸€ä½¿ç”¨Unicodeç¼–ç ï¼Œå½“éœ€è¦ä¿å­˜åˆ°ç¡¬ç›˜æˆ–è€…éœ€è¦ä¼ è¾“çš„æ—¶å€™ï¼Œå°±è½¬æ¢ä¸ºUTF-8ç¼–ç ã€‚**\n\nä¸€å¼€å§‹è¿™äº‹å¾ˆç®€å•ï¼Œç›´æ¥ä½¿ç”¨UCSçš„ç ä½æ¥ä¿å­˜ï¼Œè¿™å°±æ˜¯UTF-16ï¼Œæ¯”å¦‚ï¼Œ\"æ±‰\"ç›´æ¥ä½¿ç”¨\\x6C\\x49ä¿å­˜(UTF-16-BE)ï¼Œæˆ–æ˜¯å€’è¿‡æ¥ä½¿ç”¨\\x49\\x6Cä¿å­˜(UTF-16-LE)ã€‚ä½†ç”¨ç€ç”¨ç€ç¾å›½äººè§‰å¾—è‡ªå·±åƒäº†å¤§äºï¼Œä»¥å‰è‹±æ–‡å­—æ¯åªéœ€è¦ä¸€ä¸ªå­—èŠ‚å°±èƒ½ä¿å­˜äº†ï¼Œç°åœ¨å¤§é”…é¥­ä¸€åƒå˜æˆäº†ä¸¤ä¸ªå­—èŠ‚ï¼Œç©ºé—´æ¶ˆè€—å¤§äº†ä¸€å€â€¦â€¦äºæ˜¯UTF-8æ¨ªç©ºå‡ºä¸–ã€‚\n\nUTF-8ç¼–ç æŠŠä¸€ä¸ªUnicodeå­—ç¬¦æ ¹æ®ä¸åŒçš„æ•°å­—å¤§å°ç¼–ç æˆ1-6ä¸ªå­—èŠ‚ï¼ŒASCIIå­—ç¬¦ä½¿ç”¨1å­—èŠ‚è¡¨ç¤ºï¼Œæ±‰å­—é€šå¸¸æ˜¯3ä¸ªå­—èŠ‚ï¼Œåªæœ‰å¾ˆç”Ÿåƒ»çš„å­—ç¬¦æ‰ä¼šè¢«ç¼–ç æˆ4-6ä¸ªå­—èŠ‚ã€‚\n\nå¦å¤–å€¼å¾—ä¸€æçš„æ˜¯BOM(Byte Order Mark)ã€‚æˆ‘ä»¬åœ¨å‚¨å­˜æ–‡ä»¶æ—¶ï¼Œæ–‡ä»¶ä½¿ç”¨çš„ç¼–ç å¹¶æ²¡æœ‰ä¿å­˜ï¼Œæ‰“å¼€æ—¶åˆ™éœ€è¦æˆ‘ä»¬è®°ä½åŸå…ˆä¿å­˜æ—¶ä½¿ç”¨çš„ç¼–ç å¹¶ä½¿ç”¨è¿™ä¸ªç¼–ç æ‰“å¼€ï¼Œè¿™æ ·ä¸€æ¥å°±äº§ç”Ÿäº†è®¸å¤šéº»çƒ¦ã€‚è€ŒUTFåˆ™å¼•å…¥äº†BOMæ¥è¡¨ç¤ºè‡ªèº«ç¼–ç ï¼Œå¦‚æœä¸€å¼€å§‹è¯»å…¥çš„å‡ ä¸ªå­—èŠ‚æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œåˆ™ä»£è¡¨æ¥ä¸‹æ¥è¦è¯»å–çš„æ–‡å­—ä½¿ç”¨çš„ç¼–ç æ˜¯ç›¸åº”çš„ç¼–ç ï¼š\n\n> BOM_UTF8 '\\xef\\xbb\\xbf' \n> BOM_UTF16_LE '\\xff\\xfe' \n> BOM_UTF16_BE '\\xfe\\xff'\n\nå¹¶ä¸æ˜¯æ‰€æœ‰çš„ç¼–è¾‘å™¨éƒ½ä¼šå†™å…¥BOMï¼Œä½†å³ä½¿æ²¡æœ‰BOMï¼ŒUnicodeè¿˜æ˜¯å¯ä»¥è¯»å–çš„ï¼Œåªæ˜¯åƒMBCSçš„ç¼–ç ä¸€æ ·ï¼Œéœ€è¦å¦è¡ŒæŒ‡å®šå…·ä½“çš„ç¼–ç ï¼Œå¦åˆ™è§£ç å°†ä¼šå¤±è´¥ã€‚\n\nä½ å¯èƒ½å¬è¯´è¿‡UTF-8ä¸éœ€è¦BOMï¼Œè¿™ç§è¯´æ³•æ˜¯ä¸å¯¹çš„ï¼Œåªæ˜¯ç»å¤§å¤šæ•°ç¼–è¾‘å™¨åœ¨æ²¡æœ‰BOMæ—¶éƒ½æ˜¯ä»¥UTF-8ä½œä¸ºé»˜è®¤ç¼–ç è¯»å–ã€‚å³ä½¿æ˜¯ä¿å­˜æ—¶é»˜è®¤ä½¿ ç”¨ANSI(MBCS)çš„è®°äº‹æœ¬ï¼Œåœ¨è¯»å–æ–‡ä»¶æ—¶ä¹Ÿæ˜¯å…ˆä½¿ç”¨UTF-8æµ‹è¯•ç¼–ç ï¼Œå¦‚æœå¯ä»¥æˆåŠŸè§£ç ï¼Œåˆ™ä½¿ç”¨UTF-8è§£ç ã€‚è®°äº‹æœ¬è¿™ä¸ªåˆ«æ‰­çš„åšæ³•é€ æˆäº†ä¸€ä¸ª BUGï¼šå¦‚æœä½ æ–°å»ºæ–‡æœ¬æ–‡ä»¶å¹¶è¾“å…¥\"å§¹å¡§\"ç„¶åä½¿ç”¨ANSI(MBCS)ä¿å­˜ï¼Œå†æ‰“å¼€å°±ä¼šå˜æˆ\"æ±‰a\"ï¼Œä½ ä¸å¦¨è¯•è¯• ï¼šï¼‰\n\n\n\n\n# å‚è€ƒèµ„æ–™\n\n[Pythonå­—ç¬¦ç¼–ç è¯¦è§£](http://www.cnblogs.com/huxi/archive/2010/12/05/1897271.html)\n\n[Pythonä¸­çš„å­—ç¬¦ä¸²ä¸å­—ç¬¦ç¼–ç ](http://www.cnblogs.com/yyds/p/6171340.html)\n\n[pythonå­—ç¬¦ä¸²ç¼–ç åŠä¹±ç è§£å†³æ–¹æ¡ˆ](http://blog.csdn.net/pipisorry/article/details/44136297)\n","tags":["Python"],"categories":["Python"]},{"title":"GloVe","url":"%2Fblog%2FGloVe.html","content":"# 1. æ¦‚è¿°\n\nåšè‡ªç„¶è¯­è¨€å¤„ç†çš„æ—¶å€™å¾ˆå¤šæ—¶å€™ä¼šç”¨çš„Word Embeddingï¼Œç›®å‰å¸¸ç”¨çš„æ–¹æ³•æ˜¯word2vecç®—æ³•è®­ç»ƒè¯å‘é‡ã€‚ä¸è¿‡è®­ç»ƒè¯å‘é‡çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œä»Šå¤©ä»‹ç»GloVeç®—æ³•ã€‚\n\nGloVeï¼šGlobal Vectorsã€‚\n\næ¨¡å‹è¾“å…¥ï¼šè¯­æ–™åº“ corpus\n\næ¨¡å‹è¾“å‡ºï¼šæ¯ä¸ªè¯çš„è¡¨ç¤ºå‘é‡\n\n# 2. åŸºæœ¬æ€æƒ³\n\n## 2.1. èƒŒæ™¯çŸ¥è¯†\n\nè¦è®²GloVeæ¨¡å‹çš„æ€æƒ³æ–¹æ³•ï¼Œæˆ‘ä»¬å…ˆä»‹ç»ä¸¤ä¸ªå…¶ä»–æ–¹æ³•ï¼š\n\nä¸€ä¸ªæ˜¯åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„[LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis)ç®—æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯topic modelçš„ä¸€ç§ï¼Œå¯¹word-documentçŸ©é˜µï¼ˆçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ ä¸ºtf-idfï¼‰è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼Œä»è€Œå¾—åˆ°termçš„å‘é‡è¡¨ç¤ºå’Œdocumentçš„å‘é‡è¡¨ç¤ºã€‚\n\nå½“ä½¿ç”¨LSAè®­ç»ƒè¯å‘é‡çš„æ—¶å€™ï¼Œéœ€è¦å°†word-documentçŸ©é˜µæ¢æˆword-contextçŸ©é˜µ$M_{nm}$ï¼Œè¯¥çŸ©é˜µçš„è¡Œæ˜¯wordï¼Œåˆ—æ˜¯wordçš„ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªwordä¸å…¶ä¸Šä¸‹æ–‡çš„å…±ç°æ¬¡æ•°ï¼Œå³åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å…¨å±€ç»Ÿè®¡ç‰¹å¾ã€‚çŸ©é˜µä¸­å„å…ƒç´ çš„å€¼è¿˜æœ‰å…¶ä»–æ›´ä¼˜çš„å–å€¼æ–¹æ³•ï¼Œè¯·å‚è§[Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://www.transacl.org/ojs/index.php/tacl/article/view/570)çš„ç¬¬ä¸€äºŒéƒ¨åˆ†ã€‚\n\nå¯¹$M_{nm}$å¥‡å¼‚å€¼åˆ†è§£$M_{nm}=U_{nk}\\Sigma_{kk}V_{km}^T$,å–å…¶$\\Sigma$å‰$d$ä¸ªä¸»è¦å…ƒç´ ï¼Œåˆ™\n$$\nM_{nm} \\approx U_{nd}\\Sigma_{dd}V_{dm}^T\n$$\nå…¬å¼çš„å³è¾¹å¯ä»¥åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†\n$$\nW_{nd}=U_{nd}\\Sigma_{dd}\\\\\nC_{md}=V_{md}\n$$\nå…¶ä¸­$W_{nd}$çš„æ¯ä¸€è¡Œä¸ºä¸€ä¸ªwordçš„è¯å‘é‡ï¼Œ$C_{md}$çš„æ¯ä¸€è¡Œä¸ºä¸€ä¸ªcontextçš„å‘é‡ã€‚\n\nå…³äºLSAï¼Œåœ¨æ–¯å¦ç¦å¤§å­¦çš„è‡ªç„¶è¯­è¨€çš„è¯¾ç¨‹çš„ç¬¬ä¸‰æ¬¡è¯¾ç¨‹ä¸­æœ‰æ¶‰åŠ [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/syllabus.html)\n\nå¦ä¸€ä¸ªæ–¹æ³•æ˜¯[word2vec]((http://blog.csdn.net/itplus/article/details/37969519))ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥åˆ†ä¸ºskip-gram å’Œ continuous bag-of-wordsï¼ˆCBOWï¼‰ä¸¤ç±»,ä½†éƒ½æ˜¯åŸºäºå±€éƒ¨æ»‘åŠ¨çª—å£è®¡ç®—çš„ã€‚å³ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†å±€éƒ¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆlocal contextï¼‰\n\n> å…³äºword2vecï¼Œåœ¨æ–¯å¦ç¦å¤§å­¦çš„è‡ªç„¶è¯­è¨€çš„è¯¾ç¨‹çš„ç¬¬ä¸‰æ¬¡è¯¾ç¨‹ä¸­æœ‰æ¶‰åŠ [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/syllabus.html)\n>\n> - word2vec ä½¿ç”¨å›ºå®šçš„windowéå†æ•´ä¸ªè¯­æ–™åº“\n> - ä½¿ç”¨æ¯ä¸ªè¯(center word)çš„ä¸Šä¸‹æ–‡(surrounding words)é¢„æµ‹è¯¥è¿™ä¸ªè¯(center word)(CBOWæ¨¡å‹)\n> - word2vecé¢„æµ‹æ¯ä¸€ä¸ªè¯(center word)çš„æ—¶å€™éƒ½åˆ©ç”¨äº†ä¸Šä¸‹æ–‡ä¸­å•è¯ä¹‹é—´çš„å…±ç°å…³ç³»(noteï¼šæ²¡æœ‰ä½¿ç”¨ä¸¤ä¸ªè¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å…±ç°æ¬¡æ•°)\n\n\n\nLSAæ€»ç»“ï¼š\n\n- è®­ç»ƒé€Ÿåº¦å¿«\n- æœ‰æ•ˆåˆ©ç”¨äº†å…¨å±€ç»Ÿè®¡ç‰¹å¾\n- Primarily used to capture word similarity\n\nword2vecæ€»ç»“ï¼š\n\n- Scales with corpus size\n- æ²¡æœ‰ä½¿ç”¨å…¨å±€ç»Ÿè®¡ç‰¹å¾\n- Can capture complex patterns beyond word similarity \n\n## 2.2. GloVeæ¨¡å‹çš„æ€æƒ³\n\nLSAå’Œword2vecä½œä¸ºä¸¤å¤§ç±»æ–¹æ³•çš„ä»£è¡¨ï¼Œä¸€ä¸ªæ˜¯åˆ©ç”¨äº†å…¨å±€ç‰¹å¾çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ï¼Œä¸€ä¸ªæ˜¯åˆ©ç”¨å±€éƒ¨ä¸Šä¸‹æ–‡çš„æ–¹æ³•ã€‚GloVeå°†è¿™ä¸¤ç±»æ–¹æ³•çš„ä¼˜ç‚¹ç»“åˆåˆ°äº†ä¸€èµ·ï¼Œç‰¹ç‚¹å¦‚ä¸‹ï¼š\n\n> Fast training\n>\n> Scalable to huge corpora\n>\n> Good performance even with small corpus,\tand\tsmall vectors\n\nGloVeæ¨¡å‹å°±æ˜¯å°†è¿™ä¸¤ä¸­ç‰¹å¾åˆå¹¶åˆ°ä¸€èµ·çš„ï¼Œå³ä½¿ç”¨äº†è¯­æ–™åº“çš„å…¨å±€ç»Ÿè®¡ï¼ˆoverall statisticsï¼‰ç‰¹å¾ï¼Œä¹Ÿä½¿ç”¨äº†å±€éƒ¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå³æ»‘åŠ¨çª—å£ï¼‰ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹GloVeæ¨¡å‹å¼•å…¥äº†Co-occurrence Probabilities Matrixã€‚\n\né¦–å…ˆå¼•å…¥word-wordçš„å…±ç°çŸ©é˜µ$X$ï¼Œ\n$X$çš„å…ƒç´ $X_{ij}$æ˜¯è¯­æ–™åº“ä¸­å‡ºç°åœ¨word $i$ä¸Šä¸‹æ–‡ä¸­çš„word $j$ çš„æ¬¡æ•°ï¼›\n$X_i=\\sum_{k}{X_{ik}}$,æ˜¯å‡ºç°åœ¨word $i$ ä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰wordçš„æ€»æ¬¡æ•°ï¼›\n$P_{ij}=P\\left(j\\vert{i}\\right)=\\frac{X_{ij}}{X_i}$,æ˜¯word $j$å‡ºç°åœ¨word $i$ ä¸Šä¸‹æ–‡çš„æ¦‚ç‡ã€‚\n\nç”±ä»¥ä¸Šæ¦‚å¿µå¼•ç”³å‡ºå…±ç°æ¦‚ç‡çŸ©é˜µï¼ˆCo-occurrence Probabilities Matrixï¼‰ï¼Œä»¥ä¸‹ä¸ºè®ºæ–‡ä¸­çš„ä¾‹å­ï¼š\n\n-![](GloVe/Co-occurrence-probilities.png)\nè¯¥çŸ©é˜µçš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºiceå‡ºç°æ—¶solidå‡ºç°çš„æ¦‚ç‡ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºiceå‡ºç°æ—¶gaså‡ºç°çš„æ¦‚ç‡ï¼Œä»¥æ­¤ç±»æ¨ã€‚\n\nç”±Co-occurrence Probabilities Matrixå¯ä»¥çœ‹å‡º$Ratio=\\frac{P_{ik}}{P_{jk}}$çš„å–å€¼æ˜¯æœ‰ä¸€å®šçš„è§„å¾‹çš„ã€‚[æ–‡ç« ](http://blog.csdn.net/coderTC/article/details/73864097)å¯¹è¯¥è§„å¾‹è¿›è¡Œäº†æ€»ç»“ï¼š\n\n| **ratioi,j,kçš„å€¼** | **å•è¯j,kç›¸å…³** | **å•è¯j,kä¸ç›¸å…³** |\n| ---------------- | ----------- | ------------ |\n| **å•è¯i,kç›¸å…³**      | è¶‹è¿‘1         | å¾ˆå¤§           |\n| **å•è¯i,kä¸ç›¸å…³**     | å¾ˆå°          | è¶‹è¿‘1          |\n\nä¹Ÿå°±æ˜¯è¯´Ratioå€¼èƒ½å¤Ÿåæ˜ wordä¹‹é—´çš„ç›¸å…³æ€§ï¼Œ**è€ŒGloVeæ¨¡å‹å°±æ˜¯åˆ©ç”¨äº†è¿™ä¸ªRatioå€¼**ã€‚\n\n å†æ˜ç¡®ä¸€ä¸‹ï¼Œ**GloVeæ¨¡å‹çš„ç›®æ ‡å°±æ˜¯è·å–æ¯ä¸€ä¸ªwordçš„å‘é‡è¡¨ç¤ºv**ã€‚ä¸å¦¨å‡è®¾ç°åœ¨å·²ç»å¾—åˆ°äº†word $i, j, k$ çš„è¯å‘é‡$w_i, w_j, w_k$ã€‚**GloVeè®¤ä¸ºï¼Œè¿™ä¸‰ä¸ªå‘é‡é€šè¿‡æŸç§å‡½æ•°çš„ä½œç”¨åæ‰€å‘ˆç°å‡ºæ¥çš„è§„å¾‹å’Œ$Ratio=\\frac{P_{ik}}{P_{jk}}$å…·æœ‰ä¸€è‡´æ€§ï¼Œå³ç›¸ç­‰ï¼Œä¹Ÿå°±å¯ä»¥è®¤ä¸ºè¯å‘é‡ä¸­åŒ…å«äº†å…±ç°æ¦‚ç‡çŸ©é˜µä¸­çš„ä¿¡æ¯ã€‚**\n\nå‡è®¾è¿™ä¸ªæœªçŸ¥çš„å‡½æ•°æ˜¯$F$,åˆ™:\n\n$$F(w_i, w_j, w_k)=\\frac{P_{ik}}{P_{jk}}$$\n\n*æ­¤å¤„å¯ä»¥ç±»æ¯”word2vecçš„åŸºæœ¬æ€æƒ³ï¼ˆä»¥åŸºäºå“ˆå¼—æ›¼æ ‘çš„CBOWä¸ºä¾‹ï¼‰ï¼Œå‡è®¾word $i$ï¼Œå’Œå…¶context wordsçš„è¯å‘é‡å·²çŸ¥ï¼Œé€šè¿‡ä¸€å±‚ç¥ç»ç½‘ç»œä½œç”¨äºcontext wordsçš„å‘é‡å¾—åˆ°çš„ç»“æœä¸word $i$åœ¨å“ˆå¤«æ›¼æ ‘ä¸­çš„ä½ç½®å…·æœ‰ä¸€è‡´æ€§ã€‚*\n\n# 3. æ¨¡å‹æ¨å¯¼\n\nå…¬å¼\n\n$$\nF(w_i, w_j, w_k)=\\frac{P_{ik}}{P_{jk}} \n$$\n\nå³ä¾§çš„$\\frac{P_{ik}}{P_{jk}}$å¯ä»¥é€šè¿‡ç»Ÿè®¡æ±‚çš„ï¼›\n\nå·¦ä¾§çš„$w_i, w_j, w_k$æ˜¯æˆ‘ä»¬æ¨¡å‹è¦æ±‚çš„é‡ï¼›\n\nåŒæ—¶å‡½æ•°$F$æ˜¯æœªçŸ¥çš„ã€‚\n\nå¦‚æœèƒ½å¤Ÿå°†å‡½æ•°Fçš„å½¢å¼ç¡®å®šä¸‹æ¥ï¼Œå°±å¯ä»¥é€šè¿‡ä¼˜åŒ–ç®—æ³•æ±‚è§£è¯å‘é‡äº†ã€‚é‚£ä¹ˆGloVeæ¨¡å‹çš„ä½œè€…æ˜¯æ€ä¹ˆå°†Fç¡®å®šä¸‹æ¥çš„å‘¢ï¼Ÿ*ä¸ªäººè§‰ç€è¿™ä¸ªè¿‡ç¨‹çœŸæ˜¯è„‘æ´å¤§å¼€ï¼Œåæ­£æˆ‘æ˜¯æƒ³ä¸åˆ°ã€‚*\n\n1. $\\frac{P_{ik}}{P_{jk}}$è€ƒå¯Ÿäº†$i, j, k$ä¸‰ä¸ªwordä¸¤ä¸¤ä¹‹é—´çš„ç›¸ä¼¼å…³ç³»ï¼Œä¸å¦¨å•ç‹¬è€ƒå¯Ÿ$i, j$ ä¸¤ä¸ªè¯å’Œä»–ä»¬è¯å‘é‡$w_i, w_j$ï¼Œçº¿æ€§ç©ºé—´ä¸­çš„ç›¸ä¼¼å…³ç³»è‡ªç„¶æƒ³åˆ°çš„æ˜¯ä¸¤ä¸ªå‘é‡çš„å·®$(v_i-v_j)$ã€‚ æ‰€ä»¥Få‡½æ•°çš„å½¢å¼å¯ä»¥æ˜¯\n   $$\n   F(w_i - w_j, w_k)=\\frac{P_{ik}}{P_{jk}}\n   $$\n\n2. $\\frac{P_{ik}}{P_{jk}}$æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œè€ŒFæ˜¯ä½œç”¨åœ¨ä¸¤ä¸ªå‘é‡ä¸Šçš„ï¼Œå‘é‡å’Œæ ‡é‡ä¹‹é—´çš„å…³ç³»è‡ªç„¶æƒ³åˆ°äº†ä½¿ç”¨å†…ç§¯ã€‚æ‰€ä»¥Få‡½æ•°çš„å½¢å¼å¯ä»¥è¿›ä¸€æ­¥ç¡®å®šä¸º\n   $$\n   F((w_i - w_j)^T w_k)=F(w_i^T w_k- w_j^T w_k)=\\frac{P_{ik}}{P_{jk}}\n   $$\n\n3. åˆ°æ­¤ä¸ºæ­¢æ¨¡å‹å…¬å¼çš„å½¢å¼æ˜¯ $F(w_i^T w_k- w_j^T w_k)=\\frac{P_{ik}}{P_{jk}}$ã€‚ å·¦è¾¹æ˜¯å·®ï¼Œå³è¾¹æ˜¯å•†ï¼Œæ¨¡å‹é€šè¿‡å°†Få–ä½œexpæ¥å°†å·®å’Œå•†å…³è”èµ·æ¥\n   $$\n   exp(w_i^T w_k- w_j^T w_k)=\\frac{exp(w_i^T w_k)}{exp(w_j^T w_k)}=\\frac{P_{ik}}{P_{jk}}\n   $$\n\n4. ç°åœ¨åªéœ€è¦è®©åˆ†å­åˆ†æ¯åˆ†åˆ«ç›¸ç­‰ä¸Šå¼å°±èƒ½å¤Ÿæˆç«‹ï¼Œæ‰€ä»¥\n   $$\n   exp(w_i^T w_k)=P_{ik} \\\\\n   exp(w_j^T w_k)=P_{jk}\n   $$\n\n5. æ‰€ä»¥åªéœ€è¦åœ¨æ•´ä¸ªæ–‡æœ¬åº“ä¸­è€ƒå¯Ÿ$exp(w_i^T w_k)=P_{ik}=\\frac{X_{ik}}{X_i}$ ï¼Œå³\n   $$\n   w_i^T w_k=log(\\frac{X_{ik}}{X_i}) = logX_{ik} - logX_i \n   $$\n\n6. ä½œä¸ºå‘é‡ï¼Œäº¤æ¢$i$ å’Œ$k$ çš„é¡ºåº $w_i^T w_k$ å’Œ$w_k^T w_i$ æ˜¯ç›¸ç­‰çš„ï¼Œå³å…¬å¼å·¦è¾¹å¯¹äº$i$ å’Œ$k$ çš„é¡ºåºæ˜¯ä¸æ•æ„Ÿçš„ï¼Œä½†æ˜¯å…¬å¼å³è¾¹äº¤æ¢$i$ å’Œ$k$ çš„é¡ºåº$ logX_{ik} - logX_i  \\ne logX_{ki} - logX_k $ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå¯¹ç§°æ€§é—®é¢˜ï¼Œæ¨¡å‹å¼•å…¥äº†ä¸¤ä¸ªåæ‰§é¡¹ $b_i, b_k$,ä»è€Œå°†æ¨¡å‹å˜æˆäº†\n   $$\n   logX_{ik}  = w_i^T w_k + b_i + b_k\n   $$\n   å…¶ä¸­$b_i $ä¸­åŒ…å«äº†$logX_k$,æ‰€ä»¥å…¬å¼ä¸­æ²¡æœ‰æ˜¾ç¤ºçš„å†™æ˜è¿™ä¸€é¡¹ï¼Œä¸ºäº†ä¿æŒæ¨¡å‹çš„å¯¹ç§°æ€§ï¼ŒåˆåŠ å…¥äº† $b_k$.\n7. ä¸Šé¢çš„å…¬å¼åªæ˜¯ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨å®é™…å®éªŒä¸­å·¦å³ä¸¤è¾¹åªèƒ½è¦æ±‚æ¥è¿‘ã€‚ä»è€Œå°±æœ‰äº†ä»£ä»·å‡½æ•°ï¼ˆcost functionï¼‰\n   $$\n   J= \\sum_{ik}(w_i^T w_k + b_i + b_k -logX_{ik})^2\n   $$\n\n8. æ ¹æ®ç»éªŒï¼Œå¦‚æœä¸¤ä¸ªè¯å…±åŒå‡ºç°çš„æ¬¡æ•°è¶Šå¤šï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªè¯åœ¨ä»£ä»·å‡½æ•°ä¸­çš„å½±å“å°±åº”è¯¥çº¦å¤§ï¼Œæ‰€ä»¥å¯ä»¥æ ¹æ®ä¸¤ä¸ªè¯å…±åŒå‡ºç°çš„æ¬¡æ•°è®¾è®¡ä¸€ä¸ªæƒé‡é¡¹æ¥å¯¹ä»£ä»·å‡½æ•°ä¸­çš„æ¯ä¸€é¡¹è¿›è¡ŒåŠ æƒï¼š\n   $$\n   J= \\sum_{ik}f(X_{ik})(w_i^T w_k + b_i + b_k -logX_{ik})^2\n   $$\n   æ¨¡å‹è®¤ä¸ºæƒé‡å‡½æ•°$f$åº”è¯¥ç¬¦åˆä»¥ä¸‹ä¸‰ä¸ªç‰¹ç‚¹ï¼Œ1. $f(0)=0$ ï¼ˆå¦‚æœä¸¤ä¸ªè¯æ²¡æœ‰å…±åŒå‡ºç°è¿‡ï¼Œæƒé‡å°±æ˜¯0ï¼‰ï¼›2. $f(x)$å¿…é¡»æ˜¯éå‡å‡½æ•°ï¼ˆä¸¤ä¸ªè¯å…±åŒå‡ºç°çš„æ¬¡æ•°å¤šï¼Œåè€Œæƒé‡å˜å°äº†ï¼Œè¿åäº†è®¾ç½®æƒé‡é¡¹çš„åˆè¡·ï¼‰ï¼›3. $f(x)$å¯¹äºè¾ƒå¤§çš„$x$ä¸èƒ½å–å¤ªå¤§çš„å€¼ï¼ˆå°±åƒæ˜¯æ±‰è¯­ä¸­â€œçš„â€è¿™ä¸ªå­—ï¼Œåœ¨å¾ˆå¤šæ–‡ç« ä¸­éƒ½ä¼šå‡ºç°å¾ˆå¤šæ¬¡ï¼Œä½†æ˜¯å…¶åœ¨æ–‡ä¸­çš„é‡è¦ç¨‹åº¦éå¸¸å°ï¼‰ã€‚ç»¼åˆè¿™ä¸‰æ¡ç‰¹ç‚¹çš„$f(x)$å®šä¹‰ä¸ºï¼š\n   $$\n   f(x)=\\left\\{\\begin{aligned} (\\frac{x}{x_{max}})^\\alpha &,   if \\;  x<x_{max} \\\\1&,\\;otherwise   \\end{aligned}\\right.\n   $$\n\n   â€‹\n   ![](GloVe/Weighting-function.png)\n   æ ¹æ®ç»éªŒï¼ŒGloVeä½œè€…è®¤ä¸º$x_{max}=100$, $\\alpha=\\frac34$æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„é€‰æ‹©ã€‚\n\n\n\nnoteï¼šæœ¬æ–‡æ˜¯ä¸»è¦æ˜¯åŸºäºåŸå§‹paperè®ºè¿°ï¼Œä½†ä¹Ÿå±€é™äºä¸ªäººçŸ¥è¯†é¢å’Œç†è§£æ°´å“ï¼Œé‡Œé¢æºæ‚äº†ä¸€äº›ä¸ªäººçš„è§‚ç‚¹ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œè¯·è°…è§£\n\n# å‚è€ƒæ–‡çŒ®\n\n1. [LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n2. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://www.transacl.org/ojs/index.php/tacl/article/view/570)\n3. [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/syllabus.html)\n4. [word2vec ä¸­çš„æ•°å­¦åŸç†è¯¦è§£](http://blog.csdn.net/itplus/article/details/37969519)\n5. [GloVe: Global Vectors for Word Representation paper](https://nlp.stanford.edu/pubs/glove.pdf)\n6. [GloVe: Global Vectors for Word Representation tutorial](https://nlp.stanford.edu/projects/glove/)\n7. [ç†è§£GloVeæ¨¡å‹ï¼ˆGlobal vectors for word representationï¼‰](http://blog.csdn.net/coderTC/article/details/73864097)\n","tags":["NLP"],"categories":["NLP"]},{"title":"xgboost","url":"%2Fblog%2Fxgboost.html","content":"\n\n\nåœ¨æ–‡ç« [GBDT](https://weirping.github.io/blog/GBDT.html)ä¸­ä»‹ç»äº† Gradientã€Boosterã€Gradient Booster ç­‰ä¸€äº›åŸºæœ¬ç†è®ºæ¨å¯¼ï¼Œå’Œä¸€ç§Tree boostingç®—æ³•-GBDTç®—æ³•ã€‚ sklearn ä¹Ÿæä¾›äº† GBDT çš„å®ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„ä½¿ç”¨ã€‚ ä½†æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æˆ‘ä»¬é€šå¸¸ä½¿ç”¨çš„æ˜¯ [Xgboost](https://xgboost.readthedocs.io/en/latest/) è¿™ä¸ªå·¥å…·æä¾›çš„Tree boostingç®—æ³•ã€‚åœ¨æœ¬æ–‡è®°å½•ä¸€ä¸‹Xgboostä¸­å®ç°çš„Tree boostingçš„åŸç†å’Œå…¶ä¸»è¦çš„åº”ç”¨ï¼Œ å¦‚åˆ†ç±»ï¼Œå›å½’ï¼Œrankingã€‚\n\n# Regression Tree Ensemble\n\n\n\nTree Ensemble å®é™…ä¸Šå¯ä»¥ç†è§£ä¸ºå°† Gradient Booster(å‚è€ƒæ–‡ç«  [GBDT](https://weirping.github.io/blog/GBDT.html)) ä¸­å°†æ¯ä¸€æ­¥çš„æ¨¡å‹(å¦‚å¼±åˆ†ç±»å™¨)æ¢æˆå›å½’æ ‘çš„ä¸€ç±»æ¨¡å‹ã€‚\n\nXgboost ä¸­å®ç°çš„ Tree Ensemble ç®—æ³•å’Œ GBDT éå¸¸ç›¸ä¼¼ï¼Œ ä½†ä¹Ÿæœ‰æ‰€ä¸åŒã€‚æˆ‘è§‰å¾—åœ¨åŸç†ä¸Šæ˜¯æœ‰æ‰€å‡çº§çš„ã€‚æˆ‘ä»¬å…ˆä» Tree Ensemble çš„ç›®æ ‡å‡½æ•°å‡ºå‘ï¼Œæ²¿ç€å…¶æ¨å¯¼è¿‡ç¨‹æ¥çœ‹å…¶æ”¹è¿›ä¹‹å¤„ã€‚\n\nTree Ensemble æ¨¡å‹å®šä¹‰ä¸º ç”± $K$ ä¸ªå›å½’æ ‘ç›¸åŠ ç»„æˆçš„æ¨¡å‹ ï¼š\n$$\n\\hat y_i= \\sum_{i=1}^K f_k(x_i), f_k \\in \\mathcal F \\tag 1\n$$\nå…¶ä¸­: $\\mathcal F$ è¡¨ç¤ºå›å½’æ ‘çš„é›†åˆ, å®šä¹‰ä¸º:\n$$\n\\mathcal F = \\{ f(x) = w_{q(x)}\\} (q: R^m \\to T, w \\in R^T) \\tag 2\n$$\nç®€å•è§£é‡Šä¸€ä¸‹ä¸Šé¢çš„å…¬å¼ï¼š\n\n- $q$ è¡¨ç¤ºä¸€ä¸ªæ ‘çš„ç»“æ„ï¼Œå…¶ä½œç”¨å°±æ˜¯å°†ä¸€ä¸ªå…·æœ‰ $m$ ç»´ç‰¹å¾çš„ sample æ˜ å°„åˆ°$T$ä¸ªå¶å­èŠ‚ç‚¹ä¸Šã€‚å³ï¼Œ$q: R^m \\to T$ ã€‚ $q(x)$ è¡¨ç¤ºå¶å­èŠ‚ç‚¹çš„åºå·ã€‚\n- $w$ æ˜¯ä¸€ä¸ª $T$ ç»´å‘é‡ï¼Œå…¶æ¯ä¸ªç»´åº¦å¯¹åº”æ ‘çš„ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œè¡¨ç¤ºå›å½’æ ‘ä¸­æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„å–å€¼ã€‚ $w_{q(x)}$ è¡¨ç¤ºæ ·æœ¬ $x$ åœ¨å›å½’æ ‘ä¸Šçš„é¢„æµ‹å€¼ã€‚\n\n\n\nTree Ensemble çš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\n& obj = \\sum_{i=1}^N l(y_i, \\hat y_i)  + \\sum_{k=1}^K \\Omega (f_k) \\\\\n& where\\ \\Omega (f) = \\gamma T + \\frac 12 \\lambda||w||^2\n\\end {align} \\tag 3\n$$\nå…¶ä¸­ï¼š \n\n- ç¬¬ä¸€é¡¹ ä¸º è®­ç»ƒé›†ä¸Šçš„æŸå¤±å‡½æ•°(éœ€è¦æ˜¯å¯å¾®çš„å‡¸å‡½æ•°)ã€‚æ ¹æ®ä»»åŠ¡ç›®æ ‡çš„ä¸åŒï¼ŒæŸå¤±å‡½æ•°çš„å®šä¹‰ä¹Ÿä¸åŒã€‚å¦‚ï¼š\n  å›å½’ä»»åŠ¡å¯ä»¥ä½¿ç”¨å¹³æ–¹å’ŒæŸå¤±å‡½æ•° (Square loss) $ l(y_i, \\hat y_i) = (y- \\hat y_i)^2 $ ï¼›\n  åˆ†ç±»ä»»åŠ¡å¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼›\n  rankä»»åŠ¡ä½¿ç”¨ ......  è¿™ä¸€ä¸ªè¯´æ¥è¯é•¿ï¼Œåé¢å†è¯´å§ã€‚\n- ç¬¬äºŒé¡¹ ä¸º æ¨¡å‹æ­£åˆ™é¡¹ï¼Œå®ƒä»æ ‘çš„ç»“æ„å’Œæ ‘çš„é¢„æµ‹å€¼ä¸¤ä¸ªæ–¹é¢å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œæ­£åˆ™åŒ–\n  1. $\\gamma T$ å€¾å‘äºé€‰æ‹©å¶å­èŠ‚ç‚¹å°‘çš„å›å½’æ ‘ï¼ˆç»“æ„ç®€å•ï¼‰ï¼›\n  2. $\\frac 12 \\lambda||w||^2$å€¾å‘äºé¢„æµ‹å€¼ç¨³å®šçš„æ ‘ï¼ˆé¢„æµ‹å€¼ï¼‰ã€‚\n\n  å³è®ºæ–‡ä¸­æ‰€è¯´çš„ *Intuitively, the regularized objective will tend to select a model employing simple and predictive functions.* \n\n# Gradient Boosting\n\nå¦‚æ–‡ç« [GBDT](https://weirping.github.io/blog/GBDT.html)ä¸­æ‰€è¿° åœ¨ Gradient Boosting ä¸­è¿›è¡Œè‹¥å¹²è½®è¿­ä»£è®¡ç®—ï¼Œæ¯ä¸€è½®äº§ç”Ÿä¸€ä¸ªæ–°çš„å†³ç­–æ ‘å’Œå…¶å¯¹åº”çš„é¢„æµ‹å€¼ã€‚ä¸”ï¼Œæ¯ä¸€ä¸ªæ–°çš„å†³ç­–æ ‘éƒ½æ˜¯ä¸Šä¸€è½®é¢„æµ‹å€¼çš„æ®‹å·®ã€‚é‚£ä¹ˆæ€ä¹ˆç¡®å®šè¿™é¢—æ–°çš„å†³ç­–æ ‘å‘¢ï¼Ÿæœ¬èŠ‚æ¨å¯¼ç¬¬ $t$ é¢—å†³ç­–æ ‘çš„ç¡®å®šæ–¹æ³•ã€‚\n\nå¦‚ä¸‹æ‰€ç¤ºï¼š\n$$\n\\begin {align}\n\\hat y_i^{(0)} &= 0 \\\\\n\\hat y_i^{(1)} &= f_1(x_i) = \\hat y_i^{(0)} +  f_1(x_i)   \\\\\n\\hat y_i^{(2)} &= f_1(x_i) + f_2(x_i) = \\hat y_i^{(1)} +  f_2(x_i)   \\\\\n\\dots \\\\\n\\hat y_i^{(t)} &= \\sum_{k=1}^t f_k(x_i)  = \\hat y_i^{(t-1)} +  f_t(x_i)   \\\\\n\\end {align}\n$$\n\nå¯ä»¥çœ‹å‡ºç¬¬ $t$ è½®çš„é¢„æµ‹å€¼æ˜¯ç¬¬ $t-1$ è½®çš„é¢„æµ‹å€¼ åŠ ä¸Š ä¸€ä¸ªæ–°çš„å†³ç­–æ ‘çš„é¢„æµ‹å€¼ã€‚é‚£ä¹ˆè¿™é¢—æ–°çš„å†³ç­–æ ‘æ€ä¹ˆç¡®å®šå‘¢ï¼Ÿè™½ç„¶æ­¤å¤„å€Ÿé‰´äº†æ¢¯åº¦ä¸‹é™æ³•çš„åŸç†ï¼Œå¯æ˜¯æˆ‘ä»¬å´ä¸èƒ½ç›´æ¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•(å¦‚SGD)æ±‚è§£è¿™é¢—æ–°çš„å†³ç­–æ ‘( since they are \ntrees, instead of just numerical vectors)ã€‚\n\næˆ‘ä»¬çš„è§£å†³æ–¹æ³•ç§°ä¸º **Additive Training** å³(Boosting)ã€‚å›åˆ°æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°å’Œç¬¬ $t$ è½®è¿­ä»£æ—¶çš„é¢„æµ‹å€¼è¡¨è¾¾å¼ $\\hat y_i^{(t)} = \\sum_{k=1}^t f_k(x_i)  = \\hat y_i^{(t-1)} +  f_t(x_i) $, æˆ‘ä»¬å¯ä»¥å°†ç›®æ ‡å‡½æ•°è¡¨è¾¾å¦‚ä¸‹: \n$$\nobj^{(t)} = \\sum_{i=1}^N l(y_i, \\hat y_i^{(t-1)} +  f_t(x_i))  + \\Omega (f_t) + constant \\tag 4\n$$\nå…¶ä¸­: $\\sum_{i=1}^t \\Omega (f_i) = \\Omega (f_t) +  \\sum_{i=1}^{t-1} \\Omega (f_i)= \\Omega (f_t) + constant$, å¯¹äºç¬¬ $t$ è½®æ¥è¯´ å‰ $t-1$ ä¸ªå†³ç­–æ ‘éƒ½æ˜¯ç¡®å®šçš„ï¼Œæ‰€ä»¥ $ \\sum_{i=1}^{t-1} \\Omega (f_i)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚\n\næ‰€ä»¥ç¬¬ $t$ è½®è¿­ä»£æ—¶æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ä¸­åªæœ‰ä¸€ä¸ªæœªçŸ¥å‚æ•° $f_t(x_i)$ å³æˆ‘ä»¬éœ€è¦çš„å†³ç­–æ ‘ã€‚ æ‰€ä»¥ç¬¬ $t$ ä¸ªå†³ç­–æ ‘åªè¦èƒ½å¤Ÿä½¿å¾—ä¸Šé¢çš„ç›®æ ‡å‡½æ•°æœ€å°åŒ–å°±å¥½äº†ã€‚\n\nä½†æ˜¯ä¸Šé¢çš„ç›®æ ‡å‡½æ•°è¿˜æ˜¯æœ‰ç‚¹å¤æ‚ï¼Œèƒ½ä¸èƒ½ç®€åŒ–ä¸€ç‚¹å‘¢ï¼Ÿ å½“ç„¶ï¼Œå€ŸåŠ©æŸå¤±å‡½æ•°çš„äºŒçº§æ³°å‹’å±•å¼€å¼æ¥è¿‘ä¼¼æŸå¤±å‡½æ•°ã€‚\n\nå›å¿†ä¸€ä¸‹ äºŒé˜¶æ³°å‹’å±•å¼€å¼ï¼š\n$$\nf(x + \\Delta x) \\approx f(x) + f'(x) \\Delta x + \\frac 12 f''(x) \\Delta x^2 \\tag 5\n$$\nä¸ºäº†ç®€åŒ–è¡¨ç¤º å®šä¹‰å¦‚ä¸‹ä¸¤ä¸ªç¬¦å·: \n$$\ng_i = \\frac {\\partial l(y_i, \\hat y_i )} {\\partial \\hat y_i }|_{\\hat y_i^{(t-1)}} \\\\\nh_i= \\frac {\\partial ^2 l(y_i, \\hat y_i )} {\\partial \\hat y_i^2 }|_{\\hat y_i^{(t-1)}}  \\tag 6\n$$\nnote: å¯¹äºç¡®å®šçš„æŸå¤±å‡½æ•°(ç”±ä»»åŠ¡ç›®æ ‡å†³å®š) , ç¬¬ $t$ è½®è¿­ä»£æ—¶ä»¥ä¸Šä¸¤ä¸ªå€¼æ˜¯ç¡®å®šå€¼(å¸¸æ•°)ã€‚\n\nä½¿ç”¨æŸå¤±å‡½æ•°çš„äºŒé˜¶æ³°å‹’å±•å¼€å¼è¿‘ä¼¼çš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n$$\nobj^{(t)} \\approx \\sum_{i=1}^N [l(y_i, \\hat y_i^{(t-1)}) +  g_i f_t(x_i) + \\frac 12 h_i f^2_t(x_i)]  + \\Omega (f_t) + constant \\tag 7\n$$\n\nå¯¹äºç¬¬ $t$ è½®è¿­ä»£, ä¸Šå¼ä¸­ $l(y_i, \\hat y_i^{(t-1)})$ ä¹Ÿæ˜¯å¸¸æ•°é¡¹ç›®ï¼Œå»æ‰å¸¸æ•°é¡¹ï¼Œ æ‰€ä»¥å¯¹äºç¬¬ $t$ è½®è¿­ä»£çš„ç›®æ ‡å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æˆä¸‹å¼ï¼š\n$$\n\\tilde{\\mathcal L}^{(t)} = \\sum_{i=1}^N [ g_i f_t(x_i) + \\frac 12 h_i f^2_t(x_i)]  + \\Omega (f_t) \\tag 8\n$$\n\né€šè¿‡ä¸Šå¼å¯¹ç¬¬ $t$ æ­¥å¯èƒ½çš„å€™é€‰å†³ç­–æ ‘$f_t$è¿›è¡Œæ‰“åˆ†ï¼Œèƒ½å¤Ÿä½¿ $\\tilde{\\mathcal L}^{(t)}$ æœ€å°çš„ä¸€ä¸ª $f_t$ å°±æ˜¯è¿™ä¸€æ­¥éœ€è¦çš„å†³ç­–æ ‘ã€‚\n\n---\n\n# ç¡®å®šå›å½’æ ‘é¢„æµ‹å€¼\n\næœ¬èŠ‚å…ˆå‡è®¾å›å½’æ ‘çš„ç»“æ„ $q$ æ˜¯ç¡®å®šçš„ï¼Œæ¨å¯¼å›å½’æ ‘ä¸­æ¯ä¸ªå¶å­èŠ‚ç‚¹é¢„æµ‹å€¼ $\\hat w$ã€‚\n\nå¯¹äºä¸€ä¸ªå†³ç­–æ ‘ $q: R^m \\to T, w \\in R^T$ æ¥è¯´ï¼Œä»¤ $I_j = \\{ i | q(x_i) = j \\}$ è¡¨ç¤ºæ‰€æœ‰æ˜ å°„åˆ°ç¬¬ $j$ ä¸ªå¶å­èŠ‚ç‚¹çš„æ ·æœ¬çš„é›†åˆã€‚è”åˆå…¬å¼3 å¯¹æ­£åˆ™åŒ–é¡¹çš„å®šä¹‰ï¼Œå…¬å¼ 7 å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\n\\tilde{\\mathcal L}^{(t)} &= \\sum_{i=1}^N [ g_i f_t(x_i) + \\frac 12 h_i f^2_t(x_i)]  + \\Omega (f_t) \\\\\n&= \\sum_{i=1}^N [ g_i f_t(x_i) + \\frac 12 h_i f^2_t(x_i)]  + \\gamma T + \\frac 12 \\lambda||w||^2 \\\\\n&= \\sum_{i=1}^N [ g_i f_t(x_i) + \\frac 12 h_i f^2_t(x_i)]  + \\gamma T + \\frac 12 \\lambda \\sum_{j=1}^Tw_j^2 \\\\\n&=  \\sum_{j=1}^T [(\\sum_{i \\in I_j} g_i) w_j + \\frac 12 (\\sum_{i \\in I_j} h_i) w_j^2 ]  + \\gamma T + \\frac 12 \\lambda \\sum_{j=1}^Tw_j^2 \\\\\n&=  \\sum_{j=1}^T [(\\sum_{i \\in I_j} g_i) w_j + \\frac 12 (\\sum_{i \\in I_j} h_i + \\lambda) w_j^2 ]  + \\gamma T \\\\\n\\end {aligned}\n$$\n\n\nä»¤$G_j = \\sum_{i \\in I_j} g_i; H_j = \\sum_{i \\in I_j} h_i $ ä¸Šå¼å¯ä»¥è¡¨ç¤ºä¸º:\n$$\n\\tilde{\\mathcal L}^{(t)} =  \\sum_{j=1}^T [G_j w_j + \\frac 12 (H_j + \\lambda) w_j^2 ]  + \\gamma T \\tag 9\n$$\nç»“åˆå…¬å¼6 å¯ä»¥å‘ç° $G_j, H_j$ åªä¸æ ‘ç»“æ„ $q$ æœ‰å…³ã€‚\n\nå›æƒ³ä¸Šé¢æˆ‘ä»¬å¯¹å›å½’æ ‘çš„å®šä¹‰ï¼š $q$ è¡¨ç¤ºæ ‘çš„ç»“æ„ï¼Œ $w$ è¡¨ç¤ºæ¯ä¸ªå¶å­èŠ‚ç‚¹ä¸Šçš„é¢„æµ‹å€¼ã€‚æ‰“åˆ†å…¬å¼ $\\tilde{\\mathcal L}^{(t)}$ åªä¸ $q$ å’Œ $w$ æœ‰å…³ã€‚é‚£ä¹ˆå¯¹äºä¸€ä¸ªç¡®å®šçš„æ ‘ç»“æ„ $q$ ï¼Œ æœ€ä¼˜çš„å¶å­èŠ‚ç‚¹é¢„æµ‹å€¼ $w$ å¯ä»¥è¡¨ç¤ºä¸º ï¼š\n$$\n\\hat w_j = - \\frac {G_j}{H_j + \\lambda}\n$$\n\n$$\n\\begin {aligned}\n\\tilde{\\mathcal L}^{(t)} &= - \\frac 12 \\sum_{j=1}^T \\frac {G_j^2}{H_j + \\lambda}  + \\gamma T \\\\\n&= - \\frac 12 \\sum_{j=1}^T \\frac {(\\sum_{i \\in I_j} g_i)^2}{(\\sum_{i \\in I_j} h_i) + \\lambda}  + \\gamma T\n\\end {aligned} \\tag {10}\n$$\n\nnote: $ \\hat w_j = - \\frac {G_j}{H_j + \\lambda}$ æ˜¯ä½¿å…¬å¼8 å–æœ€å°å€¼çš„ $w$ã€‚\n\n![](xgboost/structure-score.png)\n\nç”±å…¬å¼ 10 å¯ä»¥å‘ç° å¯¹äºä¸€ä¸ªç»“æ„ç¡®å®šçš„å›å½’æ ‘ $q$ï¼Œ$\\tilde{\\mathcal L}^{(t)}$ çš„å–å€¼ä¸$w$ æ˜¯æ— å…³çš„ã€‚å³ï¼Œ$\\tilde{\\mathcal L}^{(t)}$ å€¼ä¸å›å½’æ ‘çš„ç»“æ„ q æœ‰å…³, æ‰€ä»¥è®ºæ–‡ä¸­ç§°å…¬å¼10 ä¸º **structure score**ã€‚å³ *a scoring function to measure the quality of a tree structure q.*\n\næ‰€ä»¥åªéœ€è¦ç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„æ ‘ç»“æ„ q ï¼Œæ±‚èƒ½å¤Ÿä½¿çš„$\\tilde{\\mathcal L}^{(t)}$æœ€å°çš„ä¸€ä¸ª $q$ å³å¯ã€‚\n\nä½†æ˜¯ï¼Œ$q$ æœ‰æ— ç©·ä¸ªå¯èƒ½çš„å–å€¼ã€‚\n\n---\n\n# ç¡®å®šçš„å›å½’æ ‘ç»“æ„\n\næ— è®ºæ˜¯å†³ç­–æ ‘ï¼Œå›å½’æ£®æ—è¿˜æ˜¯GBDTï¼Œxgboostä¸­çš„Tree boostingç®—æ³• éƒ½éœ€è¦ä»¥è¶…å‚çš„å½¢å¼å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ã€‚åœ¨æœ€å¤§æ·±åº¦çš„é™åˆ¶ä¸‹å¦‚æœèƒ½ç¡®å®šæ¯ä¸ªèŠ‚ç‚¹çš„åˆ†è£‚æ¡ä»¶ï¼Œå°±èƒ½å¤Ÿç¡®å®šä¸€é¢—å”¯ä¸€çš„å›å½’æ ‘ç»“æ„ $q$ã€‚é€‰å–æ–¹æ³•ä¸ºï¼š\n\n> 1. Start from tree with depth 0\n> 2. For each leaf node of the tree, try to add a split. \n> 3. é‡å¤ç¬¬äºŒæ­¥ï¼ŒçŸ¥é“è¾¾åˆ°æœ€å¤§æ·±åº¦\n\nå®Œæˆä»¥ä¸Šä¸‰æ­¥ï¼Œå…¶å®å°±æšä¸¾äº†æ‰€æœ‰å¯èƒ½çš„æ ‘ç»“æ„ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªèƒ½å¤Ÿä½¿å…¬å¼10å–æœ€å°å€¼çš„ç»“æ„ã€‚å…¶ä¸­å…³é”®é—®é¢˜åœ¨äºç¬¬äºŒæ­¥ä¸­æ€ä¹ˆç¡®å®šåˆ†å‰²ç‚¹ã€‚\n\nå…¬å¼10 å…¶å®å°±æ˜¯ç±»ä¼¼ä¸å†³ç­–æ ‘ä¸­ä¸çº¯åº¦çš„å®šä¹‰ã€‚åœ¨xgboostç¡®å®šå›å½’æ ‘çš„ç»“æ„çš„è¿‡ç¨‹ä¸­ï¼Œå°±æ˜¯ä½¿ç”¨å…¬å¼10æ¥å¯¹æ ‘çš„å¶å­èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†çš„ã€‚\n\nä»¤ $I_L$ å’Œ$I_R$ åˆ†åˆ«è¡¨ç¤ºä¸€ä¸ªå¶å­èŠ‚ç‚¹åˆ†ç±»åå·¦è¾¹å’Œå³è¾¹èŠ‚ç‚¹çš„æ ·æœ¬é›†åˆï¼Œ $I = I_L \\cup I_R$, é‚£ä¹ˆè¿™ä¸€ä¸ªåˆ†å‰²ç‚¹çš„æ‰“åˆ†å…¬å¼è¡¨ç¤ºå¦‚ä¸‹ï¼š\n$$\n\\begin {aligned}\ngain &=  \\mathcal L -  (\\mathcal L_L + \\mathcal L_R) \\\\\n&= \\frac 12 [\\frac {(\\sum_{i \\in I_L} g_i)^2}{(\\sum_{i \\in I_L} h_i)} + \\frac {(\\sum_{i \\in I_R} g_i)^2}{(\\sum_{i \\in I_R} h_i)} - [\\frac {(\\sum_{i \\in I} g_i)^2}{(\\sum_{i \\in I} h_i)} ] - \\gamma\n\\end {aligned} \\tag {11}\n$$\nç»“åˆå…¬å¼10 å¯ä»¥å‘ç°ï¼Œ$gain$è¶Šå¤§ï¼ŒæŸå¤±å‡½æ•°è¶Šå°ï¼Œè·Ÿä¸ªç‚¹é€‰å–çš„è¶Šå¥½ã€‚æ‰€æœ‰æˆ‘ä»¬åªéœ€è¦é€‰å–ä¸€ä¸ªèƒ½ä½¿$gain$æœ€å¤§çš„ä¸€ä¸ªåˆ†å‰²ç‚¹å³å¯ã€‚xgboostä¸­æ˜¯ç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹ï¼Œå³æ ·æœ¬æ•°æ®ä¸­æ¯ä¸€ç»´åº¦çš„æ¯ä¸€ä¸ªæ ·æœ¬å€¼éƒ½ä½œä¸ºå€™é€‰åˆ†å‰²ç‚¹ï¼Œé€‰å–gainæœ€å¤§çš„ä¸€ä¸ªã€‚\n\n![](xgboost/greedy-algorithm.png)\n\nxgboost æä¾›äº†å¤šç§æ–¹æ³•æ¥æ±‚è§£åˆ†å‰²ç‚¹ï¼Œä¸Šè¿°æ–¹æ³•ç§°ä¸º exact æ–¹æ³•ï¼Œé’ˆå¯¹å†…å­˜èƒ½å¤Ÿä¸€æ¬¡è½½å…¥çš„æ•°æ®é›†ã€‚å…¶æ¬¡è¿˜æœ‰approxæ–¹æ³•(ä¸€ç§è¿‘ä¼¼æ–¹æ³•)ï¼Œ å’Œ histæ–¹æ³•(åŸºäºç›´æ–¹å›¾çš„æ–¹æ³•)ã€‚è¿˜æœ‰GPUç‰ˆæœ¬ã€‚\n\nå…¶ä»–æ–¹æ³•å‚è§åŸå§‹è®ºæ–‡ã€‚\n\n# Shrinkage and Column Subsampling\n\nShrinkage å’Œ Column Subsampling æ˜¯åˆä¸€ç§å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„æŠ€æœ¯ã€‚\n\n## Shrinkage\n\nshrinkage reduces the inuence of each individual tree and leaves space for future trees to improve the model.\n\nå³ç»è¿‡æ ‡å‡†çš„tree boostingæµç¨‹ç¡®å®šå®Œä¸€é¢—å›å½’æ ‘ä»¥åï¼Œç¬¬ $t$ æ­¥çš„é¢„æµ‹å€¼ä½¿ç”¨ä¸‹å¼è®¡ç®—:\n$$\n\\hat y_i^{(t)} = \\hat y_i^{(t-1)} +  \\eta f_t(x_i)\n$$\n\nåŸç†å’Œæ¢¯åº¦ä¸‹é™æ³•ä¸­çš„å­¦ä¹ ç‡ç±»ä¼¼ã€‚\n\n##ã€€Column Subsampling\n\nå¦‚ä¸Šæ–‡æ‰€è¿°åœ¨ç¡®å®šå†³ç­–æ ‘åˆ†å‰²ç‚¹æ—¶ä½¿ç”¨äº†æ‰€ä»¥ç‰¹å¾çš„æ‰€æœ‰æ ·æœ¬å€¼è®¡ç®— gain, ä»ä¸­é€‰å–ä¸€ä¸ªæœ€å¥½çš„ã€‚ä»è®¡ç®—é‡å’Œè¿‡æ‹Ÿåˆé—®é¢˜çš„è§’åº¦çœ‹ï¼Œå…¶å®æˆ‘ä»¬å¯ä»¥ä»æ‰€æœ‰ç‰¹å¾ä¸­éšæœºçš„æŠ½å–å›ºå®šæ¯”ä¾‹çš„ç‰¹å¾(å¦‚ 30%)ä½œä¸ºå€™é€‰ç‰¹å¾ã€‚\n\nåŒ…æ‹¬å¦‚ä¸‹ä¸‰ä¸ªå‚æ•°ï¼š `colsample_bytree`, `colsample_bylevel`, `colsample_bynode`\n\n# ä¸ GBDT çš„ä¸åŒç‚¹\n\n\næˆ‘å¹¶æ²¡æœ‰çœ‹ åŸå§‹ GBDTçš„å®ç°ç»†èŠ‚ï¼Œä»¥ä¸‹æ˜¯æˆ‘ç›´æ¥æ ¹ xgboost çš„æ®è®ºæ–‡å’Œ æ–‡ç«  GBDT çš„å‚è€ƒèµ„æ–™æ•´ç†çš„ xgboost å’Œ åŸå§‹ GBDT çš„ä¸åŒç‚¹ï¼Œå¦‚æœä»¥ååˆæœºä¼šäº†è§£æ›´ç»†èŠ‚çš„å†…å®¹å†å›æ¥ä¿®æ”¹ï¼š\n\n1. å¢åŠ äº†æ¨¡å‹æ­£åˆ™é¡¹ï¼ŒGBDTç®—æ³•ä¸­å¹¶æ²¡æœ‰ä½¿ç”¨æ­£åˆ™åŒ–é¡¹ï¼›\n2. é™¤äº†æ­£åˆ™åŒ–ä»¥å¤–è¿˜ä½¿ç”¨äº†å¤šç§æŠ€æœ¯è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¦‚subsampleï¼ŒShrinkageï¼ŒColumn Subsamplingï¼›\n3. ä½¿ç”¨äº†äºŒé˜¶æ³°å‹’å±•å¼€å¼è¿‘ä¼¼ç›®æ ‡å‡½æ•°ï¼ŒGBDTä¸­ä½¿ç”¨çš„æ˜¯ç›´æ¥ä½¿ç”¨çš„ä¸€é˜¶æ¢¯åº¦ã€‚\n\n\n# å‚è€ƒèµ„æ–™\n\n[Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n\n[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)\n\n","tags":["LTR"],"categories":["Machine Learning"]},{"title":"GBDT","url":"%2Fblog%2FGBDT.html","content":"\nGBDT(Gradient Boosting Decision Tree) åˆå« MARTï¼ˆMultiple Additive Regression Tree)ï¼Œæ˜¯ä¸€ç§è¿­ä»£çš„å†³ç­–æ ‘ç®—æ³•ï¼Œè¯¥ç®—æ³•ç”±å¤šæ£µå†³ç­–æ ‘ç»„æˆï¼Œæ‰€æœ‰æ ‘çš„ç»“è®ºç´¯åŠ èµ·æ¥åšæœ€ç»ˆç­”æ¡ˆã€‚å®ƒåœ¨è¢«æå‡ºä¹‹åˆå°±å’ŒSVMä¸€èµ·è¢«è®¤ä¸ºæ˜¯æ³›åŒ–èƒ½åŠ›ï¼ˆgeneralization)è¾ƒå¼ºçš„ç®—æ³•ã€‚è¯¥æ¨¡å‹è¢«å¹¿æ³›çš„åº”ç”¨ä¸ctré¢„ä¼°å’Œæœç´¢æ’åºä¸­ã€‚\n\næœ¬æ–‡æŒ‰ç…§Gradientï¼ˆæ¢¯åº¦ä¸‹é™æ³•ï¼‰ -> Boostingï¼ˆåŠ æ€§æ¨¡å‹ï¼‰ -> Decision Tree ï¼ˆå†³ç­–æ ‘ï¼‰çš„é¡ºåºæè¿°GBDTè¿™ä¸ªæ¨¡å‹ã€‚\n\n# Gradientï¼ˆæ¢¯åº¦ä¸‹é™æ³•ï¼‰\n\nå¯¹äºä¸€ä¸ªä¼˜åŒ–ç›®æ ‡å‡½æ•° \n$$\n\\min_{\\Theta} J(\\Theta)\n$$\nå…¶ä¸­ $\\Theta$ ä¸ºå‚æ•°ã€‚æ±‚è§£$\\Theta$ çš„è¿­ä»£å…¬å¼ä¸º: \n$$\n\\Theta^{i+1} = \\Theta^i - \\alpha \\nabla J(\\Theta)|_{\\Theta^i}\n$$\nå…¶ä¸­ $\\alpha$ ä¸ºæ­¥é•¿ã€‚\n\n# Boosting\n\nBoostingï¼Œè¿­ä»£ï¼Œå³é€šè¿‡è¿­ä»£å¤šä¸ªæ¨¡å‹æ¥å…±åŒå†³ç­–ã€‚å®ƒ\n\n- æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•\n- æ˜¯ä¸€ç§ç®—æ³•æ¡†æ¶\n- æ˜¯ä¸€ç§åŠ æ€§æ¨¡å‹\n\nå…¶ä¸­æ¯”è¾ƒç»å…¸çš„ç®—æ³•æœ‰  AdaBoostï¼ŒRealAdaBoostã€‚\n\nBoosting ä½¿ç”¨å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœçš„ç´¯åŠ æ¥ä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœ(å³ï¼ŒåŠ æ€§æ¨¡å‹)ã€‚æ¯”å¦‚åœ¨é¢„æµ‹äººçš„å¹´é¾„ä»»åŠ¡ä¸­ï¼ŒAçš„çœŸå®å¹´é¾„æ˜¯18å²ï¼Œä½†ç¬¬ä¸€ä¸ªæ¨¡å‹çš„é¢„æµ‹å¹´é¾„æ˜¯12å²ï¼Œå·®äº†6å²ï¼Œå³æ®‹å·®ä¸º6å²ã€‚é‚£ä¹ˆåœ¨ç¬¬äºŒä¸ªæ¨¡å‹é‡Œæˆ‘ä»¬æŠŠAçš„å¹´é¾„è®¾ä¸º6å²å»å­¦ä¹ ï¼Œå¦‚æœç¬¬äºŒä¸ªæ¨¡å‹çœŸçš„èƒ½æŠŠAåˆ†åˆ°6å²çš„å¶å­èŠ‚ç‚¹ï¼Œé‚£ç´¯åŠ ä¸¤æ£µæ ‘çš„ç»“è®ºå°±æ˜¯Açš„çœŸå®å¹´é¾„ï¼›å¦‚æœç¬¬äºŒæ£µæ ‘çš„ç»“è®ºæ˜¯5å²ï¼Œåˆ™Aä»ç„¶å­˜åœ¨1å²çš„æ®‹å·®ï¼Œç¬¬ä¸‰æ£µæ ‘é‡ŒAçš„å¹´é¾„å°±å˜æˆ1å²ï¼Œç»§ç»­å­¦ã€‚è¿™å°±æ˜¯ Boosting  çš„æ„ä¹‰ã€‚\n\nå…³äºè¿™ä¸ªæ–¹é¢çš„ç†è®ºè¿™é‡Œç®€å•ä»‹ç»å¦‚ä¸‹ï¼š\n\nå¯¹äºä¸€ä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œä½¿ç”¨äº†Mçš„æ¨¡å‹ï¼Œ é‚£ä¹ˆå¯¹äºä¸€æ¡æ•°æ®$x_i$ çš„é¢„æµ‹å€¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n$$\n\\hat y_i = \\sum_{m=1}^M \\beta_m b(x_i, \\gamma_m)\n$$\nå…¶ä¸­ï¼š\n\n- $b$ : åŸºç¡€æ¨¡å‹ï¼Œ å¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œä¹Ÿç§°ä¸º **åŸºåˆ†ç±»å™¨**ã€‚\n- $\\beta$: æ¯ä¸ªåŸºç¡€æ¨¡å‹åœ¨æ€»çš„é¢„æµ‹ç»“æœä¸­å æœ‰çš„æƒé‡ã€‚\n- $\\gamma$: æ¯ä¸ªåŸºç¡€æ¨¡å‹çš„å‚æ•°ã€‚\n\né‚£ä¹ˆä¸€ä¸ªé¢„æµ‹ä»»åŠ¡æ¥è¯´ï¼Œæ€»çš„ä¼˜åŒ–ç›®æ ‡å°±æ˜¯\n$$\n\\min_{\\{\\beta_m, \\gamma_m\\}_1^M}  \\sum_{i=1}^N L[y_i,  \\sum_{m=1}^M \\beta_m b(x_i, \\gamma_m)]\n$$\nå…¶ä¸­ï¼š $N$ ä¸ºè®­ç»ƒæ ·æœ¬é‡ã€‚\n\n# Gradient + Boosting\n\nä¸‹é¢æ¨å¯¼ Gradient å’Œ Boosting æ˜¯æ€ä¹ˆç»“åˆèµ·æ¥çš„ã€‚\n\nåŒæ ·è¿˜æ˜¯å¯¹äºä¸€ä¸ªé¢„æµ‹ä»»åŠ¡ï¼Œç»™å®šæ ·æœ¬é‡ä¸º $N$ æ ·æœ¬é›†, æ±‚é¢„æµ‹æ¨¡å‹ä¸º $f(x)$ã€‚ å³æˆ‘ä»¬çš„å­¦ä¹ ç›®æ ‡æ˜¯: \n$$\n\\vec f = {\\arg\\min}_{\\vec f} \\mathcal L(\\vec y, \\vec f)  ={\\arg\\min}_{\\vec f}   \\sum_{i=1}^N L[y_i,  f(x_i)]\n$$\nå…¶ä¸­: \n\n- $\\vec f = \\{f(x_1), f(x_2), \\dots , f(x_N) \\}$ , ä¸”å‡½æ•°å¯¹äºæ‰€æœ‰çš„æ ·æœ¬ $x_i$ æ˜¯åŒä¸€ä¸ªå‡½æ•°, å‘é‡çš„é•¿åº¦å’Œæ ·æœ¬é‡ç›¸åŒã€‚\n- $\\vec y = y_1, y_2, \\dots, y_N $ , å‘é‡çš„é•¿åº¦å’Œæ ·æœ¬é‡ç›¸åŒã€‚\n- $\\mathcal L(\\vec y, \\vec f)$ æ˜¯æ•´ä¸ªæ ·æœ¬é›†çš„æŸå¤±å‡½æ•°ã€‚\n\nå³ï¼Œä¼˜åŒ–ç›®æ ‡ä¸ºï¼šåœ¨æ•´ä¸ªæ ·æœ¬é›†ä¸Šçš„æŸå¤±æœ€å°ã€‚ ä¸‹é¢å€Ÿé‰´æ¢¯åº¦ä¸‹é™æ³•çš„åŸç†ï¼Œ **è®¤ä¸º $\\vec f$ ä¸ºå‚æ•°, è¿­ä»£æ±‚$\\vec f$**ã€‚ ç±»æ¯”å¦‚ä¸‹ï¼š\n\n| $ \\min_{\\vec f}\\mathcal L(\\vec y, \\vec f) $ | $\\min_{\\Theta} J(\\Theta)$ |\n| ---------------------------------------- | ------------------------- |\n| $\\vec f$                                 | $\\Theta$                  |\n| $\\mathcal L$                             | $J$                       |\n\n$\\min_{\\vec f}\\mathcal L(\\vec y, \\vec f)$ åœ¨$ \\vec f_{m-1}$ å¤„çš„æ¢¯åº¦ï¼š\n$$\n\\vec g_m = [\\frac{\\partial \\mathcal L(\\vec y, \\vec f)}{\\partial {\\vec f}}]_{\\vec f = \\vec f_{m-1}}\n$$\né‚£ä¹ˆæ±‚è§£$\\vec f_m$ çš„è¿­ä»£å…¬å¼ä¸º: \n$$\n\\vec f_m = \\vec f_{m-1} + \\alpha_m (- \\vec g_m)\n$$\nå…¶ä¸­ï¼š $\\alpha$ ä¸ºæ¯ä¸€æ­¥çš„æ­¥é•¿ï¼Œ åœ¨GBDTé‡Œé¢ä½¿ç”¨çš„æ˜¯ç»Ÿä¸€çš„æ­¥é•¿ï¼Œå³ï¼Œå¯¹äºæ‰€æœ‰çš„ åŸºç¡€æ¨¡å‹ ä½¿ç”¨åŒä¸€ä¸ªé¢„è®¾çš„æ­¥é•¿(è¶…å‚)ã€‚\n\né‚£ä¹ˆ: \n$$\n\\vec f_m =  \\alpha_0 (- \\vec g_0) + \\alpha_1 (- \\vec g_1) + \\alpha_2 (- \\vec g_2) + \\dots + \\alpha_m (- \\vec g_m) \\tag 1\n$$\nå¯¹äºä¸Šè¯•æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼š\n\n- æ¯ä¸€æ­¥çš„ $(- \\vec g)$ éƒ½è¡¨ç¤ºä¸€ä¸ªåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ç»“æœ, åœ¨GBDTä¸­è¿™ä¸ªåŸºç¡€æ¨¡å‹å°±æ˜¯ å†³ç­–æ ‘ã€‚é‚£ä¹ˆæœ€ç»ˆçš„æ¨¡å‹é¢„æµ‹ç»“æœï¼Œæ˜¯æ‰€æœ‰å•ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœçš„ç´¯åŠ ï¼›\n- å¢åŠ æ–°çš„æ¨¡å‹ï¼Œä½¿ç»“æœåœ¨å‰ä¸€æ­¥çš„åŸºç¡€ä¸Šæ›´åŠ é€¼è¿‘æœ€ä¼˜ç‚¹ï¼›\n- æ–°æ¨¡å‹è®­ç»ƒçš„æ˜¯ä¸Šä¸€ä¸ªæ¨¡å‹çš„**æ®‹å·®**ã€‚\n\nè¿™å°±æ˜¯ Gradient + Boosting çš„æ•°å­¦æ¨å¯¼ã€‚\n\n# GBDT\n\nDTåœ¨è¿™é‡ŒæŒ‡çš„æ˜¯å†³ç­–æ ‘(Decesion Tree)ï¼Œ è€Œå†³ç­–æ ‘æœ‰åˆ†ä¸º  åˆ†ç±»æ ‘(Classification Tree) å’Œ å›å½’æ ‘ (Regression Tree)ã€‚ å®é™…ä¸Šåœ¨GBDTæ¨¡å‹ä¸­ä½¿ç”¨çš„æ˜¯å›å½’æ ‘ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆGBDTåˆå MART(Multiple Additive Regression Tree) çš„åŸå› ã€‚\n\n>Decesion Trees partition the space of all joint predictor variable values into disjoint regions $R_j,  j = 1, 2,  \\dots, Jâ€‹$ , as represented by the terminal nodes of the tree. A constant $\\gamma_jâ€‹$ is assigned to each such region and the predictive  rule is \n>$$\n>x \\in R_j  \\Rightarrow f(x) = \\gamma_j\n>$$\n>å†³ç­–æ ‘å¯ä»¥è¢«å®šä¹‰ä¸ºï¼š\n>$$\n>T(x; \\Theta) = \\sum_{j=i}^J \\gamma_j I(x \\in R_j)\n>$$\n>å…¶ä¸­ï¼š $\\Theta = \\{R_j, \\gamma_j\\}_1^J $ è¡¨ç¤ºæ¨¡å‹å‚æ•°, $J$ è¡¨ç¤ºå†³ç­–æ ‘å¶å­èŠ‚ç‚¹çš„æ•°é‡(é€šå¸¸æ˜¯é€šè¿‡é™åˆ¶æ ‘çš„æ·±åº¦é™åˆ¶å¶å­èŠ‚ç‚¹çš„æ•°é‡)ã€‚\n\nå†³ç­–æ ‘å°±æ˜¯å°†æ ·æœ¬çš„å±æ€§æ˜ å°„åˆ°ä¸€ä¸ªå…·ä½“çš„ç±»åˆ«(åˆ†ç±»æ ‘)æˆ–è€…score(å›å½’æ ‘)ã€‚\n\nä½¿ç”¨ è‹¥å¹²é¢—å›å½’æ ‘å¯¹ å¼ 1 ä¸­çš„æ‰€æœ‰æ¢¯åº¦ $-g$ è¿›è¡Œæ›¿æ¢å³å¾—åˆ°ä¸€ä¸ªGBDTæ¨¡å‹çš„è¡¨ç¤ºæ–¹æ³•ã€‚å¦‚ä¸‹ï¼š\n$$\n\\begin {align}\n\\vec f_m &=  \\alpha_0 (- \\vec g_0 &) + \\alpha_1 (- \\vec g_1 &) + \\alpha_2 ( - \\vec g_2 &) + \\dots & + \\alpha_m (- \\vec g_m &) \\\\\n\\downarrow \\\\\n\\vec f_m &=  \\alpha_0 (T_0 &) + \\alpha_1 (T_1 &) + \\alpha_2 (T_2 &) + \\dots & + \\alpha_m (T_m &) \\\\\n\\end {align}\n$$\n\né€šå¸¸æ¥è¯´ï¼Œä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå®é™…ä¸Šå°±æ˜¯å¯»æ‰¾æ¨¡å‹å‚æ•°çš„è¿‡ç¨‹ã€‚é‚£ä¹ˆGBDTæ¨¡å‹çš„å‚æ•°æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ\n\n1. parameters Including structure of each tree, and the score in the leafï¼› æˆ–è€…è¯´GBDTæ¨¡å‹çš„å‚æ•°å°±æ˜¯æ¨¡å‹ä¸­çš„æ‰€æœ‰å†³ç­–æ ‘ $\\{T_0, T_1, T_2, \\dots, T_m\\}$\n2. ä¸ä¸€èˆ¬çš„æ¨¡å‹(å¦‚LR)ä¸åŒï¼ŒInstead learning weights in , we are learning functions(trees) ã€‚\n\n\n\n\n\nGBDTçš„ç®—æ³•æµç¨‹å¦‚ä¸‹:\n\n![](GBDT/GBDT-algorithm.png)\n\n# GBDT åœ¨é¢„å¤„ç†ä¸­çš„çš„åº”ç”¨\n\nGBDTä½œé™¤äº†å¯ä»¥è§£å†³å…¶åˆ†å†…åº”è¯¥è§£å†³çš„é—®é¢˜ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼Œå›å½’ä»»åŠ¡ï¼‰å¤–ï¼Œè¿˜å¯ä»¥ç”¨æ¥è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¦‚ï¼š\n\n1.  ç‰¹å¾ç­›é€‰ï¼š GBDTå¯ä»¥å‘ç°æœ‰åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚ç”±äºGBDTåŸºç¡€æ˜¯å†³ç­–æ ‘ï¼Œæ‰€ä»¥å®ƒä¹Ÿè§‰æœ‰å†³ç­–æ ‘çš„ç‰¹ç‚¹ï¼Œå†³ç­–æ ‘æˆ–è€…éšæœºæ£®æ—å¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œé€‰æ‹©çš„ä¾æ®å°±æ˜¯ç‰¹å¾é‡è¦æ€§ã€‚å…¶ä¸­å¸¸ç”¨é“çš„ç‰¹å¾é‡è¦æ€§è¡¡é‡æŒ‡æ ‡æœ‰ä¸¤ä¸ªï¼š\n\n   - Gini:(Mean Decrease in Impurity (MDI))\n   - â€¢Permutation:(Mean Decrease in Accuracy (MDA)\n\n   è¯¦ç»†å†…å®¹è¿™é‡Œä¸è®ºè¿°äº†ã€‚\n\n   ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒè¿™ç‰‡blog [ä½¿ç”¨GBDTé€‰å–ç‰¹å¾](https://www.letiantian.me/2015-03-31-use-gbdt-to-select-features/), å¤šå¹´å‰æˆ‘æ›¾ç»ä½¿ç”¨å…¶ä¸­çš„æ•°æ®é›†åˆï¼Œåšè¿‡ç›¸åŒçš„å®éªŒï¼Œå¥ˆä½•ä»£ç æ‰¾ä¸åˆ°äº†ï¼Œåªæœ‰å®éªŒç»“è®ºï¼Œä¸”äºblogä¸­çš„ç›¸è¿‘ã€‚å½“æ—¶çš„ç»“è®ºå¦‚ä¸‹ï¼š\n\n   ![](GBDT/GBDT-LR.png)\n\n2. ç‰¹å¾ç»„åˆ\n   GBDTå¯ä»¥å¯¹ç°æœ‰çš„ç‰¹å¾è¿›è¡Œç‰¹å¾ç»„åˆï¼Œç›´æ¥ç”Ÿæˆæ–°çš„æ›´æœ‰æ•ˆçš„ç‰¹å¾ï¼Œçœå»äº†äººå·¥ç‰¹å¾ç»„åˆçš„éº»çƒ¦ã€‚é€šè¿‡GBDTç”Ÿæˆçš„ç‰¹å¾ï¼Œå¯ç›´æ¥ä½œä¸ºLRçš„ç‰¹å¾ä½¿ç”¨ï¼Œçœå»äººå·¥å¤„ç†åˆ†æç‰¹å¾çš„ç¯èŠ‚ï¼ŒLRçš„è¾“å…¥ç‰¹å¾å®Œå…¨ä¾èµ–äºé€šè¿‡GBDTå¾—åˆ°çš„ç‰¹å¾ã€‚è¿™ä¸ªæ–¹æ³•è¢«facebookç”¨æ¥åšctré¢„ä¼°ã€‚åŒæ—¶ä¹Ÿæœ‰ä½¿ç”¨GBDT+FMçš„æ–¹æ¡ˆï¼Œåœ¨ 2014 Kaggle CTRç«èµ›å† å†›å°±æ˜¯ä½¿ç”¨GBDT+FMã€‚\n\n   GBDTè¿›è¡Œç‰¹å¾ç»„åˆçš„åŸç†æ¯”è¾ƒç®€å•ï¼Œå…¶æœ¬è´¨è¿˜è¦è¿½æº¯åˆ°å†³ç­–æ ‘åŸç†ã€‚ å¯¹äºä¸€ä¸ªå†³ç­–æ ‘æ¥è¯´ï¼Œå…¶æœ¬è´¨æ˜¯å°†ç‰¹å¾ç©ºé—´åˆ†å‰²æˆè‹¥å¹²ä¸ªä¸ç›¸äº¤çš„ç¦»æ•£åŒºåŸŸï¼Œå¯¹äºæ¯ä¸€ä¸ªåŒºåŸŸèµ‹äºˆä¸€ä¸ªé¢„æµ‹å€¼ã€‚\n   ![](GBDT/dt-feature-compose.png)\n\n   â€‹\n   å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå‡è®¾æˆ‘ä»¬ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬åªæœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œå³æˆ‘ä¹ˆçš„æ ·æœ¬ç©ºé—´å°±æ˜¯å³å›¾é‚£æ ·çš„ä¸€ä¸ªå¹³é¢ã€‚ä½¿ç”¨ä¸€é¢—å†³ç­–æ ‘è®­ç»ƒåå¾—åˆ°å·¦å›¾æ‰€ç¤ºçš„å†³ç­–æ ‘ï¼Œå…¶å››æ ¼å¶å­èŠ‚ç‚¹å®é™…ä¸Šå°±æ˜¯å°†å³å›¾é‚£æ ·çš„ç‰¹å¾ç©ºé—´åˆ†å‰²ç§°ä¸ºå››ä¸ªåŒºåŸŸã€‚å…¶ä¸€ä¸€å¯¹äºå…³ç³»å¦‚å›¾ä¸­çš„æ•°å­—æ‰€ç¤ºã€‚ä¸€ä¸ªæ ·æœ¬å¿…ç„¶ä¼šè½åœ¨å››ä¸ªå¶å­èŠ‚ç‚¹ä¸­çš„ä¸€ä¸ªå†…ã€‚\n\n   å½“æˆ‘ä»¬ä½¿ç”¨GBDTæ—¶ï¼Œå°†ä¼šæœ‰å¤šé¢—å†³ç­–æ ‘ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŸºäºè®­ç»ƒæ ·æœ¬å¾—åˆ°ç”±ä¸‰é¢—å†³ç­–æ ‘ç»„æˆçš„GBDTæ¨¡å‹ã€‚\n\n   ![](GBDT/gbdt-feature-compose.png)\n\n   â€‹\n\n   å¯¹äºä»»ä½•ä¸€ä¸ªæ ·æœ¬(record A)ï¼Œè¯¥æ ·æœ¬å°†ä¼šåœ¨æ¯é¢—å†³ç­–æ ‘ä¸­è½åˆ°ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå‡è®¾ record A è½åœ¨äº† 3, 5, 13 è¿™ä¸‰ä¸ªå¶å­èŠ‚ç‚¹ä¸Šã€‚é‚£ä¹ˆ record  A çš„æ–°ç‰¹å¾ä¸º :(0,0,1,0**,** 1,0,0,0**,** 0,0,0,1)ã€‚noteï¼š æ–°ç‰¹å¾ä¸­æ¯ä¸€ä½å¯¹åº”ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ¯å››ä½å¯¹åº”ä¸€é¢—å†³ç­–æ ‘ï¼Œ 1 è¡¨ç¤ºæ ·æœ¬è½åœ¨äº†ç›¸åº”èŠ‚ç‚¹ä¸Šï¼Œ0 è¡¨ç¤ºæ ·æœ¬æ²¡æœ‰è½åœ¨äº†ç›¸åº”èŠ‚ç‚¹ä¸Šã€‚\n\n# å‚è€ƒèµ„æ–™\n\nThe Elements of Statistical Learning\n\n[Practical Lessons from Predicting Clicks on Ads at Facebook](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)","tags":["Machine Learning"],"categories":["Machine Learning"]}]